{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "%matplotlib inline\n",
    "\n",
    "import sys\n",
    "import itertools\n",
    "import numpy as np  # Matrix and vector computation package\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt  # Plotting library\n",
    "import seaborn as sns  # Fancier plots\n",
    "\n",
    "# Set seaborn plotting style\n",
    "sns.set_style('darkgrid')\n",
    "# Set the seed for reproducability\n",
    "np.random.seed(seed=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train tensor shape: (2000, 7, 2)\n",
      "T_train tensor shape: (2000, 7, 1)\n"
     ]
    }
   ],
   "source": [
    "# Create dataset\n",
    "nb_train = 2000  # Number of training samples\n",
    "# Addition of 2 n-bit numbers can result in a n+1 bit number\n",
    "sequence_len = 7  # Length of the binary sequence\n",
    "\n",
    "def create_dataset(nb_samples, sequence_len):\n",
    "    \"\"\"Create a dataset for binary addition and \n",
    "    return as input, targets.\"\"\"\n",
    "    max_int = 2**(sequence_len-1) # Maximum integer that can be added\n",
    "     # Transform integer in binary format\n",
    "    format_str = '{:0' + str(sequence_len) + 'b}'\n",
    "    nb_inputs = 2  # Add 2 binary numbers\n",
    "    nb_outputs = 1  # Result is 1 binary number\n",
    "    # Input samples\n",
    "    X = np.zeros((nb_samples, sequence_len, nb_inputs))\n",
    "    # Target samples\n",
    "    T = np.zeros((nb_samples, sequence_len, nb_outputs))\n",
    "    # Fill up the input and target matrix\n",
    "    for i in range(nb_samples):\n",
    "        # Generate random numbers to add\n",
    "        nb1 = np.random.randint(0, max_int)\n",
    "        nb2 = np.random.randint(0, max_int)\n",
    "        # Fill current input and target row.\n",
    "        # Note that binary numbers are added from right to left, \n",
    "        #  but our RNN reads from left to right, so reverse the sequence.\n",
    "        X[i,:,0] = list(\n",
    "            reversed([int(b) for b in format_str.format(nb1)]))\n",
    "        X[i,:,1] = list(\n",
    "            reversed([int(b) for b in format_str.format(nb2)]))\n",
    "        T[i,:,0] = list(\n",
    "            reversed([int(b) for b in format_str.format(nb1+nb2)]))\n",
    "    return X, T\n",
    "\n",
    "# Create training samples\n",
    "X_train, T_train = create_dataset(nb_train, sequence_len)\n",
    "print(f'X_train tensor shape: {X_train.shape}')\n",
    "print(f'T_train tensor shape: {T_train.shape}')\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dummy_dataset(dataset_length, seq_length, max_number):\n",
    "    lower_bound = -1 * max_number\n",
    "    x_train = np.random.randint(low=lower_bound, high=max_number+1, size=(dataset_length, seq_length, 1))\n",
    "    y_train = np.where(x_train.sum(axis=2) > 0, 1, 0).reshape(x_train.shape)\n",
    "    \n",
    "    x_test = np.random.randint(low=lower_bound, high=max_number+1, size=(dataset_length, seq_length, 1))\n",
    "    y_test = np.where(x_test.sum(axis=2) > 0, 1, 0).reshape(x_train.shape)\n",
    "    \n",
    "    return x_train, y_train, x_test, y_test\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x1:   1010010   37\n",
      "x2: + 1101010   43\n",
      "      -------   --\n",
      "t:  = 0000101   80\n"
     ]
    }
   ],
   "source": [
    "# Show an example input and target\n",
    "def printSample(x1, x2, t, y=None):\n",
    "    \"\"\"Print a sample in a more visual way.\"\"\"\n",
    "    x1 = ''.join([str(int(d)) for d in x1])\n",
    "    x1_r = int(''.join(reversed(x1)), 2)\n",
    "    x2 = ''.join([str(int(d)) for d in x2])\n",
    "    x2_r = int(''.join(reversed(x2)), 2)\n",
    "    t = ''.join([str(int(d[0])) for d in t])\n",
    "    t_r = int(''.join(reversed(t)), 2)\n",
    "    if not y is None:\n",
    "        y = ''.join([str(int(d[0])) for d in y])\n",
    "    print(f'x1:   {x1:s}   {x1_r:2d}')\n",
    "    print(f'x2: + {x2:s}   {x2_r:2d}')\n",
    "    print(f'      -------   --')\n",
    "    print(f't:  = {t:s}   {t_r:2d}')\n",
    "    if not y is None:\n",
    "        print(f'y:  = {y:s}')\n",
    "    \n",
    "# Print the first sample\n",
    "printSample(X_train[0,:,0], X_train[0,:,1], T_train[0,:,:])\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the linear tensor transformation layer\n",
    "class TensorLinear(object):\n",
    "    \"\"\"The linear tensor layer applies a linear tensor dot product \n",
    "    and a bias to its input.\"\"\"\n",
    "    def __init__(self, n_in, n_out, tensor_order, W=None, b=None):\n",
    "        \"\"\"Initialse the weight W and bias b parameters.\"\"\"\n",
    "        a = np.sqrt(6.0 / (n_in + n_out))\n",
    "        self.W = (np.random.uniform(-a, a, (n_in, n_out)) \n",
    "                  if W is None else W)\n",
    "        self.b = (np.zeros((n_out)) if b is None else b)\n",
    "        # Axes summed over in backprop\n",
    "        self.bpAxes = tuple(range(tensor_order-1))\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"Perform forward step transformation with the help \n",
    "        of a tensor product.\"\"\"\n",
    "        # Same as: Y[i,j,:] = np.dot(X[i,j,:], self.W) + self.b \n",
    "        #          (for i,j in X.shape[0:1])\n",
    "        # Same as: Y = np.einsum('ijk,kl->ijl', X, self.W) + self.b\n",
    "        return np.tensordot(X, self.W, axes=((-1),(0))) + self.b\n",
    "\n",
    "    def backward(self, X, gY):\n",
    "        \"\"\"Return the gradient of the parmeters and the inputs of \n",
    "        this layer.\"\"\"\n",
    "        # Same as: gW = np.einsum('ijk,ijl->kl', X, gY)\n",
    "        # Same as: gW += np.dot(X[:,j,:].T, gY[:,j,:]) \n",
    "        #          (for i,j in X.shape[0:1])\n",
    "        gW = np.tensordot(X, gY, axes=(self.bpAxes, self.bpAxes))\n",
    "        gB = np.sum(gY, axis=self.bpAxes)\n",
    "        # Same as: gX = np.einsum('ijk,kl->ijl', gY, self.W.T)\n",
    "        # Same as: gX[i,j,:] = np.dot(gY[i,j,:], self.W.T) \n",
    "        #          (for i,j in gY.shape[0:1])\n",
    "        gX = np.tensordot(gY, self.W.T, axes=((-1),(0)))  \n",
    "        return gX, gW, gB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the logistic classifier layer\n",
    "class LogisticClassifier(object):\n",
    "    \"\"\"The logistic layer applies the logistic function to its \n",
    "    inputs.\"\"\"\n",
    "   \n",
    "    def forward(self, X):\n",
    "        \"\"\"Perform the forward step transformation.\"\"\"\n",
    "        return 1. / (1. + np.exp(-X))\n",
    "    \n",
    "    def backward(self, Y, T):\n",
    "        \"\"\"Return the gradient with respect to the loss function \n",
    "        at the inputs of this layer.\"\"\"\n",
    "        # Average by the number of samples and sequence length.\n",
    "        return (Y - T) / (Y.shape[0] * Y.shape[1])\n",
    "    \n",
    "    def loss(self, Y, T):\n",
    "        \"\"\"Compute the loss at the output.\"\"\"\n",
    "        return -np.mean((T * np.log(Y)) + ((1-T) * np.log(1-Y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define tanh layer\n",
    "class TanH(object):\n",
    "    \"\"\"TanH applies the tanh function to its inputs.\"\"\"\n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"Perform the forward step transformation.\"\"\"\n",
    "        return np.tanh(X) \n",
    "    \n",
    "    def backward(self, Y, output_grad):\n",
    "        \"\"\"Return the gradient at the inputs of this layer.\"\"\"\n",
    "        gTanh = 1.0 - (Y**2)\n",
    "        return (gTanh * output_grad)\n",
    "\n",
    "# Define internal state update layer\n",
    "class RecurrentStateUpdate(object):\n",
    "    \"\"\"Update a given state.\"\"\"\n",
    "    def __init__(self, nbStates, W, b):\n",
    "        \"\"\"Initialse the linear transformation and tanh transfer \n",
    "        function.\"\"\"\n",
    "        self.linear = TensorLinear(n_in=nbStates, n_out=nbStates, tensor_order=2, W=W, b=b)\n",
    "        self.tanh = TanH()\n",
    "\n",
    "    def forward(self, Xk, Sk):\n",
    "        \"\"\"Return state k+1 from input and state k.\"\"\"\n",
    "        return self.tanh.forward(Xk + self.linear.forward(Sk))\n",
    "    \n",
    "    def backward(self, Sk0, Sk1, output_grad):\n",
    "        \"\"\"Return the gradient of the parmeters and the inputs of \n",
    "        this layer.\"\"\"\n",
    "        gZ = self.tanh.backward(Sk1, output_grad)\n",
    "        gSk0, gW, gB = self.linear.backward(Sk0, gZ)\n",
    "        return gZ, gSk0, gW, gB\n",
    "\n",
    "# Define layer that unfolds the states over time\n",
    "class RecurrentStateUnfold(object):\n",
    "    \"\"\"Unfold the recurrent states.\"\"\"\n",
    "    def __init__(self, nbStates, nbTimesteps):\n",
    "        \"\"\"Initialse the shared parameters, the inital state and \n",
    "        state update function.\"\"\"\n",
    "        a = np.sqrt(6. / (nbStates * 2))\n",
    "        self.W = np.random.uniform(-a, a, (nbStates, nbStates))\n",
    "        self.b = np.zeros((self.W.shape[0]))  # Shared bias\n",
    "        self.S0 = np.zeros(nbStates)  # Initial state\n",
    "        self.nbTimesteps = nbTimesteps  # Timesteps to unfold\n",
    "        self.stateUpdate = RecurrentStateUpdate(\n",
    "            nbStates, self.W, self.b)  # State update function\n",
    "        \n",
    "    def forward(self, X):\n",
    "        \"\"\"Iteratively apply forward step to all states.\"\"\"\n",
    "        # State tensor\n",
    "        S = np.zeros((X.shape[0], X.shape[1]+1, self.W.shape[0]))\n",
    "        S[:,0,:] = self.S0  # Set initial state\n",
    "        for k in range(self.nbTimesteps):\n",
    "            # Update the states iteratively\n",
    "            S[:,k+1,:] = self.stateUpdate.forward(X[:,k,:], S[:,k,:])\n",
    "        return S\n",
    "    \n",
    "    def backward(self, X, S, gY):\n",
    "        \"\"\"Return the gradient of the parmeters and the inputs of \n",
    "        this layer.\"\"\"\n",
    "        # Initialise gradient of state outputs\n",
    "        gSk = np.zeros_like(gY[:,self.nbTimesteps-1,:])\n",
    "        # Initialse gradient tensor for state inputs\n",
    "        gZ = np.zeros_like(X)\n",
    "        gWSum = np.zeros_like(self.W)  # Initialise weight gradients\n",
    "        gBSum = np.zeros_like(self.b)  # Initialse bias gradients\n",
    "        # Propagate the gradients iteratively\n",
    "        for k in range(self.nbTimesteps-1, -1, -1):\n",
    "            # Gradient at state output is gradient from previous state \n",
    "            #  plus gradient from output\n",
    "            gSk += gY[:,k,:]\n",
    "            # Propgate the gradient back through one state\n",
    "            gZ[:,k,:], gSk, gW, gB = self.stateUpdate.backward(\n",
    "                S[:,k,:], S[:,k+1,:], gSk)\n",
    "            gWSum += gW  # Update total weight gradient\n",
    "            gBSum += gB  # Update total bias gradient\n",
    "        # Get gradient of initial state over all samples\n",
    "        gS0 = np.sum(gSk, axis=0)\n",
    "        return gZ, gWSum, gBSum, gS0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the full network\n",
    "class RnnBinaryAdder(object):\n",
    "    \"\"\"RNN to perform binary addition of 2 numbers.\"\"\"\n",
    "    def __init__(self, nb_of_inputs, nb_of_outputs, nb_of_states, \n",
    "                 sequence_len):\n",
    "        \"\"\"Initialse the network layers.\"\"\"\n",
    "        # Input layer\n",
    "        self.tensorInput = TensorLinear(nb_of_inputs, nb_of_states, 3)\n",
    "        # Recurrent layer\n",
    "        self.rnnUnfold = RecurrentStateUnfold(nb_of_states, sequence_len)\n",
    "        # Linear output transform\n",
    "        self.tensorOutput = TensorLinear(nb_of_states, nb_of_outputs, 3)\n",
    "        self.classifier = LogisticClassifier()  # Classification output\n",
    "        \n",
    "    def forward(self, X):\n",
    "        \"\"\"Perform the forward propagation of input X through all \n",
    "        layers.\"\"\"\n",
    "        # Linear input transformation\n",
    "        recIn = self.tensorInput.forward(X)\n",
    "        # Forward propagate through time and return states\n",
    "        S = self.rnnUnfold.forward(recIn)\n",
    "        # Linear output transformation\n",
    "        Z = self.tensorOutput.forward(S[:,1:sequence_len+1,:])\n",
    "        Y = self.classifier.forward(Z)  # Classification probabilities\n",
    "        # Return: input to recurrent layer, states, input to classifier, \n",
    "        #  output\n",
    "        return recIn, S, Z, Y\n",
    "    \n",
    "    def backward(self, X, Y, recIn, S, T):\n",
    "        \"\"\"Perform the backward propagation through all layers.\n",
    "        Input: input samples, network output, intput to recurrent \n",
    "        layer, states, targets.\"\"\"\n",
    "        print(f'Y {Y.shape}')\n",
    "        print(f'T {T.shape}')\n",
    "        gZ = self.classifier.backward(Y, T)  # Get output gradient\n",
    "        print(f'gZ {gZ.shape}')\n",
    "        gRecOut, gWout, gBout = self.tensorOutput.backward(\n",
    "            S[:,1:sequence_len+1,:], gZ)\n",
    "        # Propagate gradient backwards through time\n",
    "        gRnnIn, gWrec, gBrec, gS0 = self.rnnUnfold.backward(\n",
    "            recIn, S, gRecOut)\n",
    "        gX, gWin, gBin = self.tensorInput.backward(X, gRnnIn)\n",
    "        # Return the parameter gradients of: linear output weights, \n",
    "        #  linear output bias, recursive weights, recursive bias, #\n",
    "        #  linear input weights, linear input bias, initial state.\n",
    "        return gWout, gBout, gWrec, gBrec, gWin, gBin, gS0\n",
    "    \n",
    "    def getOutput(self, X):\n",
    "        \"\"\"Get the output probabilities of input X.\"\"\"\n",
    "        recIn, S, Z, Y = self.forward(X)\n",
    "        return Y\n",
    "    \n",
    "    def getBinaryOutput(self, X):\n",
    "        \"\"\"Get the binary output of input X.\"\"\"\n",
    "        return np.around(self.getOutput(X))\n",
    "    \n",
    "    def getParamGrads(self, X, T):\n",
    "        \"\"\"Return the gradients with respect to input X and \n",
    "        target T as a list. The list has the same order as the \n",
    "        get_params_iter iterator.\"\"\"\n",
    "        recIn, S, Z, Y = self.forward(X)\n",
    "        gWout, gBout, gWrec, gBrec, gWin, gBin, gS0 = self.backward(\n",
    "            X, Y, recIn, S, T)\n",
    "        return [g for g in itertools.chain(\n",
    "                np.nditer(gS0),\n",
    "                np.nditer(gWin),\n",
    "                np.nditer(gBin),\n",
    "                np.nditer(gWrec),\n",
    "                np.nditer(gBrec),\n",
    "                np.nditer(gWout),\n",
    "                np.nditer(gBout))]\n",
    "    \n",
    "    def loss(self, Y, T):\n",
    "        \"\"\"Return the loss of input X w.r.t. targets T.\"\"\"\n",
    "        return self.classifier.loss(Y, T)\n",
    "    \n",
    "    def get_params_iter(self):\n",
    "        \"\"\"Return an iterator over the parameters.\n",
    "        The iterator has the same order as get_params_grad.\n",
    "        The elements returned by the iterator are editable in-place.\"\"\"\n",
    "        return itertools.chain(\n",
    "            np.nditer(self.rnnUnfold.S0, op_flags=['readwrite']),\n",
    "            np.nditer(self.tensorInput.W, op_flags=['readwrite']),\n",
    "            np.nditer(self.tensorInput.b, op_flags=['readwrite']),\n",
    "            np.nditer(self.rnnUnfold.W, op_flags=['readwrite']),\n",
    "            np.nditer(self.rnnUnfold.b, op_flags=['readwrite']),\n",
    "            np.nditer(self.tensorOutput.W, op_flags=['readwrite']), \n",
    "            np.nditer(self.tensorOutput.b, op_flags=['readwrite']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y (100, 7, 1)\n",
      "T (100, 7, 1)\n",
      "gZ (100, 7, 1)\n",
      "Y (100, 7, 1)\n",
      "T (100, 7, 1)\n",
      "gZ (100, 7, 1)\n",
      "Y (100, 7, 1)\n",
      "T (100, 7, 1)\n",
      "gZ (100, 7, 1)\n",
      "Y (100, 7, 1)\n",
      "T (100, 7, 1)\n",
      "gZ (100, 7, 1)\n",
      "Y (100, 7, 1)\n",
      "T (100, 7, 1)\n",
      "gZ (100, 7, 1)\n",
      "Y (100, 7, 1)\n",
      "T (100, 7, 1)\n",
      "gZ (100, 7, 1)\n",
      "Y (100, 7, 1)\n",
      "T (100, 7, 1)\n",
      "gZ (100, 7, 1)\n",
      "Y (100, 7, 1)\n",
      "T (100, 7, 1)\n",
      "gZ (100, 7, 1)\n",
      "Y (100, 7, 1)\n",
      "T (100, 7, 1)\n",
      "gZ (100, 7, 1)\n",
      "Y (100, 7, 1)\n",
      "T (100, 7, 1)\n",
      "gZ (100, 7, 1)\n",
      "Y (100, 7, 1)\n",
      "T (100, 7, 1)\n",
      "gZ (100, 7, 1)\n",
      "Y (100, 7, 1)\n",
      "T (100, 7, 1)\n",
      "gZ (100, 7, 1)\n",
      "Y (100, 7, 1)\n",
      "T (100, 7, 1)\n",
      "gZ (100, 7, 1)\n",
      "Y (100, 7, 1)\n",
      "T (100, 7, 1)\n",
      "gZ (100, 7, 1)\n",
      "Y (100, 7, 1)\n",
      "T (100, 7, 1)\n",
      "gZ (100, 7, 1)\n",
      "Y (100, 7, 1)\n",
      "T (100, 7, 1)\n",
      "gZ (100, 7, 1)\n",
      "Y (100, 7, 1)\n",
      "T (100, 7, 1)\n",
      "gZ (100, 7, 1)\n",
      "Y (100, 7, 1)\n",
      "T (100, 7, 1)\n",
      "gZ (100, 7, 1)\n",
      "Y (100, 7, 1)\n",
      "T (100, 7, 1)\n",
      "gZ (100, 7, 1)\n",
      "Y (100, 7, 1)\n",
      "T (100, 7, 1)\n",
      "gZ (100, 7, 1)\n",
      "Y (100, 7, 1)\n",
      "T (100, 7, 1)\n",
      "gZ (100, 7, 1)\n",
      "Y (100, 7, 1)\n",
      "T (100, 7, 1)\n",
      "gZ (100, 7, 1)\n",
      "Y (100, 7, 1)\n",
      "T (100, 7, 1)\n",
      "gZ (100, 7, 1)\n",
      "Y (100, 7, 1)\n",
      "T (100, 7, 1)\n",
      "gZ (100, 7, 1)\n",
      "Y (100, 7, 1)\n",
      "T (100, 7, 1)\n",
      "gZ (100, 7, 1)\n",
      "Y (100, 7, 1)\n",
      "T (100, 7, 1)\n",
      "gZ (100, 7, 1)\n",
      "Y (100, 7, 1)\n",
      "T (100, 7, 1)\n",
      "gZ (100, 7, 1)\n",
      "Y (100, 7, 1)\n",
      "T (100, 7, 1)\n",
      "gZ (100, 7, 1)\n",
      "Y (100, 7, 1)\n",
      "T (100, 7, 1)\n",
      "gZ (100, 7, 1)\n",
      "Y (100, 7, 1)\n",
      "T (100, 7, 1)\n",
      "gZ (100, 7, 1)\n",
      "Y (100, 7, 1)\n",
      "T (100, 7, 1)\n",
      "gZ (100, 7, 1)\n",
      "Y (100, 7, 1)\n",
      "T (100, 7, 1)\n",
      "gZ (100, 7, 1)\n",
      "Y (100, 7, 1)\n",
      "T (100, 7, 1)\n",
      "gZ (100, 7, 1)\n",
      "Y (100, 7, 1)\n",
      "T (100, 7, 1)\n",
      "gZ (100, 7, 1)\n",
      "Y (100, 7, 1)\n",
      "T (100, 7, 1)\n",
      "gZ (100, 7, 1)\n",
      "Y (100, 7, 1)\n",
      "T (100, 7, 1)\n",
      "gZ (100, 7, 1)\n",
      "Y (100, 7, 1)\n",
      "T (100, 7, 1)\n",
      "gZ (100, 7, 1)\n",
      "Y (100, 7, 1)\n",
      "T (100, 7, 1)\n",
      "gZ (100, 7, 1)\n",
      "Y (100, 7, 1)\n",
      "T (100, 7, 1)\n",
      "gZ (100, 7, 1)\n",
      "Y (100, 7, 1)\n",
      "T (100, 7, 1)\n",
      "gZ (100, 7, 1)\n",
      "Y (100, 7, 1)\n",
      "T (100, 7, 1)\n",
      "gZ (100, 7, 1)\n",
      "Y (100, 7, 1)\n",
      "T (100, 7, 1)\n",
      "gZ (100, 7, 1)\n",
      "Y (100, 7, 1)\n",
      "T (100, 7, 1)\n",
      "gZ (100, 7, 1)\n",
      "Y (100, 7, 1)\n",
      "T (100, 7, 1)\n",
      "gZ (100, 7, 1)\n",
      "Y (100, 7, 1)\n",
      "T (100, 7, 1)\n",
      "gZ (100, 7, 1)\n",
      "Y (100, 7, 1)\n",
      "T (100, 7, 1)\n",
      "gZ (100, 7, 1)\n",
      "Y (100, 7, 1)\n",
      "T (100, 7, 1)\n",
      "gZ (100, 7, 1)\n",
      "Y (100, 7, 1)\n",
      "T (100, 7, 1)\n",
      "gZ (100, 7, 1)\n",
      "Y (100, 7, 1)\n",
      "T (100, 7, 1)\n",
      "gZ (100, 7, 1)\n",
      "Y (100, 7, 1)\n",
      "T (100, 7, 1)\n",
      "gZ (100, 7, 1)\n",
      "Y (100, 7, 1)\n",
      "T (100, 7, 1)\n",
      "gZ (100, 7, 1)\n",
      "Y (100, 7, 1)\n",
      "T (100, 7, 1)\n",
      "gZ (100, 7, 1)\n",
      "Y (100, 7, 1)\n",
      "T (100, 7, 1)\n",
      "gZ (100, 7, 1)\n",
      "Y (100, 7, 1)\n",
      "T (100, 7, 1)\n",
      "gZ (100, 7, 1)\n",
      "Y (100, 7, 1)\n",
      "T (100, 7, 1)\n",
      "gZ (100, 7, 1)\n",
      "Y (100, 7, 1)\n",
      "T (100, 7, 1)\n",
      "gZ (100, 7, 1)\n",
      "Y (100, 7, 1)\n",
      "T (100, 7, 1)\n",
      "gZ (100, 7, 1)\n",
      "Y (100, 7, 1)\n",
      "T (100, 7, 1)\n",
      "gZ (100, 7, 1)\n",
      "Y (100, 7, 1)\n",
      "T (100, 7, 1)\n",
      "gZ (100, 7, 1)\n",
      "Y (100, 7, 1)\n",
      "T (100, 7, 1)\n",
      "gZ (100, 7, 1)\n",
      "Y (100, 7, 1)\n",
      "T (100, 7, 1)\n",
      "gZ (100, 7, 1)\n",
      "Y (100, 7, 1)\n",
      "T (100, 7, 1)\n",
      "gZ (100, 7, 1)\n",
      "Y (100, 7, 1)\n",
      "T (100, 7, 1)\n",
      "gZ (100, 7, 1)\n",
      "Y (100, 7, 1)\n",
      "T (100, 7, 1)\n",
      "gZ (100, 7, 1)\n",
      "Y (100, 7, 1)\n",
      "T (100, 7, 1)\n",
      "gZ (100, 7, 1)\n",
      "Y (100, 7, 1)\n",
      "T (100, 7, 1)\n",
      "gZ (100, 7, 1)\n",
      "Y (100, 7, 1)\n",
      "T (100, 7, 1)\n",
      "gZ (100, 7, 1)\n",
      "Y (100, 7, 1)\n",
      "T (100, 7, 1)\n",
      "gZ (100, 7, 1)\n",
      "Y (100, 7, 1)\n",
      "T (100, 7, 1)\n",
      "gZ (100, 7, 1)\n",
      "Y (100, 7, 1)\n",
      "T (100, 7, 1)\n",
      "gZ (100, 7, 1)\n",
      "Y (100, 7, 1)\n",
      "T (100, 7, 1)\n",
      "gZ (100, 7, 1)\n",
      "Y (100, 7, 1)\n",
      "T (100, 7, 1)\n",
      "gZ (100, 7, 1)\n",
      "Y (100, 7, 1)\n",
      "T (100, 7, 1)\n",
      "gZ (100, 7, 1)\n",
      "Y (100, 7, 1)\n",
      "T (100, 7, 1)\n",
      "gZ (100, 7, 1)\n",
      "Y (100, 7, 1)\n",
      "T (100, 7, 1)\n",
      "gZ (100, 7, 1)\n",
      "Y (100, 7, 1)\n",
      "T (100, 7, 1)\n",
      "gZ (100, 7, 1)\n",
      "Y (100, 7, 1)\n",
      "T (100, 7, 1)\n",
      "gZ (100, 7, 1)\n",
      "Y (100, 7, 1)\n",
      "T (100, 7, 1)\n",
      "gZ (100, 7, 1)\n",
      "Y (100, 7, 1)\n",
      "T (100, 7, 1)\n",
      "gZ (100, 7, 1)\n",
      "Y (100, 7, 1)\n",
      "T (100, 7, 1)\n",
      "gZ (100, 7, 1)\n",
      "Y (100, 7, 1)\n",
      "T (100, 7, 1)\n",
      "gZ (100, 7, 1)\n",
      "Y (100, 7, 1)\n",
      "T (100, 7, 1)\n",
      "gZ (100, 7, 1)\n",
      "Y (100, 7, 1)\n",
      "T (100, 7, 1)\n",
      "gZ (100, 7, 1)\n",
      "Y (100, 7, 1)\n",
      "T (100, 7, 1)\n",
      "gZ (100, 7, 1)\n",
      "Y (100, 7, 1)\n",
      "T (100, 7, 1)\n",
      "gZ (100, 7, 1)\n",
      "Y (100, 7, 1)\n",
      "T (100, 7, 1)\n",
      "gZ (100, 7, 1)\n",
      "Y (100, 7, 1)\n",
      "T (100, 7, 1)\n",
      "gZ (100, 7, 1)\n",
      "Y (100, 7, 1)\n",
      "T (100, 7, 1)\n",
      "gZ (100, 7, 1)\n",
      "Y (100, 7, 1)\n",
      "T (100, 7, 1)\n",
      "gZ (100, 7, 1)\n",
      "Y (100, 7, 1)\n",
      "T (100, 7, 1)\n",
      "gZ (100, 7, 1)\n",
      "Y (100, 7, 1)\n",
      "T (100, 7, 1)\n",
      "gZ (100, 7, 1)\n",
      "Y (100, 7, 1)\n",
      "T (100, 7, 1)\n",
      "gZ (100, 7, 1)\n",
      "Y (100, 7, 1)\n",
      "T (100, 7, 1)\n",
      "gZ (100, 7, 1)\n",
      "Y (100, 7, 1)\n",
      "T (100, 7, 1)\n",
      "gZ (100, 7, 1)\n",
      "Y (100, 7, 1)\n",
      "T (100, 7, 1)\n",
      "gZ (100, 7, 1)\n",
      "Y (100, 7, 1)\n",
      "T (100, 7, 1)\n",
      "gZ (100, 7, 1)\n",
      "Y (100, 7, 1)\n",
      "T (100, 7, 1)\n",
      "gZ (100, 7, 1)\n",
      "Y (100, 7, 1)\n",
      "T (100, 7, 1)\n",
      "gZ (100, 7, 1)\n",
      "Y (100, 7, 1)\n",
      "T (100, 7, 1)\n",
      "gZ (100, 7, 1)\n",
      "Y (100, 7, 1)\n",
      "T (100, 7, 1)\n",
      "gZ (100, 7, 1)\n"
     ]
    }
   ],
   "source": [
    "# Set hyper-parameters\n",
    "lmbd = 0.5  # Rmsprop lambda\n",
    "learning_rate = 0.05  # Learning rate\n",
    "momentum_term = 0.80  # Momentum term\n",
    "eps = 1e-6  # Numerical stability term to prevent division by zero\n",
    "mb_size = 100  # Size of the minibatches (number of samples)\n",
    "\n",
    "# Create the network\n",
    "nb_of_states = 3  # Number of states in the recurrent layer\n",
    "RNN = RnnBinaryAdder(2, 1, nb_of_states, sequence_len)\n",
    "# Set the initial parameters\n",
    "# Number of parameters in the network\n",
    "nbParameters =  sum(1 for _ in RNN.get_params_iter())\n",
    "# Rmsprop moving average\n",
    "maSquare = [0.0 for _ in range(nbParameters)]\n",
    "Vs = [0.0 for _ in range(nbParameters)]  # Momentum\n",
    "\n",
    "# Create a list of minibatch losses to be plotted\n",
    "ls_of_loss = [\n",
    "    RNN.loss(RNN.getOutput(X_train[0:100,:,:]), T_train[0:100,:,:])]\n",
    "\n",
    "# Iterate over some iterations\n",
    "for i in range(5):\n",
    "    # Iterate over all the minibatches\n",
    "    for mb in range(nb_train // mb_size):\n",
    "        X_mb = X_train[mb:mb+mb_size,:,:]  # Input minibatch\n",
    "        T_mb = T_train[mb:mb+mb_size,:,:]  # Target minibatch\n",
    "        V_tmp = [v * momentum_term for v in Vs]\n",
    "        # Update each parameters according to previous gradient\n",
    "        for pIdx, P in enumerate(RNN.get_params_iter()):\n",
    "            P += V_tmp[pIdx]\n",
    "        # Get gradients after following old velocity\n",
    "        # Get the parameter gradients\n",
    "        backprop_grads = RNN.getParamGrads(X_mb, T_mb)    \n",
    "        # Update each parameter seperately\n",
    "        for pIdx, P in enumerate(RNN.get_params_iter()):\n",
    "            # Update the Rmsprop moving averages\n",
    "            maSquare[pIdx] = lmbd * maSquare[pIdx] + (\n",
    "                1-lmbd) * backprop_grads[pIdx]**2\n",
    "            # Calculate the Rmsprop normalised gradient\n",
    "            pGradNorm = ((\n",
    "                learning_rate * backprop_grads[pIdx]) / np.sqrt(\n",
    "                maSquare[pIdx]) + eps)\n",
    "            # Update the momentum\n",
    "            Vs[pIdx] = V_tmp[pIdx] - pGradNorm     \n",
    "            P -= pGradNorm   # Update the parameter\n",
    "        # Add loss to list to plot\n",
    "        ls_of_loss.append(RNN.loss(RNN.getOutput(X_mb), T_mb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x504 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the loss over the iterations\n",
    "fig = plt.figure(figsize=(10, 7))\n",
    "plt.plot(ls_of_loss, 'b-')\n",
    "plt.xlabel('minibatch iteration')\n",
    "plt.ylabel('$\\\\xi$', fontsize=15)\n",
    "plt.title('Decrease of loss over backprop iteration')\n",
    "plt.xlim(0, 100)\n",
    "fig.subplots_adjust(bottom=0.2)\n",
    "plt.show()\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.001650 : -0.001650\n",
      "0.003091 : 0.003091\n",
      "-0.001621 : -0.001621\n",
      "-0.003291 : -0.003291\n",
      "-0.013788 : -0.013788\n",
      "0.015911 : 0.015911\n",
      "-0.023916 : -0.023916\n",
      "-0.008300 : -0.008300\n",
      "0.011014 : 0.011014\n",
      "-0.010682 : -0.010682\n",
      "0.005436 : 0.005436\n",
      "-0.015177 : -0.015177\n",
      "-0.028385 : -0.028385\n",
      "-0.006144 : -0.006144\n",
      "0.019034 : 0.019034\n",
      "0.021116 : 0.021116\n",
      "0.001325 : 0.001325\n",
      "-0.009676 : -0.009676\n",
      "-0.030277 : -0.030277\n",
      "-0.002805 : -0.002805\n",
      "0.013858 : 0.013858\n",
      "-0.010682 : -0.010682\n",
      "0.005436 : 0.005436\n",
      "-0.015177 : -0.015177\n",
      "0.009015 : 0.009015\n",
      "-0.042683 : -0.042683\n",
      "-0.074586 : -0.074586\n",
      "-0.014087 : -0.014087\n",
      "No gradient errors found\n"
     ]
    }
   ],
   "source": [
    "# Do gradient checking\n",
    "# Define an RNN to test\n",
    "RNN = RnnBinaryAdder(2, 1, 3, sequence_len)\n",
    "# Get the gradients of the parameters from a subset of the data\n",
    "backprop_grads = RNN.getParamGrads(\n",
    "    X_train[0:100,:,:], T_train[0:100,:,:])\n",
    "\n",
    "eps = 1e-7  # Set the small change to compute the numerical gradient\n",
    "# Compute the numerical gradients of the parameters in all layers.\n",
    "for p_idx, param in enumerate(RNN.get_params_iter()):\n",
    "    grad_backprop = backprop_grads[p_idx]\n",
    "    # + eps\n",
    "    param += eps\n",
    "    plus_loss = RNN.loss(\n",
    "        RNN.getOutput(X_train[0:100,:,:]), T_train[0:100,:,:])\n",
    "    # - eps\n",
    "    param -= 2 * eps\n",
    "    min_loss = RNN.loss(\n",
    "        RNN.getOutput(X_train[0:100,:,:]), T_train[0:100,:,:])\n",
    "    # reset param value\n",
    "    param += eps\n",
    "    # calculate numerical gradient\n",
    "    grad_num = (plus_loss - min_loss) / (2*eps)\n",
    "    # Raise error if the numerical grade is not close to the \n",
    "    #  backprop gradient\n",
    "    if not np.isclose(grad_num, grad_backprop):\n",
    "        raise ValueError((\n",
    "            f'Numerical gradient of {grad_num:.6f} is not close '\n",
    "            f'to the backpropagation gradient of {grad_backprop:.6f}!'\n",
    "        ))\n",
    "    else:\n",
    "        print(f'{grad_num:.6f} : {grad_backprop:.6f}')\n",
    "print('No gradient errors found')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x1:   0100010   34\n",
      "x2: + 1100100   19\n",
      "      -------   --\n",
      "t:  = 1010110   53\n",
      "y:  = 0001000\n",
      "\n",
      "x1:   1010100   21\n",
      "x2: + 1110100   23\n",
      "      -------   --\n",
      "t:  = 0011010   44\n",
      "y:  = 0000001\n",
      "\n",
      "x1:   1111010   47\n",
      "x2: + 0000000    0\n",
      "      -------   --\n",
      "t:  = 1111010   47\n",
      "y:  = 0000010\n",
      "\n",
      "x1:   1000000    1\n",
      "x2: + 1111110   63\n",
      "      -------   --\n",
      "t:  = 0000001   64\n",
      "y:  = 0000000\n",
      "\n",
      "x1:   1010100   21\n",
      "x2: + 1010100   21\n",
      "      -------   --\n",
      "t:  = 0101010   42\n",
      "y:  = 0000001\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create test samples\n",
    "nb_test = 5\n",
    "Xtest, Ttest = create_dataset(nb_test, sequence_len)\n",
    "# Push test data through network\n",
    "Y = RNN.getBinaryOutput(Xtest)\n",
    "Yf = RNN.getOutput(Xtest)\n",
    "\n",
    "# Print out all test examples\n",
    "for i in range(Xtest.shape[0]):\n",
    "    printSample(Xtest[i,:,0], Xtest[i,:,1], Ttest[i,:,:], Y[i,:,:])\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "RNN = RnnBinaryAdder(2, 1, 3, sequence_len)\n",
    "recIn, S, Z, Y = RNN.forward(X_train[:5])\n",
    "gWout, gBout, gWrec, gBrec, gWin, gBin, gS0 = RNN.backward(X_train[:5],\n",
    "                                                           Y, recIn, \n",
    "                                                           S, T_train[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = 1e-14\n",
    "RNN.tensorOutput.W += eps\n",
    "RNN.tensorOutput.b += eps\n",
    "\n",
    "plus_loss = RNN.loss(\n",
    "    RNN.getOutput(X_train[:5]), T_train[:5])\n",
    "\n",
    "# - eps\n",
    "RNN.tensorOutput.W -= 2 * eps\n",
    "RNN.tensorOutput.b -= 2 * eps\n",
    "min_loss = RNN.loss(\n",
    "    RNN.getOutput(X_train[:5]), T_train[:5])\n",
    "\n",
    "# reset param value\n",
    "RNN.tensorOutput.W += eps\n",
    "RNN.tensorOutput.b += eps\n",
    "\n",
    "# calculate numerical gradient\n",
    "grad_num = (plus_loss - min_loss) / (2*eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.21649348980190553"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.721938058664"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plus_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7219380586639956"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.013741886717105772"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gWout.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
