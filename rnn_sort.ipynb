{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import *\n",
    "import sys\n",
    "import itertools\n",
    "import numpy as np  # Matrix and vector computation package\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt  # Plotting library\n",
    "import seaborn as sns  # Fancier plots\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Set seaborn plotting style\n",
    "sns.set_style('darkgrid')\n",
    "# Set the seed for reproducability\n",
    "np.random.seed(seed=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sort_dataset(dataset_length, seq_length, max_number=999, fraction=0.8):\n",
    "    x_train = np.random.randint(low=0, high=max_number+1, size=(int(dataset_length*fraction),\n",
    "                                                                seq_length,\n",
    "                                                                1))\n",
    "    y_train = np.sort(x_train, axis=1)\n",
    "    \n",
    "    x_test = np.random.randint(low=0, high=max_number+1, size=(int(dataset_length*(1-fraction)),\n",
    "                                                               seq_length,\n",
    "                                                               1))\n",
    "    y_test = np.sort(x_test, axis=1)\n",
    "    \n",
    "    return x_train, y_train, x_test, y_test\n",
    "\n",
    "def create_dummy_dataset(dataset_length, seq_length, max_number):\n",
    "    lower_bound = -1 * max_number\n",
    "    x_train = np.random.randint(low=lower_bound, high=max_number+1, size=(dataset_length, seq_length, 1))\n",
    "    y_train = np.where(x_train.sum(axis=2) > 0, 1, 0).reshape(x_train.shape)\n",
    "    \n",
    "    x_test = np.random.randint(low=lower_bound, high=max_number+1, size=(dataset_length, seq_length, 1))\n",
    "    y_test = np.where(x_test.sum(axis=2) > 0, 1, 0).reshape(x_train.shape)\n",
    "    \n",
    "    return x_train, y_train, x_test, y_test\n",
    "\n",
    "def evaluate(y_test, x_test, model):\n",
    "    y_pred = model.predict(x_test)\n",
    "    loss = model.loss(model.predict_proba(x_test), y_test)\n",
    "    \n",
    "    print(f'Cross entropy loss {loss}')\n",
    "    print('-'*100)\n",
    "    print(classification_report(y_test.flatten(), y_pred.flatten()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set hyper-parameters\n",
    "batch_size = 20  # Size of the minibatches (number of samples)\n",
    "max_num = 10 \n",
    "seq_length = 5\n",
    "hidden_size = 10 \n",
    "dataset_size = 200\n",
    "epoch = 4\n",
    "\n",
    "x_train, y_train, x_test, y_test = create_sort_dataset(dataset_size, seq_length, max_num)\n",
    "\n",
    "input_size = 1\n",
    "output_size = max_num+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "TRAIN: Cross entropy loss:  2.686409216799245\n",
      "VALIDATION: Cross entropy loss:  2.599438222561297\n",
      "Epoch 2/4\n",
      "TRAIN: Cross entropy loss:  2.680450815503051\n",
      "VALIDATION: Cross entropy loss:  2.5957300293547694\n",
      "Epoch 3/4\n",
      "TRAIN: Cross entropy loss:  2.6745933586800503\n",
      "VALIDATION: Cross entropy loss:  2.592103804705925\n",
      "Epoch 4/4\n",
      "TRAIN: Cross entropy loss:  2.668835045377737\n",
      "VALIDATION: Cross entropy loss:  2.588557944381883\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create the network\n",
    "model = ModelSort(input_size, output_size, hidden_size, seq_length)\n",
    "\n",
    "train_loss = []\n",
    "validation_loss = []\n",
    "\n",
    "for i in range(epoch):\n",
    "    print(f'Epoch {i+1}/{epoch}')\n",
    "    epoch_train_loss = []\n",
    "    \n",
    "    for mb in range(dataset_size // batch_size):\n",
    "        x_batch = x_train[mb:mb + batch_size]  # Input minibatch\n",
    "        y_batch = y_train[mb:mb + batch_size]  # Target minibatch\n",
    "        model.train_on_batch(x_batch, y_batch)\n",
    "        \n",
    "        loss = model.loss(model.predict_proba(x_batch), y_batch)\n",
    "        epoch_train_loss.append(loss)\n",
    "        \n",
    "\n",
    "    train_loss.append(np.mean(epoch_train_loss))\n",
    "    validation_loss.append(model.loss(model.predict_proba(x_test), y_test))\n",
    "\n",
    "\n",
    "    print(\"TRAIN: Cross entropy loss: \", train_loss[-1])\n",
    "    print(\"VALIDATION: Cross entropy loss: \", validation_loss[-1])\n",
    "\n",
    "\n",
    "\n",
    "# Plot the loss over the iterations\n",
    "fig = plt.figure(figsize=(10, 7))\n",
    "plt.plot(train_loss, 'b-')\n",
    "plt.plot(validation_loss, 'r')\n",
    "plt.xlabel('minibatch iteration')\n",
    "plt.ylabel('Cross entropy loss', fontsize=15)\n",
    "plt.xlim(0, 100)\n",
    "fig.subplots_adjust(bottom=0.2)\n",
    "plt.show()\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross entropy loss 2.588557944381883\n",
      "----------------------------------------------------------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        11\n",
      "           1       0.00      0.00      0.00        19\n",
      "           2       0.13      0.08      0.10        26\n",
      "           3       0.09      0.94      0.17        17\n",
      "           4       0.22      0.13      0.17        15\n",
      "           5       0.00      0.00      0.00        20\n",
      "           6       0.00      0.00      0.00        15\n",
      "           7       0.00      0.00      0.00        17\n",
      "           8       0.00      0.00      0.00        15\n",
      "           9       0.00      0.00      0.00        19\n",
      "          10       0.00      0.00      0.00        21\n",
      "\n",
      "   micro avg       0.10      0.10      0.10       195\n",
      "   macro avg       0.04      0.10      0.04       195\n",
      "weighted avg       0.04      0.10      0.04       195\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/loopdigga/Documents/python3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/loopdigga/Documents/python3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/loopdigga/Documents/python3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "evaluate(y_test, x_test, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter id: 1 Numerical gradient of 0.02630295981020936 is not close to the backpropagation gradient of 0.02630039650337739!\n",
      "Parameter id: 3 Numerical gradient of -0.008459011269223993 is not close to the backpropagation gradient of -0.008459453794921572!\n",
      "Parameter id: 4 Numerical gradient of 0.010448975018562123 is not close to the backpropagation gradient of 0.01044883236963902!\n",
      "Parameter id: 5 Numerical gradient of -0.017619683490011084 is not close to the backpropagation gradient of -0.017615340454290904!\n",
      "Parameter id: 6 Numerical gradient of 0.0021789237081293322 is not close to the backpropagation gradient of 0.0021721104049932647!\n",
      "Parameter id: 7 Numerical gradient of 0.0023507862323413065 is not close to the backpropagation gradient of 0.0023487463431521496!\n",
      "Parameter id: 8 Numerical gradient of -0.019322321520576224 is not close to the backpropagation gradient of -0.019319073806724065!\n",
      "Parameter id: 9 Numerical gradient of 0.01216426959160799 is not close to the backpropagation gradient of 0.01216777188900194!\n",
      "Parameter id: 10 Numerical gradient of -0.010012657369884437 is not close to the backpropagation gradient of -0.010023753269113287!\n",
      "Parameter id: 11 Numerical gradient of 0.0022875035199376725 is not close to the backpropagation gradient of 0.0023050890500154143!\n",
      "Parameter id: 12 Numerical gradient of -0.11328116222841798 is not close to the backpropagation gradient of -0.11323663803771293!\n",
      "Parameter id: 13 Numerical gradient of -0.12842549246272483 is not close to the backpropagation gradient of -0.12839290753819269!\n",
      "Parameter id: 14 Numerical gradient of 0.04745004389405949 is not close to the backpropagation gradient of 0.0474574562187884!\n",
      "Parameter id: 15 Numerical gradient of 0.007036149440864391 is not close to the backpropagation gradient of 0.0070411775789701765!\n",
      "Parameter id: 16 Numerical gradient of -0.007915224031762591 is not close to the backpropagation gradient of -0.007924399986639723!\n",
      "Parameter id: 17 Numerical gradient of 0.011720402426362853 is not close to the backpropagation gradient of 0.011715815809864118!\n",
      "Parameter id: 18 Numerical gradient of -0.05967026872610858 is not close to the backpropagation gradient of -0.059796154683457316!\n",
      "Parameter id: 19 Numerical gradient of 0.014256373859211633 is not close to the backpropagation gradient of 0.014260031463725668!\n",
      "Parameter id: 20 Numerical gradient of -0.009435341397079355 is not close to the backpropagation gradient of -0.009446620354302649!\n",
      "Parameter id: 21 Numerical gradient of -0.007332801033044233 is not close to the backpropagation gradient of -0.0073226595892224335!\n",
      "Parameter id: 22 Numerical gradient of -0.02652811303960334 is not close to the backpropagation gradient of -0.026530951899784817!\n",
      "Parameter id: 23 Numerical gradient of -0.039678482721683395 is not close to the backpropagation gradient of -0.039679521930873596!\n",
      "Parameter id: 24 Numerical gradient of 0.028331337276199516 is not close to the backpropagation gradient of 0.028337674774817082!\n",
      "Parameter id: 25 Numerical gradient of 0.01459166121264843 is not close to the backpropagation gradient of 0.014600341654950284!\n",
      "Parameter id: 26 Numerical gradient of -0.025631496924916064 is not close to the backpropagation gradient of -0.025643462641752483!\n",
      "Parameter id: 27 Numerical gradient of 0.015542012121727565 is not close to the backpropagation gradient of 0.015535792784499024!\n",
      "Parameter id: 28 Numerical gradient of -0.020594637106796654 is not close to the backpropagation gradient of -0.020620909915335734!\n",
      "Parameter id: 29 Numerical gradient of 0.005555778059829208 is not close to the backpropagation gradient of 0.005560531077463099!\n",
      "Parameter id: 30 Numerical gradient of 0.009204637052562248 is not close to the backpropagation gradient of 0.009212943637104959!\n",
      "Parameter id: 31 Numerical gradient of 0.004455991131635528 is not close to the backpropagation gradient of 0.004446081049398608!\n",
      "Parameter id: 32 Numerical gradient of 0.001538547067525542 is not close to the backpropagation gradient of 0.0015406167701921!\n",
      "Parameter id: 33 Numerical gradient of 0.0037314595857651507 is not close to the backpropagation gradient of 0.0037487951852744165!\n",
      "Parameter id: 34 Numerical gradient of -0.0042446046677468985 is not close to the backpropagation gradient of -0.0042485201657955825!\n",
      "Parameter id: 35 Numerical gradient of -0.007008393865248762 is not close to the backpropagation gradient of -0.007012039498541426!\n",
      "Parameter id: 36 Numerical gradient of 0.01191891030316583 is not close to the backpropagation gradient of 0.01192596790288369!\n",
      "Parameter id: 37 Numerical gradient of 0.004310996004619483 is not close to the backpropagation gradient of 0.004312904683152297!\n",
      "Parameter id: 38 Numerical gradient of -0.003690603378458945 is not close to the backpropagation gradient of -0.003663957421142538!\n",
      "Parameter id: 39 Numerical gradient of -0.02734190651665358 is not close to the backpropagation gradient of -0.027346721450670273!\n",
      "Parameter id: 40 Numerical gradient of -0.009388045896230324 is not close to the backpropagation gradient of -0.009395987451802834!\n",
      "Parameter id: 41 Numerical gradient of -0.003770095347022106 is not close to the backpropagation gradient of -0.0037609867152876737!\n",
      "Parameter id: 42 Numerical gradient of 0.00019340085088970227 is not close to the backpropagation gradient of 0.00019036152338563876!\n",
      "Parameter id: 43 Numerical gradient of -0.004278799536905353 is not close to the backpropagation gradient of -0.004292355068188236!\n",
      "Parameter id: 44 Numerical gradient of 0.0034432456885724605 is not close to the backpropagation gradient of 0.003446007160553661!\n",
      "Parameter id: 45 Numerical gradient of 0.007503775378836507 is not close to the backpropagation gradient of 0.00750736804148574!\n",
      "Parameter id: 46 Numerical gradient of -0.012573719843089748 is not close to the backpropagation gradient of -0.012579476493287953!\n",
      "Parameter id: 47 Numerical gradient of -0.004913180973176168 is not close to the backpropagation gradient of -0.0049158662929293705!\n",
      "Parameter id: 48 Numerical gradient of 0.00549937873017825 is not close to the backpropagation gradient of 0.005474204427486666!\n",
      "Parameter id: 49 Numerical gradient of 0.02680744515259903 is not close to the backpropagation gradient of 0.026812311162369283!\n",
      "Parameter id: 50 Numerical gradient of 0.0086843865432229 is not close to the backpropagation gradient of 0.008695021192480954!\n",
      "Parameter id: 51 Numerical gradient of 0.004590328117615172 is not close to the backpropagation gradient of 0.004581072220116809!\n",
      "Parameter id: 52 Numerical gradient of 0.003012701199622825 is not close to the backpropagation gradient of 0.00301464257625362!\n",
      "Parameter id: 53 Numerical gradient of -0.006490807891168515 is not close to the backpropagation gradient of -0.006460682872824319!\n",
      "Parameter id: 54 Numerical gradient of -0.003964162331726584 is not close to the backpropagation gradient of -0.003969187899285042!\n",
      "Parameter id: 55 Numerical gradient of -0.004877875880993088 is not close to the backpropagation gradient of -0.004882512977356778!\n",
      "Parameter id: 56 Numerical gradient of 0.011840528557627295 is not close to the backpropagation gradient of 0.011846958199442589!\n",
      "Parameter id: 57 Numerical gradient of 0.004373834627813267 is not close to the backpropagation gradient of 0.004375852189913829!\n",
      "Parameter id: 58 Numerical gradient of -0.0035258462816045717 is not close to the backpropagation gradient of -0.0035011105628307048!\n",
      "Parameter id: 59 Numerical gradient of -0.03002709192401198 is not close to the backpropagation gradient of -0.030034603778425956!\n",
      "Parameter id: 60 Numerical gradient of 0.0032793767701377874 is not close to the backpropagation gradient of 0.0032912576336391955!\n",
      "Parameter id: 61 Numerical gradient of 0.0022106760866336117 is not close to the backpropagation gradient of 0.002206449719945066!\n",
      "Parameter id: 62 Numerical gradient of 0.0015289991495137656 is not close to the backpropagation gradient of 0.0015303524510618444!\n",
      "Parameter id: 63 Numerical gradient of 0.0008379963389870682 is not close to the backpropagation gradient of 0.000849462403016513!\n",
      "Parameter id: 64 Numerical gradient of -0.002354783035229957 is not close to the backpropagation gradient of -0.0023593040985002683!\n",
      "Parameter id: 65 Numerical gradient of -0.004039657497401095 is not close to the backpropagation gradient of -0.004044201757059712!\n",
      "Parameter id: 66 Numerical gradient of 0.005616618281578667 is not close to the backpropagation gradient of 0.005626500580749545!\n",
      "Parameter id: 67 Numerical gradient of 0.0016331380692236053 is not close to the backpropagation gradient of 0.0016362275176514027!\n",
      "Parameter id: 68 Numerical gradient of -0.0016937562463681388 is not close to the backpropagation gradient of -0.0016797768858489194!\n",
      "Parameter id: 69 Numerical gradient of -0.01429767415572769 is not close to the backpropagation gradient of -0.014317347289679911!\n",
      "Parameter id: 70 Numerical gradient of -0.009960920976936904 is not close to the backpropagation gradient of -0.009970388715511477!\n",
      "Parameter id: 71 Numerical gradient of -0.0044504400165124025 is not close to the backpropagation gradient of -0.00444057914689714!\n",
      "Parameter id: 72 Numerical gradient of -0.0029820590441431705 is not close to the backpropagation gradient of -0.0029843830743954964!\n",
      "Parameter id: 73 Numerical gradient of -0.00023248070135650775 is not close to the backpropagation gradient of -0.00025315525461567893!\n",
      "Parameter id: 74 Numerical gradient of 0.004920952534348544 is not close to the backpropagation gradient of 0.0049259741665608195!\n",
      "Parameter id: 75 Numerical gradient of 0.006181943845717797 is not close to the backpropagation gradient of 0.006185550121304522!\n",
      "Parameter id: 76 Numerical gradient of -0.01183897424539282 is not close to the backpropagation gradient of -0.011845710171815215!\n",
      "Parameter id: 77 Numerical gradient of -0.004011235787970691 is not close to the backpropagation gradient of -0.004012302124727691!\n",
      "Parameter id: 78 Numerical gradient of 0.00272959432834341 is not close to the backpropagation gradient of 0.0027031771540012807!\n",
      "Parameter id: 79 Numerical gradient of 0.02918554287134611 is not close to the backpropagation gradient of 0.02919178343866183!\n",
      "Parameter id: 80 Numerical gradient of -0.010936140881767642 is not close to the backpropagation gradient of -0.010945465794160562!\n",
      "Parameter id: 81 Numerical gradient of -0.004225286787118421 is not close to the backpropagation gradient of -0.0042153252178370165!\n",
      "Parameter id: 82 Numerical gradient of -0.0025799362646239388 is not close to the backpropagation gradient of -0.002581538077796281!\n",
      "Parameter id: 83 Numerical gradient of -0.0020961010704922955 is not close to the backpropagation gradient of -0.00211429836497244!\n",
      "Parameter id: 84 Numerical gradient of 0.004822586774366755 is not close to the backpropagation gradient of 0.004826607963205562!\n",
      "Parameter id: 85 Numerical gradient of 0.006619371717420108 is not close to the backpropagation gradient of 0.006622891854198527!\n",
      "Parameter id: 86 Numerical gradient of -0.012744028055067247 is not close to the backpropagation gradient of -0.012750002966372985!\n",
      "Parameter id: 87 Numerical gradient of -0.004458877711499554 is not close to the backpropagation gradient of -0.004461411429291964!\n",
      "Parameter id: 88 Numerical gradient of 0.0024642510254579975 is not close to the backpropagation gradient of 0.0024377317660590446!\n",
      "Parameter id: 89 Numerical gradient of 0.03063660436453119 is not close to the backpropagation gradient of 0.030643223947870727!\n",
      "Parameter id: 90 Numerical gradient of 0.010217826584835166 is not close to the backpropagation gradient of 0.01022644164758037!\n",
      "Parameter id: 91 Numerical gradient of 0.004335865000371086 is not close to the backpropagation gradient of 0.004325661453840759!\n",
      "Parameter id: 92 Numerical gradient of 0.0025155433291956797 is not close to the backpropagation gradient of 0.0025171354343281544!\n",
      "Parameter id: 93 Numerical gradient of 0.0022022383916464605 is not close to the backpropagation gradient of 0.002220015996697208!\n",
      "Parameter id: 94 Numerical gradient of -0.004586331314726522 is not close to the backpropagation gradient of -0.004590317024558369!\n",
      "Parameter id: 95 Numerical gradient of -0.006468825475280937 is not close to the backpropagation gradient of -0.006472052924426564!\n",
      "Parameter id: 96 Numerical gradient of 0.012219558698234323 is not close to the backpropagation gradient of 0.012225964730497288!\n",
      "Parameter id: 97 Numerical gradient of 0.004154676602752261 is not close to the backpropagation gradient of 0.004156839746529302!\n",
      "Parameter id: 98 Numerical gradient of -0.002781108676686017 is not close to the backpropagation gradient of -0.0027542588417664516!\n",
      "Parameter id: 99 Numerical gradient of -0.029294788816969227 is not close to the backpropagation gradient of -0.029301398264052005!\n",
      "Parameter id: 100 Numerical gradient of 0.010899947611164862 is not close to the backpropagation gradient of 0.010908526904656962!\n",
      "Parameter id: 101 Numerical gradient of 0.00450328663248456 is not close to the backpropagation gradient of 0.0044926603167533535!\n",
      "Parameter id: 102 Numerical gradient of 0.0029827251779579456 is not close to the backpropagation gradient of 0.002984656343813633!\n",
      "Parameter id: 103 Numerical gradient of 0.0008879563750952001 is not close to the backpropagation gradient of 0.0009073229847050782!\n",
      "Parameter id: 104 Numerical gradient of -0.0049118487055466176 is not close to the backpropagation gradient of -0.004916298903100901!\n",
      "Parameter id: 105 Numerical gradient of -0.006612932423877282 is not close to the backpropagation gradient of -0.006616194155503621!\n",
      "Parameter id: 106 Numerical gradient of 0.012540413152350993 is not close to the backpropagation gradient of 0.012546832743067091!\n",
      "Parameter id: 107 Numerical gradient of 0.004342082249308987 is not close to the backpropagation gradient of 0.004344618604438572!\n",
      "Parameter id: 108 Numerical gradient of -0.0025637270084644115 is not close to the backpropagation gradient of -0.0025372501819778412!\n",
      "Parameter id: 109 Numerical gradient of -0.030620839197581514 is not close to the backpropagation gradient of -0.030626416040091062!\n",
      "Parameter id: 110 Numerical gradient of 0.005430766947256416 is not close to the backpropagation gradient of 0.005433768546817293!\n",
      "Parameter id: 111 Numerical gradient of -0.002007727317732133 is not close to the backpropagation gradient of -0.0020113749981558716!\n",
      "Parameter id: 112 Numerical gradient of -0.0020055068716828828 is not close to the backpropagation gradient of -0.0020051574203350632!\n",
      "Parameter id: 113 Numerical gradient of 0.01744737687658926 is not close to the backpropagation gradient of 0.017434923708047178!\n",
      "Parameter id: 114 Numerical gradient of 0.0002902122986370159 is not close to the backpropagation gradient of 0.00029218566080216205!\n",
      "Parameter id: 115 Numerical gradient of -0.005024203275638683 is not close to the backpropagation gradient of -0.005025028967158607!\n",
      "Parameter id: 116 Numerical gradient of 0.001817879180521231 is not close to the backpropagation gradient of 0.0018213046447853268!\n",
      "Parameter id: 117 Numerical gradient of 0.0003006483950684924 is not close to the backpropagation gradient of 0.00030250300880960536!\n",
      "Parameter id: 118 Numerical gradient of -0.006055822510120379 is not close to the backpropagation gradient of -0.006047764424141346!\n",
      "Parameter id: 119 Numerical gradient of -0.005633049582343119 is not close to the backpropagation gradient of -0.005636603820532582!\n",
      "Parameter id: 120 Numerical gradient of 0.009442890913646806 is not close to the backpropagation gradient of 0.009450472375126015!\n",
      "Parameter id: 121 Numerical gradient of 0.003649969215757664 is not close to the backpropagation gradient of 0.0036401712349100153!\n",
      "Parameter id: 122 Numerical gradient of 0.0006037392807911601 is not close to the backpropagation gradient of 0.0006053664666131341!\n",
      "Parameter id: 123 Numerical gradient of 0.002787325925623918 is not close to the backpropagation gradient of 0.0028022363688238097!\n",
      "Parameter id: 124 Numerical gradient of -0.0037352343440488762 is not close to the backpropagation gradient of -0.003737435754075246!\n",
      "Parameter id: 125 Numerical gradient of -0.006836087251826939 is not close to the backpropagation gradient of -0.00683924332476454!\n",
      "Parameter id: 126 Numerical gradient of 0.012002843163827492 is not close to the backpropagation gradient of 0.012007657651591288!\n",
      "Parameter id: 127 Numerical gradient of 0.004423350574711549 is not close to the backpropagation gradient of 0.00442609414268787!\n",
      "Parameter id: 128 Numerical gradient of -0.004813482945564829 is not close to the backpropagation gradient of -0.004789807074315942!\n",
      "Parameter id: 129 Numerical gradient of -0.02679989563603158 is not close to the backpropagation gradient of -0.026804181983549072!\n",
      "Parameter id: 130 Numerical gradient of -0.009434897307869505 is not close to the backpropagation gradient of -0.009446620354302653!\n",
      "Parameter id: 131 Numerical gradient of -0.007332801033044233 is not close to the backpropagation gradient of -0.007322659589222428!\n",
      "Parameter id: 132 Numerical gradient of -0.02652811303960334 is not close to the backpropagation gradient of -0.02653095189978479!\n",
      "Parameter id: 133 Numerical gradient of -0.039678482721683395 is not close to the backpropagation gradient of -0.03967952193087362!\n",
      "Parameter id: 134 Numerical gradient of 0.028331337276199516 is not close to the backpropagation gradient of 0.028337674774817124!\n",
      "Parameter id: 135 Numerical gradient of 0.01459166121264843 is not close to the backpropagation gradient of 0.014600341654950274!\n",
      "Parameter id: 136 Numerical gradient of -0.025631496924916064 is not close to the backpropagation gradient of -0.02564346264175249!\n",
      "Parameter id: 137 Numerical gradient of 0.015542012121727565 is not close to the backpropagation gradient of 0.015535792784499035!\n",
      "Parameter id: 138 Numerical gradient of -0.020594637106796654 is not close to the backpropagation gradient of -0.020620909915335734!\n",
      "Parameter id: 139 Numerical gradient of 0.005555778059829208 is not close to the backpropagation gradient of 0.005560531077463082!\n",
      "Parameter id: 140 Numerical gradient of 0.05564060323592912 is not close to the backpropagation gradient of 0.05565948901879875!\n",
      "Parameter id: 141 Numerical gradient of -0.02316835612248269 is not close to the backpropagation gradient of -0.02316162592755255!\n",
      "Parameter id: 142 Numerical gradient of 0.06316924761051723 is not close to the backpropagation gradient of 0.0631904127912105!\n",
      "Parameter id: 143 Numerical gradient of -0.08887246494282408 is not close to the backpropagation gradient of -0.08894305418869164!\n",
      "Parameter id: 144 Numerical gradient of 0.05617262210932949 is not close to the backpropagation gradient of 0.05619669060948739!\n",
      "Parameter id: 145 Numerical gradient of 0.04253108976115527 is not close to the backpropagation gradient of 0.042562672823733036!\n",
      "Parameter id: 146 Numerical gradient of 0.03443090257349013 is not close to the backpropagation gradient of 0.03445981732565325!\n",
      "Parameter id: 147 Numerical gradient of -0.06062528257189114 is not close to the backpropagation gradient of -0.06065243483389332!\n",
      "Parameter id: 148 Numerical gradient of -0.10488121482410406 is not close to the backpropagation gradient of -0.1049763601225758!\n",
      "Parameter id: 149 Numerical gradient of 0.025067947717616335 is not close to the backpropagation gradient of 0.025098877102083268!\n",
      "Parameter id: 150 Numerical gradient of 0.0005355715870791755 is not close to the backpropagation gradient of 0.0005655154017470722!\n",
      "Parameter id: 151 Numerical gradient of -0.05050981854992642 is not close to the backpropagation gradient of -0.0505267301279994!\n",
      "Parameter id: 152 Numerical gradient of 0.012768452961609 is not close to the backpropagation gradient of 0.012761727463427259!\n",
      "Parameter id: 153 Numerical gradient of -0.054677373739764334 is not close to the backpropagation gradient of -0.05470022702366674!\n",
      "Parameter id: 154 Numerical gradient of 0.08097300607801117 is not close to the backpropagation gradient of 0.08103852711772433!\n",
      "Parameter id: 155 Numerical gradient of -0.051642023990439156 is not close to the backpropagation gradient of -0.05166604938445116!\n",
      "Parameter id: 156 Numerical gradient of -0.040671910284117985 is not close to the backpropagation gradient of -0.04070224928849267!\n",
      "Parameter id: 157 Numerical gradient of -0.02786415542743725 is not close to the backpropagation gradient of -0.027892885530377682!\n",
      "Parameter id: 158 Numerical gradient of 0.059060090151774595 is not close to the backpropagation gradient of 0.05908679621909531!\n",
      "Parameter id: 159 Numerical gradient of 0.09871614636836057 is not close to the backpropagation gradient of 0.09881047427595774!\n",
      "Parameter id: 160 Numerical gradient of -0.021292523300076027 is not close to the backpropagation gradient of -0.021323554676020907!\n",
      "Parameter id: 161 Numerical gradient of -0.00485811391115476 is not close to the backpropagation gradient of -0.004885829045196046!\n",
      "Parameter id: 162 Numerical gradient of 0.03513211943584338 is not close to the backpropagation gradient of 0.03517528268265079!\n",
      "Parameter id: 163 Numerical gradient of -0.052057691490858815 is not close to the backpropagation gradient of -0.052046974873202506!\n",
      "Parameter id: 164 Numerical gradient of 0.050495829739816145 is not close to the backpropagation gradient of 0.050530419883398944!\n",
      "Parameter id: 165 Numerical gradient of -0.09014811119811839 is not close to the backpropagation gradient of -0.09023083229864856!\n",
      "Parameter id: 166 Numerical gradient of 0.05352096543731477 is not close to the backpropagation gradient of 0.05354870943511278!\n",
      "Parameter id: 167 Numerical gradient of 0.04003575249100777 is not close to the backpropagation gradient of 0.04006818588633483!\n",
      "Parameter id: 168 Numerical gradient of 0.04260991559590366 is not close to the backpropagation gradient of 0.04263908233963674!\n",
      "Parameter id: 169 Numerical gradient of -0.04244271600839511 is not close to the backpropagation gradient of -0.04248425780370847!\n",
      "Parameter id: 170 Numerical gradient of -0.08020228925431638 is not close to the backpropagation gradient of -0.08030871268238776!\n",
      "Parameter id: 171 Numerical gradient of 0.0345841133508884 is not close to the backpropagation gradient of 0.0346107952154467!\n",
      "Parameter id: 172 Numerical gradient of 0.008473222123939195 is not close to the backpropagation gradient of 0.008498302215366474!\n",
      "Parameter id: 173 Numerical gradient of 0.03835309847488588 is not close to the backpropagation gradient of 0.03839171036231856!\n",
      "Parameter id: 174 Numerical gradient of -0.01613442712766755 is not close to the backpropagation gradient of -0.016142619642849472!\n",
      "Parameter id: 175 Numerical gradient of 0.03972955298081615 is not close to the backpropagation gradient of 0.039765863837008295!\n",
      "Parameter id: 176 Numerical gradient of -0.053935966803919655 is not close to the backpropagation gradient of -0.054023606108417124!\n",
      "Parameter id: 177 Numerical gradient of 0.025214053067657005 is not close to the backpropagation gradient of 0.025249819539125785!\n",
      "Parameter id: 178 Numerical gradient of 0.02486078010122128 is not close to the backpropagation gradient of 0.024898137455493588!\n",
      "Parameter id: 179 Numerical gradient of 0.020144108603403765 is not close to the backpropagation gradient of 0.020178208627420983!\n",
      "Parameter id: 180 Numerical gradient of -0.03142819338108893 is not close to the backpropagation gradient of -0.03147255398035557!\n",
      "Parameter id: 181 Numerical gradient of -0.05960187898779167 is not close to the backpropagation gradient of -0.0597059590086539!\n",
      "Parameter id: 182 Numerical gradient of 0.011670664434859646 is not close to the backpropagation gradient of 0.011707507350485185!\n",
      "Parameter id: 183 Numerical gradient of 0.0011286527268339341 is not close to the backpropagation gradient of 0.0011534915684237026!\n",
      "Parameter id: 184 Numerical gradient of -0.05501243904859621 is not close to the backpropagation gradient of -0.05503501154045493!\n",
      "Parameter id: 185 Numerical gradient of 0.03908739998337296 is not close to the backpropagation gradient of 0.03908541993397032!\n",
      "Parameter id: 186 Numerical gradient of -0.06393641172053321 is not close to the backpropagation gradient of -0.06395986663824618!\n",
      "Parameter id: 187 Numerical gradient of 0.09616085705488331 is not close to the backpropagation gradient of 0.09623748435108281!\n",
      "Parameter id: 188 Numerical gradient of -0.054674709204505234 is not close to the backpropagation gradient of -0.05470129461299433!\n",
      "Parameter id: 189 Numerical gradient of -0.042659209498197015 is not close to the backpropagation gradient of -0.042691489389810114!\n",
      "Parameter id: 190 Numerical gradient of -0.03964162331726584 is not close to the backpropagation gradient of -0.03967351830727877!\n",
      "Parameter id: 191 Numerical gradient of 0.05468159258725791 is not close to the backpropagation gradient of 0.054710627235387846!\n",
      "Parameter id: 192 Numerical gradient of 0.09856249150175245 is not close to the backpropagation gradient of 0.09865862104528839!\n",
      "Parameter id: 193 Numerical gradient of -0.029842794901924204 is not close to the backpropagation gradient of -0.029875088800359276!\n",
      "Parameter id: 194 Numerical gradient of -0.0027251534362449092 is not close to the backpropagation gradient of -0.0027558832765857705!\n",
      "Parameter id: 195 Numerical gradient of -0.05566280769642162 is not close to the backpropagation gradient of -0.055683763070908075!\n",
      "Parameter id: 196 Numerical gradient of 0.03723021890778 is not close to the backpropagation gradient of 0.03722686559371307!\n",
      "Parameter id: 197 Numerical gradient of -0.06503420024728257 is not close to the backpropagation gradient of -0.06505874612231868!\n",
      "Parameter id: 198 Numerical gradient of 0.09674039347373764 is not close to the backpropagation gradient of 0.09681865943173626!\n",
      "Parameter id: 199 Numerical gradient of -0.05899969401923499 is not close to the backpropagation gradient of -0.059025209872631844!\n",
      "Parameter id: 200 Numerical gradient of -0.041765924052583614 is not close to the backpropagation gradient of -0.04179956965617611!\n",
      "Parameter id: 201 Numerical gradient of -0.040349279473161914 is not close to the backpropagation gradient of -0.04038185255249946!\n",
      "Parameter id: 202 Numerical gradient of 0.06119726947417802 is not close to the backpropagation gradient of 0.061226041075125234!\n",
      "Parameter id: 203 Numerical gradient of 0.10258438543075954 is not close to the backpropagation gradient of 0.10268351660543044!\n",
      "Parameter id: 204 Numerical gradient of -0.030177416121546227 is not close to the backpropagation gradient of -0.030210735650574125!\n",
      "Parameter id: 205 Numerical gradient of -0.005763167720829188 is not close to the backpropagation gradient of -0.005795205780896788!\n",
      "Parameter id: 206 Numerical gradient of 0.05737299524355421 is not close to the backpropagation gradient of 0.057393747234225016!\n",
      "Parameter id: 207 Numerical gradient of -0.03339262200086068 is not close to the backpropagation gradient of -0.033387762685493996!\n",
      "Parameter id: 208 Numerical gradient of 0.06553668718822792 is not close to the backpropagation gradient of 0.06555954807190525!\n",
      "Parameter id: 209 Numerical gradient of -0.09496892161564574 is not close to the backpropagation gradient of -0.09504472961836447!\n",
      "Parameter id: 210 Numerical gradient of 0.05807354597209268 is not close to the backpropagation gradient of 0.05809870609228099!\n",
      "Parameter id: 211 Numerical gradient of 0.04262279418298931 is not close to the backpropagation gradient of 0.04265560453807695!\n",
      "Parameter id: 212 Numerical gradient of 0.03881828192220382 is not close to the backpropagation gradient of 0.0388490938503261!\n",
      "Parameter id: 213 Numerical gradient of -0.060087934627972565 is not close to the backpropagation gradient of -0.06011615992844789!\n",
      "Parameter id: 214 Numerical gradient of -0.1034134999855496 is not close to the backpropagation gradient of -0.10351034745905871!\n",
      "Parameter id: 215 Numerical gradient of 0.028226310178069976 is not close to the backpropagation gradient of 0.02825861400761347!\n",
      "Parameter id: 216 Numerical gradient of 0.0012119194536808209 is not close to the backpropagation gradient of 0.0012436858969372949!\n",
      "Parameter id: 217 Numerical gradient of 0.054813265037978454 is not close to the backpropagation gradient of 0.05483208146854525!\n",
      "Parameter id: 218 Numerical gradient of -0.03952016491837185 is not close to the backpropagation gradient of -0.03951750761808985!\n",
      "Parameter id: 219 Numerical gradient of 0.06434919264108885 is not close to the backpropagation gradient of 0.0643727120042338!\n",
      "Parameter id: 220 Numerical gradient of -0.09724687721757164 is not close to the backpropagation gradient of -0.09732422360437652!\n",
      "Parameter id: 221 Numerical gradient of 0.05806199965263658 is not close to the backpropagation gradient of 0.05808755374355628!\n",
      "Parameter id: 222 Numerical gradient of 0.04242450835079126 is not close to the backpropagation gradient of 0.04245821839936405!\n",
      "Parameter id: 223 Numerical gradient of 0.040930148159645796 is not close to the backpropagation gradient of 0.04096246063276933!\n",
      "Parameter id: 224 Numerical gradient of -0.05892664134421465 is not close to the backpropagation gradient of -0.0589543073281366!\n",
      "Parameter id: 225 Numerical gradient of -0.10130918326467508 is not close to the backpropagation gradient of -0.10140628826711792!\n",
      "Parameter id: 226 Numerical gradient of 0.031017188817372695 is not close to the backpropagation gradient of 0.031050429047679042!\n",
      "Parameter id: 227 Numerical gradient of 0.005406119996109737 is not close to the backpropagation gradient of 0.005438871521573045!\n",
      "Parameter id: 228 Numerical gradient of 0.04988010005035903 is not close to the backpropagation gradient of 0.04989150917388251!\n",
      "Parameter id: 229 Numerical gradient of 0.04327160851858025 is not close to the backpropagation gradient of 0.04328104933579019!\n",
      "Parameter id: 230 Numerical gradient of 0.0234114949648756 is not close to the backpropagation gradient of 0.023420691815616995!\n",
      "Parameter id: 231 Numerical gradient of -0.026609603409610827 is not close to the backpropagation gradient of -0.026634340569862962!\n",
      "Parameter id: 232 Numerical gradient of 0.00013433698597964394 is not close to the backpropagation gradient of 0.000141657298632053!\n",
      "Parameter id: 233 Numerical gradient of 0.011602052651937811 is not close to the backpropagation gradient of 0.011613662917430973!\n",
      "Parameter id: 234 Numerical gradient of -0.013764323014697766 is not close to the backpropagation gradient of -0.013750380315138703!\n",
      "Parameter id: 235 Numerical gradient of -0.040612624374603 is not close to the backpropagation gradient of -0.04062760976896061!\n",
      "Parameter id: 236 Numerical gradient of -0.055929705311541504 is not close to the backpropagation gradient of -0.05597765109738512!\n",
      "Parameter id: 237 Numerical gradient of 0.0073852035598065404 is not close to the backpropagation gradient of 0.007399059479823194!\n",
      "Parameter id: 238 Numerical gradient of 0.0012312373343092986 is not close to the backpropagation gradient of 0.0012423517301714277!\n",
      "Parameter id: 239 Numerical gradient of 0.05091349564168013 is not close to the backpropagation gradient of 0.0509307513519032!\n",
      "Parameter id: 240 Numerical gradient of -0.01972511043391023 is not close to the backpropagation gradient of -0.019716020612423977!\n",
      "Parameter id: 241 Numerical gradient of 0.05520717216711546 is not close to the backpropagation gradient of 0.055224833997837036!\n",
      "Parameter id: 242 Numerical gradient of -0.08243272731078832 is not close to the backpropagation gradient of -0.08249482648038972!\n",
      "Parameter id: 243 Numerical gradient of 0.049492188125555 is not close to the backpropagation gradient of 0.04951163576844816!\n",
      "Parameter id: 244 Numerical gradient of 0.03976885487588788 is not close to the backpropagation gradient of 0.039796161336392785!\n",
      "Parameter id: 245 Numerical gradient of 0.028822944031503536 is not close to the backpropagation gradient of 0.02884871587516212!\n",
      "Parameter id: 246 Numerical gradient of -0.055204285587251434 is not close to the backpropagation gradient of -0.0552285584776303!\n",
      "Parameter id: 247 Numerical gradient of -0.09427436609144024 is not close to the backpropagation gradient of -0.09436295320460011!\n",
      "Parameter id: 248 Numerical gradient of 0.02356981276818715 is not close to the backpropagation gradient of 0.023598860021344672!\n",
      "Parameter id: 249 Numerical gradient of 0.003862465902670919 is not close to the backpropagation gradient of 0.0038914004239561754!\n",
      "Parameter id: 250 Numerical gradient of -0.08212053259626373 is not close to the backpropagation gradient of -0.08214217419619087!\n",
      "Parameter id: 251 Numerical gradient of 0.041059822208922014 is not close to the backpropagation gradient of 0.04105779740038739!\n",
      "Parameter id: 252 Numerical gradient of -0.0645536957222248 is not close to the backpropagation gradient of -0.06457856463276429!\n",
      "Parameter id: 253 Numerical gradient of 0.1037079311316802 is not close to the backpropagation gradient of 0.10378899168970955!\n",
      "Parameter id: 254 Numerical gradient of -0.056007865012475115 is not close to the backpropagation gradient of -0.056034772466546943!\n",
      "Parameter id: 255 Numerical gradient of -0.03797206993283453 is not close to the backpropagation gradient of -0.03800712545545263!\n",
      "Parameter id: 256 Numerical gradient of -0.040468961515216506 is not close to the backpropagation gradient of -0.04050144326640129!\n",
      "Parameter id: 257 Numerical gradient of 0.06578870781481783 is not close to the backpropagation gradient of 0.06581797740437674!\n",
      "Parameter id: 258 Numerical gradient of 0.1065376675768448 is not close to the backpropagation gradient of 0.10663819967149908!\n",
      "Parameter id: 259 Numerical gradient of -0.031094016250676756 is not close to the backpropagation gradient of -0.031127747043094914!\n",
      "Parameter id: 260 Numerical gradient of -0.004878542014807863 is not close to the backpropagation gradient of -0.0049111391055217495!\n"
     ]
    }
   ],
   "source": [
    "# Do gradient checking\n",
    "model = ModelSort(input_size, output_size, hidden_size, seq_length)\n",
    "# Get the gradients of the parameters from a subset of the data\n",
    "backprop_grads = model.get_gradients(\n",
    "    x_train[:100], y_train[:100])\n",
    "\n",
    "eps = 1e-9  # Set the small change to compute the numerical gradient\n",
    "# Compute the numerical gradients of the parameters in all layers.\n",
    "for p_idx, param in enumerate(model.get_params_iter()):\n",
    "    grad_backprop = backprop_grads[p_idx]\n",
    "    \n",
    "    # + eps\n",
    "    param += eps\n",
    "    plus_loss = model.loss(\n",
    "        model.predict_proba(x_train[0:100,:,:]), y_train[0:100,:,:])\n",
    "    \n",
    "    # - eps\n",
    "    param -= 2 * eps\n",
    "    min_loss = model.loss(\n",
    "        model.predict_proba(x_train[0:100,:,:]), y_train[0:100,:,:])\n",
    "    \n",
    "    # reset param value\n",
    "    param += eps\n",
    "    \n",
    "    # calculate numerical gradient\n",
    "    grad_num = (plus_loss - min_loss) / (2*eps)\n",
    "    numerator = np.linalg.norm(grad_backprop - grad_num)\n",
    "    denominator = np.linalg.norm(grad_backprop) + np.linalg.norm(grad_num)\n",
    "    difference = numerator / denominator\n",
    "    \n",
    "    if not np.isclose(grad_num, grad_backprop, ):\n",
    "        print((\n",
    "            f'Parameter id: {p_idx} '\n",
    "            f'Numerical gradient of {grad_num} is not close '\n",
    "            f'to the backpropagation gradient of {grad_backprop}!'\n",
    "        ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
