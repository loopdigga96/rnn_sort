{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import *\n",
    "import sys\n",
    "import itertools\n",
    "import numpy as np  # Matrix and vector computation package\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt  # Plotting library\n",
    "import seaborn as sns  # Fancier plots\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Set seaborn plotting style\n",
    "sns.set_style('darkgrid')\n",
    "# Set the seed for reproducability\n",
    "np.random.seed(seed=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sort_dataset(dataset_length, seq_length, max_number=999, fraction=0.8):\n",
    "    x_train = np.random.randint(low=0, high=max_number+1, size=(int(dataset_length*fraction),\n",
    "                                                                seq_length,\n",
    "                                                                1))\n",
    "    y_train = np.sort(x_train, axis=1)\n",
    "    \n",
    "    x_test = np.random.randint(low=0, high=max_number+1, size=(int(dataset_length*(1-fraction)),\n",
    "                                                               seq_length,\n",
    "                                                               1))\n",
    "    y_test = np.sort(x_test, axis=1)\n",
    "    \n",
    "    return x_train, y_train, x_test, y_test\n",
    "\n",
    "def create_dummy_dataset(dataset_length, seq_length, max_number):\n",
    "    lower_bound = -1 * max_number\n",
    "    x_train = np.random.randint(low=lower_bound, high=max_number+1, size=(dataset_length, seq_length, 1))\n",
    "    y_train = np.where(x_train.sum(axis=2) > 0, 1, 0).reshape(x_train.shape)\n",
    "    \n",
    "    x_test = np.random.randint(low=lower_bound, high=max_number+1, size=(dataset_length, seq_length, 1))\n",
    "    y_test = np.where(x_test.sum(axis=2) > 0, 1, 0).reshape(x_train.shape)\n",
    "    \n",
    "    return x_train, y_train, x_test, y_test\n",
    "\n",
    "def evaluate(y_test, x_test, model):\n",
    "    y_pred = model.predict(x_test)\n",
    "    loss = model.loss(model.predict_proba(x_test), y_test)\n",
    "    \n",
    "    print(f'Cross entropy loss {loss}')\n",
    "    print('-'*100)\n",
    "    print(classification_report(y_test.flatten(), y_pred.flatten()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set hyper-parameters\n",
    "batch_size = 20  # Size of the minibatches (number of samples)\n",
    "max_num = 10 \n",
    "seq_length = 5\n",
    "hidden_size = 10 \n",
    "dataset_size = 200\n",
    "epoch = 4\n",
    "\n",
    "x_train, y_train, x_test, y_test = create_sort_dataset(dataset_size, seq_length, max_num)\n",
    "\n",
    "input_size = 1\n",
    "output_size = max_num+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "TRAIN: Cross entropy loss:  2.686409216799245\n",
      "VALIDATION: Cross entropy loss:  2.599438222561297\n",
      "Epoch 2/4\n",
      "TRAIN: Cross entropy loss:  2.680450815503051\n",
      "VALIDATION: Cross entropy loss:  2.5957300293547694\n",
      "Epoch 3/4\n",
      "TRAIN: Cross entropy loss:  2.6745933586800503\n",
      "VALIDATION: Cross entropy loss:  2.592103804705925\n",
      "Epoch 4/4\n",
      "TRAIN: Cross entropy loss:  2.668835045377737\n",
      "VALIDATION: Cross entropy loss:  2.588557944381883\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create the network\n",
    "model = ModelSort(input_size, output_size, hidden_size, seq_length)\n",
    "\n",
    "train_loss = []\n",
    "validation_loss = []\n",
    "\n",
    "for i in range(epoch):\n",
    "    print(f'Epoch {i+1}/{epoch}')\n",
    "    epoch_train_loss = []\n",
    "    \n",
    "    for mb in range(dataset_size // batch_size):\n",
    "        x_batch = x_train[mb:mb + batch_size]  # Input minibatch\n",
    "        y_batch = y_train[mb:mb + batch_size]  # Target minibatch\n",
    "        model.train_on_batch(x_batch, y_batch)\n",
    "        \n",
    "        loss = model.loss(model.predict_proba(x_batch), y_batch)\n",
    "        epoch_train_loss.append(loss)\n",
    "        \n",
    "\n",
    "    train_loss.append(np.mean(epoch_train_loss))\n",
    "    validation_loss.append(model.loss(model.predict_proba(x_test), y_test))\n",
    "\n",
    "\n",
    "    print(\"TRAIN: Cross entropy loss: \", train_loss[-1])\n",
    "    print(\"VALIDATION: Cross entropy loss: \", validation_loss[-1])\n",
    "\n",
    "\n",
    "\n",
    "# Plot the loss over the iterations\n",
    "fig = plt.figure(figsize=(10, 7))\n",
    "plt.plot(train_loss, 'b-')\n",
    "plt.plot(validation_loss, 'r')\n",
    "plt.xlabel('minibatch iteration')\n",
    "plt.ylabel('Cross entropy loss', fontsize=15)\n",
    "plt.xlim(0, 100)\n",
    "fig.subplots_adjust(bottom=0.2)\n",
    "plt.show()\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross entropy loss 2.588557944381883\n",
      "----------------------------------------------------------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        11\n",
      "           1       0.00      0.00      0.00        19\n",
      "           2       0.13      0.08      0.10        26\n",
      "           3       0.09      0.94      0.17        17\n",
      "           4       0.22      0.13      0.17        15\n",
      "           5       0.00      0.00      0.00        20\n",
      "           6       0.00      0.00      0.00        15\n",
      "           7       0.00      0.00      0.00        17\n",
      "           8       0.00      0.00      0.00        15\n",
      "           9       0.00      0.00      0.00        19\n",
      "          10       0.00      0.00      0.00        21\n",
      "\n",
      "   micro avg       0.10      0.10      0.10       195\n",
      "   macro avg       0.04      0.10      0.04       195\n",
      "weighted avg       0.04      0.10      0.04       195\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vlad/Documents/envs/p3.6/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "evaluate(y_test, x_test, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter id: 0 Numerical gradient of 0.003204325693673127 is not close to the backpropagation gradient of 0.0032064649328820546!\n",
      "Parameter id: 1 Numerical gradient of 0.025361490685327226 is not close to the backpropagation gradient of 0.025370946943887387!\n",
      "Parameter id: 2 Numerical gradient of -0.007058353901356894 is not close to the backpropagation gradient of -0.007078366740596054!\n",
      "Parameter id: 3 Numerical gradient of 0.009889866703360894 is not close to the backpropagation gradient of 0.009914438963294955!\n",
      "Parameter id: 4 Numerical gradient of -0.012827516826519059 is not close to the backpropagation gradient of -0.012842681722661115!\n",
      "Parameter id: 5 Numerical gradient of 0.013289591649368049 is not close to the backpropagation gradient of 0.01331278286493788!\n",
      "Parameter id: 6 Numerical gradient of 0.03255928859857704 is not close to the backpropagation gradient of 0.03258356470269778!\n",
      "Parameter id: 7 Numerical gradient of 0.014646728274669838 is not close to the backpropagation gradient of 0.014661835844430267!\n",
      "Parameter id: 8 Numerical gradient of -0.007942757562773295 is not close to the backpropagation gradient of -0.007962663722056734!\n",
      "Parameter id: 9 Numerical gradient of -0.026114443940628007 is not close to the backpropagation gradient of -0.026124751532399914!\n",
      "Parameter id: 10 Numerical gradient of -0.17627610482406908 is not close to the backpropagation gradient of -0.17687447991112296!\n",
      "Parameter id: 11 Numerical gradient of 0.010844658504538529 is not close to the backpropagation gradient of 0.01092058538305312!\n",
      "Parameter id: 12 Numerical gradient of -0.06039013733527553 is not close to the backpropagation gradient of -0.0604309394242464!\n",
      "Parameter id: 13 Numerical gradient of -0.0028104185645361213 is not close to the backpropagation gradient of -0.0028244343856940078!\n",
      "Parameter id: 14 Numerical gradient of 0.38302894189712333 is not close to the backpropagation gradient of 0.384822630033942!\n",
      "Parameter id: 15 Numerical gradient of 0.007593703443831145 is not close to the backpropagation gradient of 0.007624724032983582!\n",
      "Parameter id: 16 Numerical gradient of -0.023090862555363856 is not close to the backpropagation gradient of -0.023077385364862667!\n",
      "Parameter id: 17 Numerical gradient of 0.011606937633246162 is not close to the backpropagation gradient of 0.011582275346952076!\n",
      "Parameter id: 18 Numerical gradient of -0.0032247537973262297 is not close to the backpropagation gradient of -0.0032902700291281292!\n",
      "Parameter id: 19 Numerical gradient of -0.03129052572603541 is not close to the backpropagation gradient of -0.031358637198661915!\n",
      "Parameter id: 20 Numerical gradient of -0.04065592307256338 is not close to the backpropagation gradient of -0.040750880216354014!\n",
      "Parameter id: 21 Numerical gradient of 0.0007427392034742297 is not close to the backpropagation gradient of 0.0007890001349752974!\n",
      "Parameter id: 22 Numerical gradient of 0.0003994582442601313 is not close to the backpropagation gradient of 0.00038644595927237927!\n",
      "Parameter id: 23 Numerical gradient of -0.0161268776111001 is not close to the backpropagation gradient of -0.01612328056449978!\n",
      "Parameter id: 24 Numerical gradient of 0.07437295224121954 is not close to the backpropagation gradient of 0.07462039502065182!\n",
      "Parameter id: 25 Numerical gradient of 0.012027268070369246 is not close to the backpropagation gradient of 0.01205430591889214!\n",
      "Parameter id: 26 Numerical gradient of -0.006140199459991891 is not close to the backpropagation gradient of -0.006119049914539023!\n",
      "Parameter id: 27 Numerical gradient of -0.019708457088540854 is not close to the backpropagation gradient of -0.019706070607575482!\n",
      "Parameter id: 28 Numerical gradient of -0.016561418902938385 is not close to the backpropagation gradient of -0.016603678798973995!\n",
      "Parameter id: 29 Numerical gradient of -0.009522826971419818 is not close to the backpropagation gradient of -0.009552896721746642!\n",
      "Parameter id: 30 Numerical gradient of 0.01266564630952871 is not close to the backpropagation gradient of 0.012693304503608316!\n",
      "Parameter id: 31 Numerical gradient of -0.005863531882255302 is not close to the backpropagation gradient of -0.005880617750274563!\n",
      "Parameter id: 32 Numerical gradient of 0.006036282584886976 is not close to the backpropagation gradient of 0.006053761821479085!\n",
      "Parameter id: 33 Numerical gradient of 0.0029769520182298947 is not close to the backpropagation gradient of 0.0029652652690264395!\n",
      "Parameter id: 34 Numerical gradient of -0.03791522651397372 is not close to the backpropagation gradient of -0.03803108605066984!\n",
      "Parameter id: 35 Numerical gradient of -0.003112621271839089 is not close to the backpropagation gradient of -0.003117299897907246!\n",
      "Parameter id: 36 Numerical gradient of 0.007430944748421097 is not close to the backpropagation gradient of 0.0074196769887202646!\n",
      "Parameter id: 37 Numerical gradient of -0.003713473972766223 is not close to the backpropagation gradient of -0.0037076470933958636!\n",
      "Parameter id: 38 Numerical gradient of -0.001135314064981685 is not close to the backpropagation gradient of -0.0011232480301857205!\n",
      "Parameter id: 39 Numerical gradient of 0.0057274185394362576 is not close to the backpropagation gradient of 0.0057357811763879205!\n",
      "Parameter id: 40 Numerical gradient of -0.025265789460604537 is not close to the backpropagation gradient of -0.025303404302530252!\n",
      "Parameter id: 41 Numerical gradient of 0.006339373470609644 is not close to the backpropagation gradient of 0.006374338362742928!\n",
      "Parameter id: 42 Numerical gradient of -0.014779288903810082 is not close to the backpropagation gradient of -0.014782999434558236!\n",
      "Parameter id: 43 Numerical gradient of 0.0063842264808045 is not close to the backpropagation gradient of 0.0063798398222355834!\n",
      "Parameter id: 44 Numerical gradient of 0.034217073618947325 is not close to the backpropagation gradient of 0.03437470184700625!\n",
      "Parameter id: 45 Numerical gradient of 0.003861799768856144 is not close to the backpropagation gradient of 0.003878036693504282!\n",
      "Parameter id: 46 Numerical gradient of 0.004036770917537069 is not close to the backpropagation gradient of 0.004040200762932546!\n",
      "Parameter id: 47 Numerical gradient of 0.00042943426592501055 is not close to the backpropagation gradient of 0.00042786232072143833!\n",
      "Parameter id: 48 Numerical gradient of -0.004678701870375335 is not close to the backpropagation gradient of -0.004704287140886136!\n",
      "Parameter id: 49 Numerical gradient of 0.0025910384948701903 is not close to the backpropagation gradient of 0.0025698153178958356!\n",
      "Parameter id: 50 Numerical gradient of 0.027905899813163156 is not close to the backpropagation gradient of 0.027934589734810264!\n",
      "Parameter id: 51 Numerical gradient of -0.006690648035601043 is not close to the backpropagation gradient of -0.0067268735487045605!\n",
      "Parameter id: 52 Numerical gradient of 0.016053380846869914 is not close to the backpropagation gradient of 0.016055725587591562!\n",
      "Parameter id: 53 Numerical gradient of -0.008306022536430646 is not close to the backpropagation gradient of -0.008312858529308314!\n",
      "Parameter id: 54 Numerical gradient of -0.03264433168226333 is not close to the backpropagation gradient of -0.032768956171506276!\n",
      "Parameter id: 55 Numerical gradient of -0.0035687008903551027 is not close to the backpropagation gradient of -0.00358286527545833!\n",
      "Parameter id: 56 Numerical gradient of -0.004594991054318598 is not close to the backpropagation gradient of -0.00461343455072361!\n",
      "Parameter id: 57 Numerical gradient of 0.0008819611707622242 is not close to the backpropagation gradient of 0.0008883544276628722!\n",
      "Parameter id: 58 Numerical gradient of 0.0046564974098828316 is not close to the backpropagation gradient of 0.004688827813607507!\n",
      "Parameter id: 59 Numerical gradient of -0.0025903723610554152 is not close to the backpropagation gradient of -0.0025722022843025225!\n",
      "Parameter id: 60 Numerical gradient of 0.030039526421887782 is not close to the backpropagation gradient of 0.030074324906319953!\n",
      "Parameter id: 61 Numerical gradient of -0.006439737632035758 is not close to the backpropagation gradient of -0.006477653277697653!\n",
      "Parameter id: 62 Numerical gradient of 0.016288304038880597 is not close to the backpropagation gradient of 0.01629198653355076!\n",
      "Parameter id: 63 Numerical gradient of -0.007564615600585966 is not close to the backpropagation gradient of -0.0075664842777040785!\n",
      "Parameter id: 64 Numerical gradient of -0.034424463279947304 is not close to the backpropagation gradient of -0.034566630960651226!\n",
      "Parameter id: 65 Numerical gradient of -0.0036093350530563835 is not close to the backpropagation gradient of -0.0036248167976100578!\n",
      "Parameter id: 66 Numerical gradient of -0.003627320666055311 is not close to the backpropagation gradient of -0.003639368433713564!\n",
      "Parameter id: 67 Numerical gradient of 0.00081556983388964 is not close to the backpropagation gradient of 0.0008188908913836675!\n",
      "Parameter id: 68 Numerical gradient of 0.004345190873777938 is not close to the backpropagation gradient of 0.00437439361877531!\n",
      "Parameter id: 69 Numerical gradient of -0.0026756374893466273 is not close to the backpropagation gradient of -0.0026552860593867275!\n",
      "Parameter id: 70 Numerical gradient of 0.0061239902038323635 is not close to the backpropagation gradient of 0.006071833387230556!\n",
      "Parameter id: 71 Numerical gradient of -0.005652145418366672 is not close to the backpropagation gradient of -0.0056473046309275365!\n",
      "Parameter id: 72 Numerical gradient of 0.007214673303224116 is not close to the backpropagation gradient of 0.007183094949578554!\n",
      "Parameter id: 73 Numerical gradient of -0.006717071343587122 is not close to the backpropagation gradient of -0.006717614210269242!\n",
      "Parameter id: 74 Numerical gradient of -0.013346213023623932 is not close to the backpropagation gradient of -0.013287128465610327!\n",
      "Parameter id: 75 Numerical gradient of -0.002227107387398064 is not close to the backpropagation gradient of -0.0022209504004040404!\n",
      "Parameter id: 76 Numerical gradient of -0.0066560090772327385 is not close to the backpropagation gradient of -0.006673084739694577!\n",
      "Parameter id: 77 Numerical gradient of -8.215650382226158e-06 is not close to the backpropagation gradient of 8.933206996893615e-06!\n",
      "Parameter id: 78 Numerical gradient of 0.005234257471897763 is not close to the backpropagation gradient of 0.005247460144797806!\n",
      "Parameter id: 79 Numerical gradient of 0.0002773337115513641 is not close to the backpropagation gradient of 0.0002850040653807036!\n",
      "Parameter id: 80 Numerical gradient of -0.025724977703589502 is not close to the backpropagation gradient of -0.025765061831863348!\n",
      "Parameter id: 81 Numerical gradient of 0.007490452702541005 is not close to the backpropagation gradient of 0.007531616357714231!\n",
      "Parameter id: 82 Numerical gradient of -0.01687983086640088 is not close to the backpropagation gradient of -0.016884612693644892!\n",
      "Parameter id: 83 Numerical gradient of 0.0012245759961615477 is not close to the backpropagation gradient of 0.0012264994949065036!\n",
      "Parameter id: 84 Numerical gradient of 0.057345905801753354 is not close to the backpropagation gradient of 0.0575526229997092!\n",
      "Parameter id: 85 Numerical gradient of 0.005255351709365641 is not close to the backpropagation gradient of 0.005271964322824793!\n",
      "Parameter id: 86 Numerical gradient of -0.007877476448925336 is not close to the backpropagation gradient of -0.007861748097393524!\n",
      "Parameter id: 87 Numerical gradient of 0.0032582825326699094 is not close to the backpropagation gradient of 0.003255412224362722!\n",
      "Parameter id: 88 Numerical gradient of -0.0005990763440877345 is not close to the backpropagation gradient of -0.000632257531598002!\n",
      "Parameter id: 89 Numerical gradient of -0.0017057466550340905 is not close to the backpropagation gradient of -0.0017287430769680446!\n",
      "Parameter id: 90 Numerical gradient of 0.026048718737570198 is not close to the backpropagation gradient of 0.02608541345764264!\n",
      "Parameter id: 91 Numerical gradient of -0.009133360734381313 is not close to the backpropagation gradient of -0.009172157223837274!\n",
      "Parameter id: 92 Numerical gradient of 0.015831780331154732 is not close to the backpropagation gradient of 0.01583707022833174!\n",
      "Parameter id: 93 Numerical gradient of -0.0029698465908722937 is not close to the backpropagation gradient of -0.002972191753476617!\n",
      "Parameter id: 94 Numerical gradient of -0.05466338492965406 is not close to the backpropagation gradient of -0.054854581780687764!\n",
      "Parameter id: 95 Numerical gradient of -0.005413225423467338 is not close to the backpropagation gradient of -0.005428468645298735!\n",
      "Parameter id: 96 Numerical gradient of 0.005522915458300304 is not close to the backpropagation gradient of 0.005506230285306047!\n",
      "Parameter id: 97 Numerical gradient of -0.0015587531265737198 is not close to the backpropagation gradient of -0.0015567726971485651!\n",
      "Parameter id: 98 Numerical gradient of 0.0015862866575844237 is not close to the backpropagation gradient of 0.001616904567064096!\n",
      "Parameter id: 99 Numerical gradient of 0.0003710365348297273 is not close to the backpropagation gradient of 0.0003911358246976647!\n",
      "Parameter id: 100 Numerical gradient of -0.026932012175961972 is not close to the backpropagation gradient of -0.02696966735131768!\n",
      "Parameter id: 101 Numerical gradient of 0.005848210804515475 is not close to the backpropagation gradient of 0.005886519854317637!\n",
      "Parameter id: 102 Numerical gradient of -0.015576873124700795 is not close to the backpropagation gradient of -0.015582370035258086!\n",
      "Parameter id: 103 Numerical gradient of 0.004303002398842182 is not close to the backpropagation gradient of 0.004299811417500083!\n",
      "Parameter id: 104 Numerical gradient of 0.03969669037928725 is not close to the backpropagation gradient of 0.039870905848487334!\n",
      "Parameter id: 105 Numerical gradient of 0.004139355525012434 is not close to the backpropagation gradient of 0.004156329902203007!\n",
      "Parameter id: 106 Numerical gradient of -0.00038125058665627876 is not close to the backpropagation gradient of -0.00037769575845002495!\n",
      "Parameter id: 107 Numerical gradient of 0.0018680612612342882 is not close to the backpropagation gradient of 0.0018665259003481484!\n",
      "Parameter id: 108 Numerical gradient of -0.002843059121460101 is not close to the backpropagation gradient of -0.0028708923784228884!\n",
      "Parameter id: 109 Numerical gradient of 0.001071587263368201 is not close to the backpropagation gradient of 0.0010475735571356653!\n",
      "Parameter id: 110 Numerical gradient of 0.02578803837138821 is not close to the backpropagation gradient of 0.02582376924058931!\n",
      "Parameter id: 111 Numerical gradient of -0.006600053836791631 is not close to the backpropagation gradient of -0.006639227360532714!\n",
      "Parameter id: 112 Numerical gradient of 0.015828005572871007 is not close to the backpropagation gradient of 0.015832668169511437!\n",
      "Parameter id: 113 Numerical gradient of -0.0027193802765168584 is not close to the backpropagation gradient of -0.0027200187154368055!\n",
      "Parameter id: 114 Numerical gradient of -0.04835754019438809 is not close to the backpropagation gradient of -0.048541920016067726!\n",
      "Parameter id: 115 Numerical gradient of -0.004859224134179385 is not close to the backpropagation gradient of -0.004874897556806607!\n",
      "Parameter id: 116 Numerical gradient of 0.0039328540424321545 is not close to the backpropagation gradient of 0.003922467519599304!\n",
      "Parameter id: 117 Numerical gradient of -0.0026492141813605485 is not close to the backpropagation gradient of -0.0026468382495645846!\n",
      "Parameter id: 118 Numerical gradient of 0.0016322498908039051 is not close to the backpropagation gradient of 0.001662550790276986!\n",
      "Parameter id: 119 Numerical gradient of 8.459899447643693e-05 is not close to the backpropagation gradient of 0.00010679621662894551!\n",
      "Parameter id: 120 Numerical gradient of 0.026513680140283213 is not close to the backpropagation gradient of 0.026542275406159024!\n",
      "Parameter id: 121 Numerical gradient of -0.007163825088696284 is not close to the backpropagation gradient of -0.0071999712724082204!\n",
      "Parameter id: 122 Numerical gradient of 0.015397683128526294 is not close to the backpropagation gradient of 0.015399340545586592!\n",
      "Parameter id: 123 Numerical gradient of -0.0072153394370388915 is not close to the backpropagation gradient of -0.007220727579325533!\n",
      "Parameter id: 124 Numerical gradient of -0.0354676288338851 is not close to the backpropagation gradient of -0.03559920421979543!\n",
      "Parameter id: 125 Numerical gradient of -0.004037881140561694 is not close to the backpropagation gradient of -0.0040512111824969905!\n",
      "Parameter id: 126 Numerical gradient of -0.003141709115084268 is not close to the backpropagation gradient of -0.003159415546721287!\n",
      "Parameter id: 127 Numerical gradient of 0.00014099832412739488 is not close to the backpropagation gradient of 0.00014698120624881926!\n",
      "Parameter id: 128 Numerical gradient of 0.004240385820253323 is not close to the backpropagation gradient of 0.0042720242487751005!\n",
      "Parameter id: 129 Numerical gradient of -0.0022639667918156192 is not close to the backpropagation gradient of -0.0022461359715068487!\n",
      "Parameter id: 130 Numerical gradient of -0.04065592307256338 is not close to the backpropagation gradient of -0.04075088021635408!\n",
      "Parameter id: 131 Numerical gradient of 0.0007429612480791548 is not close to the backpropagation gradient of 0.0007890001349753007!\n",
      "Parameter id: 132 Numerical gradient of 0.0003994582442601313 is not close to the backpropagation gradient of 0.0003864459592723829!\n",
      "Parameter id: 133 Numerical gradient of -0.0161268776111001 is not close to the backpropagation gradient of -0.016123280564499777!\n",
      "Parameter id: 134 Numerical gradient of 0.07437295224121954 is not close to the backpropagation gradient of 0.07462039502065182!\n",
      "Parameter id: 135 Numerical gradient of 0.012027268070369246 is not close to the backpropagation gradient of 0.012054305918892137!\n",
      "Parameter id: 136 Numerical gradient of -0.006140199459991891 is not close to the backpropagation gradient of -0.006119049914539029!\n",
      "Parameter id: 137 Numerical gradient of -0.019708457088540854 is not close to the backpropagation gradient of -0.019706070607575486!\n",
      "Parameter id: 138 Numerical gradient of -0.016561418902938385 is not close to the backpropagation gradient of -0.016603678798974005!\n",
      "Parameter id: 139 Numerical gradient of -0.009522826971419818 is not close to the backpropagation gradient of -0.009552896721746642!\n",
      "Parameter id: 140 Numerical gradient of -0.0029261038037020626 is not close to the backpropagation gradient of -0.0028684522846164117!\n",
      "Parameter id: 141 Numerical gradient of -0.01774269620113955 is not close to the backpropagation gradient of -0.01771232265069837!\n",
      "Parameter id: 142 Numerical gradient of -0.03898548150971237 is not close to the backpropagation gradient of -0.039006969113318725!\n",
      "Parameter id: 143 Numerical gradient of 0.010478284906412227 is not close to the backpropagation gradient of 0.01048199649497444!\n",
      "Parameter id: 144 Numerical gradient of 0.015174084211366788 is not close to the backpropagation gradient of 0.015194311631522212!\n",
      "Parameter id: 145 Numerical gradient of -0.04313704948799568 is not close to the backpropagation gradient of -0.043206583821660274!\n",
      "Parameter id: 146 Numerical gradient of 0.008821166019856719 is not close to the backpropagation gradient of 0.008735192369722165!\n",
      "Parameter id: 147 Numerical gradient of 0.019694468278430577 is not close to the backpropagation gradient of 0.019697977760114265!\n",
      "Parameter id: 148 Numerical gradient of 0.023410384741850976 is not close to the backpropagation gradient of 0.02343092549151083!\n",
      "Parameter id: 149 Numerical gradient of 0.016771029009987615 is not close to the backpropagation gradient of 0.016760683038393287!\n",
      "Parameter id: 150 Numerical gradient of 0.00844257996845954 is not close to the backpropagation gradient of 0.008493241084056547!\n",
      "Parameter id: 151 Numerical gradient of -0.005559108728903084 is not close to the backpropagation gradient of -0.005605375769425312!\n",
      "Parameter id: 152 Numerical gradient of -0.013054668457357366 is not close to the backpropagation gradient of -0.013043946746058912!\n",
      "Parameter id: 153 Numerical gradient of 0.0643167741287698 is not close to the backpropagation gradient of 0.06440036355050217!\n",
      "Parameter id: 154 Numerical gradient of -0.01416577966040222 is not close to the backpropagation gradient of -0.014189256671335415!\n",
      "Parameter id: 155 Numerical gradient of -0.01855071651846174 is not close to the backpropagation gradient of -0.0186353216218662!\n",
      "Parameter id: 156 Numerical gradient of 0.036391556434978156 is not close to the backpropagation gradient of 0.036508226395310386!\n",
      "Parameter id: 157 Numerical gradient of 0.060332849827204875 is not close to the backpropagation gradient of 0.06042509994846757!\n",
      "Parameter id: 158 Numerical gradient of -0.00964428537031381 is not close to the backpropagation gradient of -0.009693517144779305!\n",
      "Parameter id: 159 Numerical gradient of -0.047402082259395684 is not close to the backpropagation gradient of -0.04744535728792868!\n",
      "Parameter id: 160 Numerical gradient of -0.01219868650537137 is not close to the backpropagation gradient of -0.012211124540403162!\n",
      "Parameter id: 161 Numerical gradient of -0.04046607493535248 is not close to the backpropagation gradient of -0.040509790112483096!\n",
      "Parameter id: 162 Numerical gradient of 0.007275513524973575 is not close to the backpropagation gradient of 0.007341295833832516!\n",
      "Parameter id: 163 Numerical gradient of 0.018592016814977796 is not close to the backpropagation gradient of 0.01861529276825779!\n",
      "Parameter id: 164 Numerical gradient of -0.06266009933142414 is not close to the backpropagation gradient of -0.06271933920980172!\n",
      "Parameter id: 165 Numerical gradient of 0.01830846585448853 is not close to the backpropagation gradient of 0.018331547716445754!\n",
      "Parameter id: 166 Numerical gradient of 0.02045630331792836 is not close to the backpropagation gradient of 0.020512682945831466!\n",
      "Parameter id: 167 Numerical gradient of -0.03655409308578328 is not close to the backpropagation gradient of -0.03663393315115552!\n",
      "Parameter id: 168 Numerical gradient of -0.0649298392829678 is not close to the backpropagation gradient of -0.06502812507916376!\n",
      "Parameter id: 169 Numerical gradient of 0.008068878898370713 is not close to the backpropagation gradient of 0.008098870811706698!\n",
      "Parameter id: 170 Numerical gradient of 0.04513123208482739 is not close to the backpropagation gradient of 0.04514468517178651!\n",
      "Parameter id: 171 Numerical gradient of 0.0072739592127391 is not close to the backpropagation gradient of 0.007281307982594208!\n",
      "Parameter id: 172 Numerical gradient of 0.039036773813450054 is not close to the backpropagation gradient of 0.03905571420966612!\n",
      "Parameter id: 173 Numerical gradient of 0.007248202038567796 is not close to the backpropagation gradient of 0.007308485882232074!\n",
      "Parameter id: 174 Numerical gradient of 0.0170192748782938 is not close to the backpropagation gradient of 0.017029161164718663!\n",
      "Parameter id: 175 Numerical gradient of -0.06597633550597948 is not close to the backpropagation gradient of -0.06604411468163711!\n",
      "Parameter id: 176 Numerical gradient of 0.017903012405895424 is not close to the backpropagation gradient of 0.017929402732690523!\n",
      "Parameter id: 177 Numerical gradient of 0.022888357875672227 is not close to the backpropagation gradient of 0.022961092542384657!\n",
      "Parameter id: 178 Numerical gradient of -0.038255176804113944 is not close to the backpropagation gradient of -0.03835689646168632!\n",
      "Parameter id: 179 Numerical gradient of -0.06506706284881147 is not close to the backpropagation gradient of -0.06516921768317366!\n",
      "Parameter id: 180 Numerical gradient of 0.009866996109053616 is not close to the backpropagation gradient of 0.009907913868863784!\n",
      "Parameter id: 181 Numerical gradient of 0.0484108308995701 is not close to the backpropagation gradient of 0.04843660437072757!\n",
      "Parameter id: 182 Numerical gradient of 0.008552714092502356 is not close to the backpropagation gradient of 0.008560130157389914!\n",
      "Parameter id: 183 Numerical gradient of 0.0374094089039545 is not close to the backpropagation gradient of 0.03743743810748993!\n",
      "Parameter id: 184 Numerical gradient of 0.021118218285209878 is not close to the backpropagation gradient of 0.021181858607531995!\n",
      "Parameter id: 185 Numerical gradient of 0.023364421508631494 is not close to the backpropagation gradient of 0.023406659749036118!\n",
      "Parameter id: 186 Numerical gradient of -0.019001911155669404 is not close to the backpropagation gradient of -0.01886340161091711!\n",
      "Parameter id: 187 Numerical gradient of 0.0076121331460399224 is not close to the backpropagation gradient of 0.007587203125458691!\n",
      "Parameter id: 188 Numerical gradient of -0.002147615418834903 is not close to the backpropagation gradient of -0.002181021968190574!\n",
      "Parameter id: 189 Numerical gradient of -0.009209077944660748 is not close to the backpropagation gradient of -0.009114836335948366!\n",
      "Parameter id: 190 Numerical gradient of -0.034111602431607935 is not close to the backpropagation gradient of -0.03405708908454918!\n",
      "Parameter id: 191 Numerical gradient of -0.0017825740883381511 is not close to the backpropagation gradient of -0.0018304023458975143!\n",
      "Parameter id: 192 Numerical gradient of 0.006792344464656708 is not close to the backpropagation gradient of 0.006659633153259473!\n",
      "Parameter id: 193 Numerical gradient of 0.00034372504842394846 is not close to the backpropagation gradient of 0.00029349051045808984!\n",
      "Parameter id: 194 Numerical gradient of 0.007022604719963964 is not close to the backpropagation gradient of 0.006917906199758385!\n",
      "Parameter id: 195 Numerical gradient of -0.004118705376754406 is not close to the backpropagation gradient of -0.004180799595586052!\n",
      "Parameter id: 196 Numerical gradient of 0.0030258018313134016 is not close to the backpropagation gradient of 0.0030288751130086817!\n",
      "Parameter id: 197 Numerical gradient of 0.06766232019117524 is not close to the backpropagation gradient of 0.06771505043578238!\n",
      "Parameter id: 198 Numerical gradient of -0.01723510223428093 is not close to the backpropagation gradient of -0.017256948404479257!\n",
      "Parameter id: 199 Numerical gradient of -0.03129341230589944 is not close to the backpropagation gradient of -0.031361682466409635!\n",
      "Parameter id: 200 Numerical gradient of 0.06643352534752012 is not close to the backpropagation gradient of 0.06654925198565921!\n",
      "Parameter id: 201 Numerical gradient of 0.052510884529510804 is not close to the backpropagation gradient of 0.05263009358689821!\n",
      "Parameter id: 202 Numerical gradient of -0.018491874698156607 is not close to the backpropagation gradient of -0.018535528981967576!\n",
      "Parameter id: 203 Numerical gradient of -0.05577316386506936 is not close to the backpropagation gradient of -0.05580439135135994!\n",
      "Parameter id: 204 Numerical gradient of -0.019518164862120102 is not close to the backpropagation gradient of -0.019511283277291284!\n",
      "Parameter id: 205 Numerical gradient of -0.04320277469105349 is not close to the backpropagation gradient of -0.04327263704425472!\n",
      "Parameter id: 206 Numerical gradient of 0.010129674876679928 is not close to the backpropagation gradient of 0.010184481104892977!\n",
      "Parameter id: 207 Numerical gradient of 0.004106048834273679 is not close to the backpropagation gradient of 0.004104996861697429!\n",
      "Parameter id: 208 Numerical gradient of -0.06539591090870545 is not close to the backpropagation gradient of -0.06545389557033582!\n",
      "Parameter id: 209 Numerical gradient of 0.01458899667738933 is not close to the backpropagation gradient of 0.014610814436216263!\n",
      "Parameter id: 210 Numerical gradient of 0.025948354576144084 is not close to the backpropagation gradient of 0.026019083823074315!\n",
      "Parameter id: 211 Numerical gradient of -0.0617550455217497 is not close to the backpropagation gradient of -0.06186420734607469!\n",
      "Parameter id: 212 Numerical gradient of -0.04723066382439356 is not close to the backpropagation gradient of -0.04734648813028995!\n",
      "Parameter id: 213 Numerical gradient of 0.017457368883810886 is not close to the backpropagation gradient of 0.017499854403336013!\n",
      "Parameter id: 214 Numerical gradient of 0.04886935300874029 is not close to the backpropagation gradient of 0.04889989520506731!\n",
      "Parameter id: 215 Numerical gradient of 0.018955725877844998 is not close to the backpropagation gradient of 0.01895184279361664!\n",
      "Parameter id: 216 Numerical gradient of 0.034326097519965515 is not close to the backpropagation gradient of 0.03439362241879951!\n",
      "Parameter id: 217 Numerical gradient of -0.001839195462594034 is not close to the backpropagation gradient of -0.0018917465079229274!\n",
      "Parameter id: 218 Numerical gradient of -0.005716316309190006 is not close to the backpropagation gradient of -0.005707666883462533!\n",
      "Parameter id: 219 Numerical gradient of 0.0670596911334087 is not close to the backpropagation gradient of 0.06712680286693626!\n",
      "Parameter id: 220 Numerical gradient of -0.0159645630048999 is not close to the backpropagation gradient of -0.015989564740504225!\n",
      "Parameter id: 221 Numerical gradient of -0.024686253041750206 is not close to the backpropagation gradient of -0.024767339856224993!\n",
      "Parameter id: 222 Numerical gradient of 0.045490278210991164 is not close to the backpropagation gradient of 0.045613421778368614!\n",
      "Parameter id: 223 Numerical gradient of 0.05974354344573384 is not close to the backpropagation gradient of 0.0598402051303137!\n",
      "Parameter id: 224 Numerical gradient of -0.01419619977127695 is not close to the backpropagation gradient of -0.01424712575672505!\n",
      "Parameter id: 225 Numerical gradient of -0.05326583618625591 is not close to the backpropagation gradient of -0.0533038045913867!\n",
      "Parameter id: 226 Numerical gradient of -0.013088641281910895 is not close to the backpropagation gradient of -0.013095995327514261!\n",
      "Parameter id: 227 Numerical gradient of -0.04353628568765089 is not close to the backpropagation gradient of -0.043577186111877914!\n",
      "Parameter id: 228 Numerical gradient of 0.002192024339819909 is not close to the backpropagation gradient of 0.0022535959706197423!\n",
      "Parameter id: 229 Numerical gradient of 0.0003170796958329447 is not close to the backpropagation gradient of 0.00031495859443110216!\n",
      "Parameter id: 230 Numerical gradient of -0.06685496600766783 is not close to the backpropagation gradient of -0.0669114843695808!\n",
      "Parameter id: 231 Numerical gradient of 0.016013412817983408 is not close to the backpropagation gradient of 0.016035302653928874!\n",
      "Parameter id: 232 Numerical gradient of 0.027490010268138576 is not close to the backpropagation gradient of 0.027557405507857315!\n",
      "Parameter id: 233 Numerical gradient of -0.05364353405923339 is not close to the backpropagation gradient of -0.05375861135562186!\n",
      "Parameter id: 234 Numerical gradient of -0.053041349090676704 is not close to the backpropagation gradient of -0.053143843697481846!\n",
      "Parameter id: 235 Numerical gradient of 0.017157164577952244 is not close to the backpropagation gradient of 0.017202669198055638!\n",
      "Parameter id: 236 Numerical gradient of 0.052654769433502224 is not close to the backpropagation gradient of 0.052684751809402124!\n",
      "Parameter id: 237 Numerical gradient of 0.01751043754438797 is not close to the backpropagation gradient of 0.01750897322480755!\n",
      "Parameter id: 238 Numerical gradient of 0.04020384025693602 is not close to the backpropagation gradient of 0.04025628246358221!\n",
      "Parameter id: 239 Numerical gradient of 0.007649214595062403 is not close to the backpropagation gradient of 0.007712569131106026!\n",
      "Parameter id: 240 Numerical gradient of 0.016106005418237146 is not close to the backpropagation gradient of 0.016124617252298794!\n",
      "Parameter id: 241 Numerical gradient of -0.06191180901282677 is not close to the backpropagation gradient of -0.061971616400790824!\n",
      "Parameter id: 242 Numerical gradient of 0.01695088513997689 is not close to the backpropagation gradient of 0.01697196527820434!\n",
      "Parameter id: 243 Numerical gradient of 0.019235724124655462 is not close to the backpropagation gradient of 0.019294040053881807!\n",
      "Parameter id: 244 Numerical gradient of -0.037625458304546555 is not close to the backpropagation gradient of -0.03770797092345166!\n",
      "Parameter id: 245 Numerical gradient of -0.060378813060424356 is not close to the backpropagation gradient of -0.06047667336922513!\n",
      "Parameter id: 246 Numerical gradient of 0.009793277300218506 is not close to the backpropagation gradient of 0.00982358672291292!\n",
      "Parameter id: 247 Numerical gradient of 0.043803627391980626 is not close to the backpropagation gradient of 0.043820291171265624!\n",
      "Parameter id: 248 Numerical gradient of 0.009813261314661759 is not close to the backpropagation gradient of 0.009817879144664704!\n",
      "Parameter id: 249 Numerical gradient of 0.03656297486998028 is not close to the backpropagation gradient of 0.036591311939133384!\n",
      "Parameter id: 250 Numerical gradient of -0.03194844389042828 is not close to the backpropagation gradient of -0.0320127052004666!\n",
      "Parameter id: 251 Numerical gradient of 0.0008375522497772181 is not close to the backpropagation gradient of 0.0008425730771504673!\n",
      "Parameter id: 252 Numerical gradient of 0.07020672931901117 is not close to the backpropagation gradient of 0.07025893057113088!\n",
      "Parameter id: 253 Numerical gradient of -0.01638955637872641 is not close to the backpropagation gradient of -0.016410424811114106!\n",
      "Parameter id: 254 Numerical gradient of -0.027564173166183537 is not close to the backpropagation gradient of -0.02763221378062465!\n",
      "Parameter id: 255 Numerical gradient of 0.07772982257847616 is not close to the backpropagation gradient of 0.077848514885025!\n",
      "Parameter id: 256 Numerical gradient of 0.05537725833448803 is not close to the backpropagation gradient of 0.05549751857934601!\n",
      "Parameter id: 257 Numerical gradient of -0.014990675367698712 is not close to the backpropagation gradient of -0.015038613742083562!\n",
      "Parameter id: 258 Numerical gradient of -0.05416600501462199 is not close to the backpropagation gradient of -0.054198442241274056!\n",
      "Parameter id: 259 Numerical gradient of -0.017765788840051755 is not close to the backpropagation gradient of -0.017756587355569932!\n",
      "Parameter id: 260 Numerical gradient of -0.0413267198240419 is not close to the backpropagation gradient of -0.04139854998151952!\n"
     ]
    }
   ],
   "source": [
    "# Do gradient checking\n",
    "model = ModelSort(input_size, output_size, hidden_size, seq_length)\n",
    "# Get the gradients of the parameters from a subset of the data\n",
    "backprop_grads = model.get_gradients(\n",
    "    x_train[:100], y_train[:100])\n",
    "\n",
    "eps = 1e-9  # Set the small change to compute the numerical gradient\n",
    "# Compute the numerical gradients of the parameters in all layers.\n",
    "for p_idx, param in enumerate(model.get_params_iter()):\n",
    "    grad_backprop = backprop_grads[p_idx]\n",
    "    \n",
    "    # + eps\n",
    "    param += eps\n",
    "    plus_loss = model.loss(\n",
    "        model.predict_proba(x_train[0:100,:,:]), y_train[0:100,:,:])\n",
    "    \n",
    "    # - eps\n",
    "    param -= 2 * eps\n",
    "    min_loss = model.loss(\n",
    "        model.predict_proba(x_train[0:100,:,:]), y_train[0:100,:,:])\n",
    "    \n",
    "    # reset param value\n",
    "    param += eps\n",
    "    \n",
    "    # calculate numerical gradient\n",
    "    grad_num = (plus_loss - min_loss) / (2*eps)\n",
    "    numerator = np.linalg.norm(grad_backprop - grad_num)\n",
    "    denominator = np.linalg.norm(grad_backprop) + np.linalg.norm(grad_num)\n",
    "    difference = numerator / denominator\n",
    "    \n",
    "    if not np.isclose(grad_num, grad_backprop, ):\n",
    "        print((\n",
    "            f'Parameter id: {p_idx} '\n",
    "            f'Numerical gradient of {grad_num} is not close '\n",
    "            f'to the backpropagation gradient of {grad_backprop}!'\n",
    "        ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
