{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import *\n",
    "import sys\n",
    "import itertools\n",
    "import numpy as np  # Matrix and vector computation package\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt  # Plotting library\n",
    "import seaborn as sns  # Fancier plots\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Set seaborn plotting style\n",
    "sns.set_style('darkgrid')\n",
    "# Set the seed for reproducability\n",
    "np.random.seed(seed=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sort_dataset(dataset_length, seq_length, max_number=999, fraction=0.8):\n",
    "    x_train = np.random.randint(low=0, high=max_number+1, size=(int(dataset_length*fraction),\n",
    "                                                                seq_length,\n",
    "                                                                1))\n",
    "    y_train = np.sort(x_train, axis=1)\n",
    "    \n",
    "    x_test = np.random.randint(low=0, high=max_number+1, size=(int(dataset_length*(1-fraction)),\n",
    "                                                               seq_length,\n",
    "                                                               1))\n",
    "    y_test = np.sort(x_test, axis=1)\n",
    "    \n",
    "    return x_train, y_train, x_test, y_test\n",
    "\n",
    "def create_dummy_dataset(dataset_length, seq_length, max_number):\n",
    "    lower_bound = -1 * max_number\n",
    "    x_train = np.random.randint(low=lower_bound, high=max_number+1, size=(dataset_length, seq_length, 1))\n",
    "    y_train = np.where(x_train.sum(axis=2) > 0, 1, 0).reshape(x_train.shape)\n",
    "    \n",
    "    x_test = np.random.randint(low=lower_bound, high=max_number+1, size=(dataset_length, seq_length, 1))\n",
    "    y_test = np.where(x_test.sum(axis=2) > 0, 1, 0).reshape(x_train.shape)\n",
    "    \n",
    "    return x_train, y_train, x_test, y_test\n",
    "\n",
    "def evaluate(y_test, x_test, model):\n",
    "    y_pred = model.predict(x_test)\n",
    "    loss = model.loss(model.predict_proba(x_test), y_test)\n",
    "    \n",
    "    print(f'Cross entropy loss {loss}')\n",
    "    print('-'*100)\n",
    "    print(classification_report(y_test.flatten(), y_pred.flatten()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set hyper-parameters\n",
    "batch_size = 20  # Size of the minibatches (number of samples)\n",
    "max_num = 10 \n",
    "seq_length = 5\n",
    "hidden_size = 10 \n",
    "dataset_size = 200\n",
    "epoch = 4\n",
    "\n",
    "x_train, y_train, x_test, y_test = create_sort_dataset(dataset_size, seq_length, max_num)\n",
    "\n",
    "input_size = 1\n",
    "output_size = max_num+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "TRAIN: Cross entropy loss:  2.686409216799245\n",
      "VALIDATION: Cross entropy loss:  2.599438222561297\n",
      "Epoch 2/4\n",
      "TRAIN: Cross entropy loss:  2.680450815503051\n",
      "VALIDATION: Cross entropy loss:  2.5957300293547694\n",
      "Epoch 3/4\n",
      "TRAIN: Cross entropy loss:  2.6745933586800503\n",
      "VALIDATION: Cross entropy loss:  2.592103804705925\n",
      "Epoch 4/4\n",
      "TRAIN: Cross entropy loss:  2.668835045377737\n",
      "VALIDATION: Cross entropy loss:  2.588557944381883\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create the network\n",
    "model = ModelSort(input_size, output_size, hidden_size, seq_length)\n",
    "\n",
    "train_loss = []\n",
    "validation_loss = []\n",
    "\n",
    "for i in range(epoch):\n",
    "    print(f'Epoch {i+1}/{epoch}')\n",
    "    epoch_train_loss = []\n",
    "    \n",
    "    for mb in range(dataset_size // batch_size):\n",
    "        x_batch = x_train[mb:mb + batch_size]  # Input minibatch\n",
    "        y_batch = y_train[mb:mb + batch_size]  # Target minibatch\n",
    "        model.train_on_batch(x_batch, y_batch)\n",
    "        \n",
    "        loss = model.loss(model.predict_proba(x_batch), y_batch)\n",
    "        epoch_train_loss.append(loss)\n",
    "        \n",
    "\n",
    "    train_loss.append(np.mean(epoch_train_loss))\n",
    "    validation_loss.append(model.loss(model.predict_proba(x_test), y_test))\n",
    "\n",
    "\n",
    "    print(\"TRAIN: Cross entropy loss: \", train_loss[-1])\n",
    "    print(\"VALIDATION: Cross entropy loss: \", validation_loss[-1])\n",
    "\n",
    "\n",
    "\n",
    "# Plot the loss over the iterations\n",
    "fig = plt.figure(figsize=(10, 7))\n",
    "plt.plot(train_loss, 'b-')\n",
    "plt.plot(validation_loss, 'r')\n",
    "plt.xlabel('minibatch iteration')\n",
    "plt.ylabel('Cross entropy loss', fontsize=15)\n",
    "plt.xlim(0, 100)\n",
    "fig.subplots_adjust(bottom=0.2)\n",
    "plt.show()\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross entropy loss 2.588557944381883\n",
      "----------------------------------------------------------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        11\n",
      "           1       0.00      0.00      0.00        19\n",
      "           2       0.13      0.08      0.10        26\n",
      "           3       0.09      0.94      0.17        17\n",
      "           4       0.22      0.13      0.17        15\n",
      "           5       0.00      0.00      0.00        20\n",
      "           6       0.00      0.00      0.00        15\n",
      "           7       0.00      0.00      0.00        17\n",
      "           8       0.00      0.00      0.00        15\n",
      "           9       0.00      0.00      0.00        19\n",
      "          10       0.00      0.00      0.00        21\n",
      "\n",
      "   micro avg       0.10      0.10      0.10       195\n",
      "   macro avg       0.04      0.10      0.04       195\n",
      "weighted avg       0.04      0.10      0.04       195\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/loopdigga/Documents/python3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/loopdigga/Documents/python3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/loopdigga/Documents/python3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "evaluate(y_test, x_test, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The gradient is correct\n",
      "The gradient is correct\n",
      "The gradient is correct\n",
      "The gradient is correct\n",
      "The gradient is correct\n",
      "The gradient is correct\n",
      "The gradient is correct\n",
      "The gradient is correct\n",
      "The gradient is correct\n",
      "The gradient is correct\n",
      "The gradient is correct\n",
      "The gradient is correct\n",
      "The gradient is correct\n",
      "The gradient is correct\n",
      "The gradient is correct\n",
      "The gradient is correct\n",
      "The gradient is correct\n",
      "The gradient is correct\n",
      "The gradient is correct\n",
      "The gradient is correct\n",
      "The gradient is correct\n",
      "The gradient is correct\n",
      "The gradient is correct\n",
      "The gradient is correct\n",
      "The gradient is correct\n",
      "The gradient is correct\n",
      "The gradient is correct\n",
      "The gradient is correct\n",
      "The gradient is correct\n",
      "The gradient is correct\n",
      "The gradient is correct\n",
      "The gradient is correct\n",
      "The gradient is correct\n",
      "The gradient is correct\n",
      "The gradient is correct\n",
      "The gradient is correct\n",
      "The gradient is correct\n",
      "The gradient is correct\n",
      "The gradient is correct\n",
      "The gradient is correct\n",
      "The gradient is correct\n",
      "The gradient is correct\n",
      "The gradient is correct\n",
      "The gradient is correct\n",
      "The gradient is correct\n",
      "The gradient is correct\n",
      "The gradient is correct\n",
      "The gradient is correct\n",
      "The gradient is correct\n",
      "The gradient is correct\n",
      "The gradient is correct\n",
      "The gradient is correct\n",
      "The gradient is correct\n",
      "The gradient is correct\n",
      "The gradient is correct\n",
      "The gradient is correct\n",
      "The gradient is correct\n",
      "The gradient is correct\n",
      "The gradient is correct\n",
      "The gradient is correct\n",
      "The gradient is correct\n",
      "The gradient is correct\n",
      "The gradient is correct\n",
      "The gradient is correct\n",
      "The gradient is correct\n",
      "The gradient is correct\n",
      "The gradient is correct\n",
      "The gradient is correct\n",
      "The gradient is correct\n",
      "The gradient is correct\n",
      "The gradient is correct\n",
      "The gradient is correct\n",
      "The gradient is correct\n",
      "The gradient is correct\n",
      "The gradient is correct\n",
      "The gradient is correct\n",
      "The gradient is correct\n",
      "The gradient is correct\n",
      "The gradient is correct\n",
      "The gradient is correct\n",
      "The gradient is correct\n",
      "The gradient is correct\n",
      "The gradient is correct\n",
      "The gradient is correct\n",
      "The gradient is correct\n",
      "The gradient is correct\n",
      "The gradient is correct\n",
      "The gradient is correct\n",
      "The gradient is correct\n",
      "The gradient is correct\n",
      "The gradient is correct\n",
      "The gradient is correct\n",
      "The gradient is correct\n",
      "The gradient is correct\n",
      "The gradient is correct\n",
      "The gradient is correct\n",
      "The gradient is correct\n",
      "The gradient is correct\n",
      "The gradient is correct\n",
      "The gradient is correct\n",
      "The gradient is correct\n",
      "The gradient is correct\n",
      "The gradient is correct\n",
      "The gradient is correct\n",
      "The gradient is correct\n",
      "The gradient is correct\n",
      "The gradient is correct\n",
      "The gradient is correct\n",
      "The gradient is correct\n",
      "The gradient is correct\n",
      "The gradient is correct\n",
      "The gradient is correct\n",
      "The gradient is correct\n",
      "The gradient is correct\n",
      "The gradient is correct\n",
      "The gradient is correct\n",
      "The gradient is correct\n",
      "The gradient is correct\n",
      "The gradient is correct\n",
      "The gradient is correct\n",
      "The gradient is correct\n",
      "The gradient is correct\n",
      "The gradient is correct\n",
      "The gradient is correct\n",
      "The gradient is correct\n",
      "The gradient is correct\n",
      "The gradient is correct\n",
      "The gradient is correct\n",
      "The gradient is correct\n",
      "The gradient is correct\n",
      "The gradient is correct\n",
      "The gradient is correct\n",
      "The gradient is correct\n",
      "The gradient is correct\n",
      "The gradient is correct\n",
      "The gradient is correct\n",
      "The gradient is correct\n",
      "The gradient is correct\n",
      "The gradient is correct\n",
      "The gradient is correct\n",
      "The gradient is correct\n",
      "The gradient is correct\n",
      "The gradient is correct\n",
      "The gradient is correct\n",
      "The gradient is correct\n",
      "The gradient is correct\n",
      "The gradient is correct\n",
      "The gradient is correct\n",
      "The gradient is correct\n",
      "The gradient is correct\n",
      "The gradient is correct\n",
      "The gradient is correct\n",
      "The gradient is correct\n",
      "The gradient is correct\n",
      "The gradient is correct\n",
      "The gradient is correct\n",
      "The gradient is correct\n",
      "The gradient is correct\n",
      "The gradient is correct\n",
      "The gradient is correct\n",
      "The gradient is correct\n",
      "The gradient is correct\n",
      "The gradient is correct\n",
      "The gradient is correct\n",
      "The gradient is correct\n",
      "The gradient is correct\n",
      "The gradient is correct\n",
      "The gradient is correct\n",
      "The gradient is correct\n",
      "The gradient is correct\n",
      "The gradient is correct\n",
      "The gradient is correct\n",
      "The gradient is correct\n",
      "The gradient is correct\n",
      "The gradient is correct\n",
      "The gradient is correct\n",
      "The gradient is correct\n",
      "The gradient is correct\n",
      "The gradient is correct\n",
      "The gradient is correct\n",
      "The gradient is correct\n",
      "The gradient is correct\n",
      "The gradient is correct\n",
      "The gradient is correct\n",
      "The gradient is correct\n",
      "The gradient is correct\n",
      "The gradient is correct\n",
      "The gradient is correct\n",
      "The gradient is correct\n",
      "The gradient is correct\n",
      "The gradient is correct\n",
      "The gradient is correct\n",
      "The gradient is correct\n",
      "The gradient is correct\n",
      "The gradient is correct\n",
      "The gradient is correct\n",
      "The gradient is correct\n",
      "The gradient is correct\n",
      "The gradient is correct\n",
      "The gradient is correct\n",
      "The gradient is correct\n",
      "The gradient is correct\n",
      "The gradient is correct\n",
      "The gradient is correct\n",
      "The gradient is correct\n",
      "The gradient is correct\n",
      "The gradient is correct\n",
      "The gradient is correct\n",
      "The gradient is correct\n",
      "The gradient is correct\n",
      "The gradient is correct\n",
      "The gradient is correct\n",
      "The gradient is correct\n",
      "The gradient is correct\n",
      "The gradient is correct\n",
      "The gradient is correct\n",
      "The gradient is correct\n",
      "The gradient is correct\n",
      "The gradient is correct\n",
      "The gradient is correct\n",
      "The gradient is correct\n",
      "The gradient is correct\n",
      "The gradient is correct\n",
      "The gradient is correct\n",
      "The gradient is correct\n",
      "The gradient is correct\n",
      "The gradient is correct\n",
      "The gradient is correct\n",
      "The gradient is correct\n",
      "The gradient is correct\n",
      "The gradient is correct\n",
      "The gradient is correct\n",
      "The gradient is correct\n",
      "The gradient is correct\n",
      "The gradient is correct\n",
      "The gradient is correct\n",
      "The gradient is correct\n",
      "The gradient is correct\n",
      "The gradient is correct\n",
      "The gradient is correct\n",
      "The gradient is correct\n",
      "The gradient is correct\n",
      "The gradient is correct\n",
      "The gradient is correct\n",
      "The gradient is correct\n",
      "The gradient is correct\n",
      "The gradient is correct\n",
      "The gradient is correct\n",
      "The gradient is correct\n",
      "The gradient is correct\n",
      "The gradient is correct\n",
      "The gradient is correct\n",
      "The gradient is correct\n",
      "The gradient is correct\n",
      "The gradient is correct\n",
      "The gradient is correct\n",
      "The gradient is correct\n",
      "The gradient is correct\n",
      "The gradient is correct\n",
      "The gradient is correct\n",
      "The gradient is correct\n"
     ]
    }
   ],
   "source": [
    "# Do gradient checking\n",
    "model = ModelSort(input_size, output_size, hidden_size, seq_length)\n",
    "# Get the gradients of the parameters from a subset of the data\n",
    "backprop_grads = model.get_gradients(\n",
    "    x_train[:100], y_train[:100])\n",
    "\n",
    "eps = 1e-9  # Set the small change to compute the numerical gradient\n",
    "# Compute the numerical gradients of the parameters in all layers.\n",
    "for p_idx, param in enumerate(model.get_params_iter()):\n",
    "    grad_backprop = backprop_grads[p_idx]\n",
    "    \n",
    "    # + eps\n",
    "    param += eps\n",
    "    plus_loss = model.loss(\n",
    "        model.predict_proba(x_train[0:100,:,:]), y_train[0:100,:,:])\n",
    "    \n",
    "    # - eps\n",
    "    param -= 2 * eps\n",
    "    min_loss = model.loss(\n",
    "        model.predict_proba(x_train[0:100,:,:]), y_train[0:100,:,:])\n",
    "    \n",
    "    # reset param value\n",
    "    param += eps\n",
    "    \n",
    "    # calculate numerical gradient\n",
    "    grad_num = (plus_loss - min_loss) / (2*eps)\n",
    "    numerator = np.linalg.norm(grad_backprop - grad_num)\n",
    "    denominator = np.linalg.norm(grad_backprop) + np.linalg.norm(grad_num)\n",
    "    difference = numerator / denominator\n",
    "    \n",
    "    if not np.isclose(grad_num, grad_backprop/100):\n",
    "#     if difference < 1e-7:\n",
    "        print('The gradient is correct')\n",
    "    else:\n",
    "        print((\n",
    "            f'Parameter id: {p_idx} '\n",
    "            f'Numerical gradient of {grad_num:.6f} is not close '\n",
    "            f'to the backpropagation gradient of {grad_backprop:.6f}!'\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
