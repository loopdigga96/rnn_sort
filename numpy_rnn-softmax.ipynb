{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "%matplotlib inline\n",
    "\n",
    "import sys\n",
    "import itertools\n",
    "import numpy as np  # Matrix and vector computation package\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt  # Plotting library\n",
    "import seaborn as sns  # Fancier plots\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Set seaborn plotting style\n",
    "sns.set_style('darkgrid')\n",
    "# Set the seed for reproducability\n",
    "np.random.seed(seed=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sort_dataset(dataset_length, seq_length, max_number=999):\n",
    "    x_train = np.random.randint(low=0, high=max_number+1, size=(dataset_length, seq_length, 1))\n",
    "    y_train = np.sort(x_train, axis=1)\n",
    "    \n",
    "    x_test = np.random.randint(low=0, high=max_number+1, size=(dataset_length, seq_length, 1))\n",
    "    y_test = np.sort(x_test, axis=1)\n",
    "    \n",
    "    return x_train, y_train, x_test, y_test\n",
    "\n",
    "def create_dummy_dataset(dataset_length, seq_length, max_number):\n",
    "    lower_bound = -1 * max_number\n",
    "    x_train = np.random.randint(low=lower_bound, high=max_number+1, size=(dataset_length, seq_length, 1))\n",
    "    y_train = np.where(x_train.sum(axis=2) > 0, 1, 0).reshape(x_train.shape)\n",
    "    \n",
    "    x_test = np.random.randint(low=lower_bound, high=max_number+1, size=(dataset_length, seq_length, 1))\n",
    "    y_test = np.where(x_test.sum(axis=2) > 0, 1, 0).reshape(x_train.shape)\n",
    "    \n",
    "    return x_train, y_train, x_test, y_test\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the linear tensor transformation layer\n",
    "class TensorLinear(object):\n",
    "    \"\"\"The linear tensor layer applies a linear tensor dot product \n",
    "    and a bias to its input.\"\"\"\n",
    "    def __init__(self, n_in, n_out, tensor_order, W=None, b=None):\n",
    "        \"\"\"Initialse the weight W and bias b parameters.\"\"\"\n",
    "        a = np.sqrt(6.0 / (n_in + n_out))\n",
    "        self.W = (np.random.uniform(-a, a, (n_in, n_out)) \n",
    "                  if W is None else W)\n",
    "        self.b = (np.zeros((n_out)) if b is None else b)\n",
    "        # Axes summed over in backprop\n",
    "        self.bpAxes = tuple(range(tensor_order-1))\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"Perform forward step transformation with the help \n",
    "        of a tensor product.\"\"\"\n",
    "        # Same as: Y[i,j,:] = np.dot(X[i,j,:], self.W) + self.b \n",
    "        #          (for i,j in X.shape[0:1])\n",
    "        # Same as: Y = np.einsum('ijk,kl->ijl', X, self.W) + self.b\n",
    "        return np.tensordot(X, self.W, axes=((-1),(0))) + self.b\n",
    "\n",
    "    def backward(self, X, gY):\n",
    "        \"\"\"Return the gradient of the parmeters and the inputs of \n",
    "        this layer.\"\"\"\n",
    "        # Same as: gW = np.einsum('ijk,ijl->kl', X, gY)\n",
    "        # Same as: gW += np.dot(X[:,j,:].T, gY[:,j,:]) \n",
    "        #          (for i,j in X.shape[0:1])\n",
    "        gW = np.tensordot(X, gY, axes=(self.bpAxes, self.bpAxes))\n",
    "        gB = np.sum(gY, axis=self.bpAxes)\n",
    "        # Same as: gX = np.einsum('ijk,kl->ijl', gY, self.W.T)\n",
    "        # Same as: gX[i,j,:] = np.dot(gY[i,j,:], self.W.T) \n",
    "        #          (for i,j in gY.shape[0:1])\n",
    "        gX = np.tensordot(gY, self.W.T, axes=((-1),(0)))  \n",
    "        return gX, gW, gB\n",
    "\n",
    "# Define the logistic classifier layer\n",
    "class LogisticClassifier(object):\n",
    "    \"\"\"The logistic layer applies the logistic function to its \n",
    "    inputs.\"\"\"\n",
    "   \n",
    "    def forward(self, X):\n",
    "        \"\"\"Perform the forward step transformation.\"\"\"\n",
    "        return 1. / (1. + np.exp(-X))\n",
    "    \n",
    "    def backward(self, Y, T):\n",
    "        \"\"\"Return the gradient with respect to the loss function \n",
    "        at the inputs of this layer.\"\"\"\n",
    "        # Average by the number of samples and sequence length.\n",
    "        return (Y - T) / (Y.shape[0] * Y.shape[1])\n",
    "    \n",
    "    def loss(self, Y, T):\n",
    "        \"\"\"Compute the loss at the output.\"\"\"\n",
    "        return -np.mean((T * np.log(Y)) + ((1-T) * np.log(1-Y)))\n",
    "\n",
    "class LogisticClassifierSoftmax:\n",
    "    def forward(self, X, theta = 1.0, axis = 2):\n",
    "        \"Takes X as 3d tensor\"\n",
    "\n",
    "        # multiply y against the theta parameter,\n",
    "        y = X * float(theta)\n",
    "        # subtract the max for numerical stability\n",
    "        y = y - np.expand_dims(np.max(y, axis = axis), axis)\n",
    "        # exponentiate y\n",
    "        y = np.exp(y)\n",
    "        # take the sum along the specified axis\n",
    "        ax_sum = np.expand_dims(np.sum(y, axis = axis), axis)\n",
    "        # finally: divide elementwise\n",
    "        return y / ax_sum\n",
    "        \n",
    "    def loss(self, Y, T):\n",
    "        \"\"\"\n",
    "        Y is the output from fully connected layer passed through softmax (batch_size x num_examples x num_classes)\n",
    "        T is labels (batch_size x num_examples x 1)\n",
    "            Note that y is not one-hot encoded vector. \n",
    "            It can be computed as y.argmax(axis=1) from one-hot encoded vectors of labels if required.\n",
    "        \"\"\"\n",
    "#         print(f\"Y.shape {Y.shape}\")\n",
    "#         print(f\"T.shape {T.shape}\")\n",
    "        m = T.shape[1]\n",
    "        #ps = self.forward(Y, axis=2)\n",
    "\n",
    "        losses = []\n",
    "        for idx, p in enumerate(Y):  \n",
    "            log_likelihood = -np.log(p[range(m), T[idx].flatten()])\n",
    "            loss = np.sum(log_likelihood) / m\n",
    "            losses.append(loss)\n",
    "        return np.mean(losses)\n",
    "\n",
    "    def backward(self, X, T):\n",
    "        \"\"\"\n",
    "        X is the output from fully connected layer passed through softmax (batch_size x num_examples x num_classes)\n",
    "        T is labels (batch_size x num_examples x 1)\n",
    "            Note that y is not one-hot encoded vector. \n",
    "            It can be computed as y.argmax(axis=1) from one-hot encoded vectors of labels if required.\n",
    "        \"\"\"\n",
    "        delta = np.zeros(X.shape)\n",
    "        m = T.shape[1]\n",
    "        \n",
    "        for idx in range(len(delta)):\n",
    "            #x = Y[idx]\n",
    "            #grad = self.forward(x, axis=1)\n",
    "            #grad = x\n",
    "            grad = X[idx]\n",
    "            grad[range(m),T[idx].flatten()] -= 1\n",
    "#             grad = grad/m\n",
    "            delta[idx] = grad\n",
    "#         return delta\n",
    "        return delta / (X.shape[0] * X.shape[1])\n",
    "\n",
    "# Define tanh layer\n",
    "class TanH(object):\n",
    "    \"\"\"TanH applies the tanh function to its inputs.\"\"\"\n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"Perform the forward step transformation.\"\"\"\n",
    "        return np.tanh(X) \n",
    "    \n",
    "    def backward(self, Y, output_grad):\n",
    "        \"\"\"Return the gradient at the inputs of this layer.\"\"\"\n",
    "        gTanh = 1.0 - (Y**2)\n",
    "        return (gTanh * output_grad)\n",
    "\n",
    "# Define internal state update layer\n",
    "class RecurrentStateUpdate(object):\n",
    "    \"\"\"Update a given state.\"\"\"\n",
    "    def __init__(self, nbStates, W, b):\n",
    "        \"\"\"Initialse the linear transformation and tanh transfer \n",
    "        function.\"\"\"\n",
    "        self.linear = TensorLinear(nbStates, nbStates, 2, W, b)\n",
    "        self.tanh = TanH()\n",
    "\n",
    "    def forward(self, Xk, Sk):\n",
    "        \"\"\"Return state k+1 from input and state k.\"\"\"\n",
    "        return self.tanh.forward(Xk + self.linear.forward(Sk))\n",
    "    \n",
    "    def backward(self, Sk0, Sk1, output_grad):\n",
    "        \"\"\"Return the gradient of the parmeters and the inputs of \n",
    "        this layer.\"\"\"\n",
    "        gZ = self.tanh.backward(Sk1, output_grad)\n",
    "        gSk0, gW, gB = self.linear.backward(Sk0, gZ)\n",
    "        return gZ, gSk0, gW, gB\n",
    "\n",
    "# Define layer that unfolds the states over time\n",
    "class RecurrentStateUnfold(object):\n",
    "    \"\"\"Unfold the recurrent states.\"\"\"\n",
    "    def __init__(self, nbStates, nbTimesteps):\n",
    "        \"\"\"Initialse the shared parameters, the inital state and \n",
    "        state update function.\"\"\"\n",
    "        a = np.sqrt(6. / (nbStates * 2))\n",
    "        self.W = np.random.uniform(-a, a, (nbStates, nbStates))\n",
    "        self.b = np.zeros((self.W.shape[0]))  # Shared bias\n",
    "        self.S0 = np.zeros(nbStates)  # Initial state\n",
    "        self.nbTimesteps = nbTimesteps  # Timesteps to unfold\n",
    "        self.stateUpdate = RecurrentStateUpdate(\n",
    "            nbStates, self.W, self.b)  # State update function\n",
    "        \n",
    "    def forward(self, X):\n",
    "        \"\"\"Iteratively apply forward step to all states.\"\"\"\n",
    "        # State tensor\n",
    "        S = np.zeros((X.shape[0], X.shape[1]+1, self.W.shape[0]))\n",
    "        S[:,0,:] = self.S0  # Set initial state\n",
    "        for k in range(self.nbTimesteps):\n",
    "            # Update the states iteratively\n",
    "            S[:,k+1,:] = self.stateUpdate.forward(X[:,k,:], S[:,k,:])\n",
    "        return S\n",
    "    \n",
    "    def backward(self, X, S, gY):\n",
    "        \"\"\"Return the gradient of the parmeters and the inputs of \n",
    "        this layer.\"\"\"\n",
    "        # Initialise gradient of state outputs\n",
    "        gSk = np.zeros_like(gY[:,self.nbTimesteps-1,:])\n",
    "        # Initialse gradient tensor for state inputs\n",
    "        gZ = np.zeros_like(X)\n",
    "        gWSum = np.zeros_like(self.W)  # Initialise weight gradients\n",
    "        gBSum = np.zeros_like(self.b)  # Initialse bias gradients\n",
    "        # Propagate the gradients iteratively\n",
    "        for k in range(self.nbTimesteps-1, -1, -1):\n",
    "            # Gradient at state output is gradient from previous state \n",
    "            #  plus gradient from output\n",
    "            gSk += gY[:,k,:]\n",
    "            # Propgate the gradient back through one state\n",
    "            gZ[:,k,:], gSk, gW, gB = self.stateUpdate.backward(\n",
    "                S[:,k,:], S[:,k+1,:], gSk)\n",
    "            gWSum += gW  # Update total weight gradient\n",
    "            gBSum += gB  # Update total bias gradient\n",
    "        # Get gradient of initial state over all samples\n",
    "        gS0 = np.sum(gSk, axis=0)\n",
    "        return gZ, gWSum, gBSum, gS0\n",
    "\n",
    "# Define the full network\n",
    "class RnnBinaryAdder(object):\n",
    "    \"\"\"RNN to perform binary addition of 2 numbers.\"\"\"\n",
    "    def __init__(self, nb_of_inputs, nb_of_outputs, nb_of_states, \n",
    "                 sequence_len):\n",
    "        \"\"\"Initialse the network layers.\"\"\"\n",
    "        # Input layer\n",
    "        self.tensorInput = TensorLinear(nb_of_inputs, nb_of_states, 3)\n",
    "        # Recurrent layer\n",
    "        self.rnnUnfold = RecurrentStateUnfold(nb_of_states, sequence_len)\n",
    "        # Linear output transform\n",
    "        self.tensorOutput = TensorLinear(nb_of_states, nb_of_outputs, 3)\n",
    "        self.classifier = LogisticClassifierSoftmax()  # Classification output\n",
    "        #self.classifier = LogisticClassifier()  # Classification output\n",
    "        self.sequence_len = sequence_len\n",
    "        \n",
    "    def forward(self, X):\n",
    "        \"\"\"Perform the forward propagation of input X through all \n",
    "        layers.\"\"\"\n",
    "        # Linear input transformation\n",
    "        recIn = self.tensorInput.forward(X)\n",
    "        # Forward propagate through time and return states\n",
    "        S = self.rnnUnfold.forward(recIn)\n",
    "        # Linear output transformation\n",
    "        Z = self.tensorOutput.forward(S[:,1:self.sequence_len+1,:])\n",
    "        Y = self.classifier.forward(Z)  # Classification probabilities\n",
    "        # Return: input to recurrent layer, states, input to classifier, \n",
    "        #  output\n",
    "        return recIn, S, Z, Y\n",
    "    \n",
    "    def backward(self, X, Y, recIn, S, T):\n",
    "        \"\"\"Perform the backward propagation through all layers.\n",
    "        Input: input samples, network output, input to recurrent \n",
    "        layer, states, targets.\"\"\"\n",
    "        gZ = self.classifier.backward(Y, T)  # Get output gradient\n",
    "        gRecOut, gWout, gBout = self.tensorOutput.backward(\n",
    "            S[:,1:self.sequence_len+1,:], gZ)\n",
    "        # Propagate gradient backwards through time\n",
    "        gRnnIn, gWrec, gBrec, gS0 = self.rnnUnfold.backward(\n",
    "            recIn, S, gRecOut)\n",
    "        gX, gWin, gBin = self.tensorInput.backward(X, gRnnIn)\n",
    "        # Return the parameter gradients of: linear output weights, \n",
    "        #  linear output bias, recursive weights, recursive bias, #\n",
    "        #  linear input weights, linear input bias, initial state.\n",
    "        return gWout, gBout, gWrec, gBrec, gWin, gBin, gS0\n",
    "    \n",
    "    def getOutput(self, X):\n",
    "        \"\"\"Get the output probabilities of input X.\"\"\"\n",
    "        recIn, S, Z, Y = self.forward(X)\n",
    "        return Y\n",
    "    \n",
    "    def getBinaryOutput(self, X):\n",
    "        \"\"\"Get the binary output of input X.\"\"\"\n",
    "        return np.around(self.getOutput(X))\n",
    "    \n",
    "    def getParamGrads(self, X, T):\n",
    "        \"\"\"Return the gradients with respect to input X and \n",
    "        target T as a list. The list has the same order as the \n",
    "        get_params_iter iterator.\"\"\"\n",
    "        recIn, S, Z, Y = self.forward(X)\n",
    "        gWout, gBout, gWrec, gBrec, gWin, gBin, gS0 = self.backward(\n",
    "            X, Y, recIn, S, T)\n",
    "        return [g for g in itertools.chain(\n",
    "                np.nditer(gS0),\n",
    "                np.nditer(gWin),\n",
    "                np.nditer(gBin),\n",
    "                np.nditer(gWrec),\n",
    "                np.nditer(gBrec),\n",
    "                np.nditer(gWout),\n",
    "                np.nditer(gBout))]\n",
    "    \n",
    "    def loss(self, Y, T):\n",
    "        \"\"\"Return the loss of input X w.r.t. targets T.\"\"\"\n",
    "        return self.classifier.loss(Y, T)\n",
    "    \n",
    "    def get_params_iter(self):\n",
    "        \"\"\"Return an iterator over the parameters.\n",
    "        The iterator has the same order as get_params_grad.\n",
    "        The elements returned by the iterator are editable in-place.\"\"\"\n",
    "        return itertools.chain(\n",
    "            np.nditer(self.rnnUnfold.S0, op_flags=['readwrite']),\n",
    "            np.nditer(self.tensorInput.W, op_flags=['readwrite']),\n",
    "            np.nditer(self.tensorInput.b, op_flags=['readwrite']),\n",
    "            np.nditer(self.rnnUnfold.W, op_flags=['readwrite']),\n",
    "            np.nditer(self.rnnUnfold.b, op_flags=['readwrite']),\n",
    "            np.nditer(self.tensorOutput.W, op_flags=['readwrite']), \n",
    "            np.nditer(self.tensorOutput.b, op_flags=['readwrite']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "Epoch 2\n",
      "Epoch 3\n",
      "Epoch 4\n",
      "Epoch 5\n"
     ]
    }
   ],
   "source": [
    "# Set hyper-parameters\n",
    "lmbd = 0.5  # Rmsprop lambda\n",
    "learning_rate = 0.001  # Learning rate\n",
    "momentum_term = 0.80  # Momentum term\n",
    "eps = 1e-6  # Numerical stability term to prevent division by zero\n",
    "mb_size = 20  # Size of the minibatches (number of samples)\n",
    "max_num = 8\n",
    "seq_length = 9\n",
    "hidden_size = 30\n",
    "nb_train = 200\n",
    "\n",
    "x_train, y_train, x_test, y_test = create_sort_dataset(nb_train, seq_length, max_num)\n",
    "\n",
    "input_size = 1\n",
    "output_size = max_num+1\n",
    "\n",
    "# Create the network\n",
    "RNN = RnnBinaryAdder(1, max_num+1, hidden_size, seq_length)\n",
    "#RNN = RnnBinaryAdder(1, output_size, hidden_size, seq_length)\n",
    "# Set the initial parameters\n",
    "# Number of parameters in the network\n",
    "nbParameters =  sum(1 for _ in RNN.get_params_iter())\n",
    "# Rmsprop moving average\n",
    "maSquare = [0.0 for _ in range(nbParameters)]\n",
    "Vs = [0.0 for _ in range(nbParameters)]  # Momentum\n",
    "\n",
    "# Create a list of minibatch losses to be plotted\n",
    "ls_of_loss = [\n",
    "    RNN.loss(RNN.getOutput(x_train[0:mb_size]), y_train[0:mb_size])]\n",
    "\n",
    "# Iterate over some iterations\n",
    "for i in range(5):\n",
    "    print(f'Epoch {i+1}')\n",
    "    # Iterate over all the minibatches\n",
    "    for mb in range(nb_train // mb_size):\n",
    "        X_mb = x_train[mb:mb+mb_size,:,:]  # Input minibatch\n",
    "        T_mb = y_train[mb:mb+mb_size,:,:]  # Target minibatch\n",
    "        V_tmp = [v * momentum_term for v in Vs]\n",
    "        # Update each parameters according to previous gradient\n",
    "        for pIdx, P in enumerate(RNN.get_params_iter()):\n",
    "            P += V_tmp[pIdx]\n",
    "        # Get gradients after following old velocity\n",
    "        # Get the parameter gradients\n",
    "        backprop_grads = RNN.getParamGrads(X_mb, T_mb)    \n",
    "        # Update each parameter seperately\n",
    "        for pIdx, P in enumerate(RNN.get_params_iter()):\n",
    "            #P -= learning_rate * backprop_grads[pIdx]\n",
    "            # Update the Rmsprop moving averages\n",
    "            maSquare[pIdx] = lmbd * maSquare[pIdx] + (\n",
    "                1-lmbd) * backprop_grads[pIdx]**2\n",
    "            # Calculate the Rmsprop normalised gradient\n",
    "            pGradNorm = ((\n",
    "                learning_rate * backprop_grads[pIdx]) / np.sqrt(\n",
    "                maSquare[pIdx]) + eps)\n",
    "            # Update the momentum\n",
    "            Vs[pIdx] = V_tmp[pIdx] - pGradNorm     \n",
    "            P -= pGradNorm   # Update the parameter\n",
    "        # Add loss to list to plot\n",
    "        ls_of_loss.append(RNN.loss(RNN.getOutput(X_mb), T_mb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAm8AAAGUCAYAAABji8XPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAIABJREFUeJzs3Xl4TGf/BvB7ZpLJZCHREEu1KlRL7EKUCCK2EEFssUcrYm2lRVBetBTV0FJU7dVFtWntpZZS+9baGu1rD4mELGSf7fn94TU/qSSGzHYm9+e6elVyzpzznfkmM3eeszwyIYQAEREREUmC3NoFEBEREZHxGN6IiIiIJIThjYiIiEhCGN6IiIiIJIThjYiIiEhCGN6IiIiIJIThjYhM4vTp0+jQoQMaNWqEPXv2PLE8MDAQR44csUJllvXaa6/hxo0bJt1mXFwcwsPDTbpNU2nUqBESEhKstv9Tp06hY8eOVts/kTU4WLsAIqkKDAzEvXv3oFAooFAoULNmTYSGhqJv376Qy0vf30WfffYZBgwYgCFDhli7FLKgP/74w/DvmJgYVKxYEePHjzfb/l577TXs3r0b1apVAwD4+vpi165dZtsfkS1ieCMqgeXLl6NFixbIzMzEiRMnMHv2bJw7dw4fffSRyfYhhIAQwuYDYWJiIl599VVrl2ExWq0WDg7Segu19Z8lKb6mRNZgm7/BRBJTpkwZtGvXDosWLcJPP/2Ef/75BwCgVqsxb948tGnTBi1atMD06dORl5dneNyePXsQGhqKxo0bIygoCAcPHgQADBo0CAsXLkS/fv3QoEEDJCQkIDMzE1OmTIG/vz9atWqFhQsXQqfTAQBu3ryJwYMHw8/PD35+fnj33Xfx4MEDw35WrFiBVq1aoVGjRujYsSOOHj0KANDr9VixYgWCgoLg5+eHt99+GxkZGUU+z++//x7t27dHs2bNEBUVheTkZABAUFAQEhISEBUVhUaNGkGtVhf7eqnVasyePRv+/v7w9/fH7NmzDY9JS0vDiBEj4Ovri2bNmqF///7Q6/XFPo9/y8zMxMSJE9G8eXO0bdsWS5cuhV6vh1qthq+vr6E/j/ZXv359pKamAgD279+P0NBQ+Pr6ol+/frh06ZJh3cDAQKxYsQIhISFo2LAhtFptofs/cOAA2rVrBz8/P8ybN89Q/9P6lJSUhDFjxqB58+bw8/PDrFmzCt3+vHnzEB4ejszMTMTFxaFfv36YNWsWmjRpgk6dOhV4XQr7WUpOTkZUVBSaNWuG9u3b4/vvvzesv3jxYowbNw7vvPMOGjVqhB49ehR4Df7t0WHijRs3YuvWrVi1ahUaNWqEqKgoAEBycjLGjh2L5s2bIzAwEOvXr39iX++99x4aN26Mn376CefOnUPfvn3h6+sLf39/zJo1y/CzMWDAAABAaGgoGjVqhB07duD48eMICAgwbPPKlSsYNGgQfH190aVLF+zdu9ewLCYmBjNnzkRkZCQaNWqE3r174+bNm0U+NyKbJYjoubRt21YcPnz4ie+3bt1afP3110IIIWbPni1GjBgh0tPTRWZmphgxYoRYsGCBEEKIs2fPisaNG4tDhw4JnU4n7ty5Iy5fviyEEGLgwIGidevW4p9//hEajUao1WoxatQoMW3aNJGdnS3u3bsnwsLCxLfffiuEEOL69evi0KFDIj8/X6Smpor+/fuLDz/8UAghxJUrV0RAQIC4c+eOEEKIhIQEcePGDSGEEGvXrhW9e/cWSUlJIj8/X0ybNk2MHz++0Od75MgR0axZM3HhwgWRn58vZs2aJfr37//U16Ow5YsWLRK9e/cW9+7dE6mpqaJv375i4cKFQgghFixYIKZNmybUarVQq9Xi5MmTQq/XF/s8/m3ChAkiKipKZGZmioSEBNGhQwfx/fffCyGEiImJEbGxsYZ1N2zYIIYNGyaEEOLixYuiefPm4s8//xRarVbExcWJtm3bivz8fMNz6Natm0hMTBS5ubmF7rtWrVpi4MCBIj09Xdy+fbvAvovrk1arFSEhIWL27NkiOztb5OXliZMnTwohhPjxxx9Fv379hE6nE1OnThXDhg0TOTk5hmW1a9cWa9asEWq1Wmzfvl00btxYpKenCyEK/1nq37+/+M9//iPy8vLEX3/9Jfz8/MSRI0eEEEJ89tlnok6dOmLnzp1CrVaLlStXirZt2wq1Wl3k871+/boQQohJkyYVeG11Op3o0aOHWLx4scjPzxc3b94UgYGB4uDBgwX29euvvwqdTidyc3PF+fPnxR9//CE0Go1ISEgQnTp1EmvWrCl0f0IIcezYMdGqVSshhBBqtVoEBQWJZcuWifz8fHHkyBHRsGFDceXKFUN9zZo1E2fPnhUajUZER0eLd955p9DnRWTLOPJGZGJeXl64f/8+hBD4/vvvMWXKFHh4eMDNzQ0jRozA9u3bAQA//PADwsLC0LJlS8jlclSsWBE1atQwbKdHjx549dVX4eDggPv37+PAgQOYMmUKXFxc4OnpiaFDhxq2Va1aNbRs2RJKpRIvvPACIiIicPLkSQCAQqGAWq3GlStXoNFoULVqVbz88ssAgO+++w7jx49HpUqVoFQqMWbMGOzatavQEaWtW7ciLCwMPj4+UCqViI6Oxp9//olbt24982u0detWjB49Gp6ennjhhRcwevRobNmyBQDg4OCAu3fvIjExEY6OjvD19YVMJiv2eTxOp9Nhx44dePfdd+Hm5oaqVasiIiLCsP2QkBDD6/aolpCQEADAxo0b0bdvXzRo0AAKhQI9evSAo6Mj/vzzT8P6gwYNQuXKlaFSqYp8fsOHD4eHhweqVKmCwYMHY9u2bQCK79O5c+eQkpKCiRMnwsXFBU5OTvD19TVsU6vVIjo6Gvfv38eyZcvg7OxsWPbCCy9gyJAhcHR0RHBwMKpXr47ffvvNsPzxn6V79+7hzJkzeO+99+Dk5ITatWujd+/e2Lx5s2F9Hx8fdOrUCY6OjoiIiIBarcbZs2ef3th/OX/+PNLS0jBmzBgolUq89NJL6NOnD3bs2GFYp2HDhggKCoJcLodKpULdunXRsGFDODg4oGrVqujbt6/hNXqas2fPIicnB5GRkVAqlXjjjTfQtm3bAv0OCgpC/fr14eDggG7duiE+Pv6ZnxeRtfHkAiITS05Ohru7O9LS0pCbm4uePXsalgkhDIfQkpKS0Lp16yK3U7lyZcO/ExMTodVq4e/vb/ieXq83rHPv3j3Mnj0bp06dQnZ2NoQQKFu2LICHgWHKlClYvHgxLl++DH9/f8OJ5YmJiRg9enSBc6DkcjlSU1NRsWLFAvWkpKTAx8fH8LWrqys8PDyQnJyMqlWrPtNrlJKSgipVqhi+rlKlClJSUgAAb775JpYsWYJhw4YBAPr27YvIyMhin8fj0tPTodFontj+o0O8fn5+yMvLw9mzZ+Hp6YlLly4hKCjI8Dr//PPP2LBhg+GxGo3GUBtQsC9FeXydF1980fD44vqUlJSEKlWqFHnO182bN3Hp0iVs2rQJSqWywLKKFStCJpMV+nr+u56UlBS4u7vDzc2twPoXLlwwfF2pUiXDvx/9YfH49ox1+/ZtpKSkFAihOp2uwNeP7wsArl27hrlz5+LChQvIzc2FTqcr8HNXnJSUFFSqVKnAz/PjvQeA8uXLG/6tUqmQk5PzzM+LyNoY3ohM6Ny5c0hOTkaTJk1Qrlw5qFQqbN++/YmAATz8QC3ufJvHP4wfjYwdO3as0A/32NhYyGQybN26FR4eHtizZ0+B86VCQkIQEhKCrKwsTJ8+HQsWLMDHH3+MSpUqYc6cOWjSpMlTn5uXlxdu375t+DonJwcZGRmFPjdjtvX4BQ5JSUnw8vICALi5uSEmJgYxMTH4559/MGTIENSrVw9vvPFGkc/jceXKlYOjoyMSExNRs2ZNw/Yf1alQKNCpUyds27YN5cuXR5s2bQxBpnLlyoiKisLIkSOLrP3xvhQlKSnJ8NwSExMNz624PlWuXBlJSUlFnrTv7e2NAQMGYPjw4Vi3bh28vb0Ny5KTkyGEMNSWlJSEwMDAQmt+NDKclZVleN6Pvz4AcOfOHcO/9Xo9kpOTDc+hOP9+bSpXroyqVati9+7dRj9mxowZqFOnDj755BO4ublh7dq1Rl9N6uXlhTt37kCv1xsCXFJSEl555RWjHk8kFTxsSmQCWVlZ2L9/P6Kjo9GtWze89tprkMvl6N27N+bMmWM4GT45ORm///47AKBXr16Ii4vD0aNHDR+QV65cKXT7Xl5eaNmyJebOnYusrCzo9XrcvHkTJ06cAABkZ2fDxcUFZcqUQXJyMlauXGl47NWrV3H06FGo1WoolUo4OTkZPtjCw8OxaNEiQyhLS0sr9B5tANC1a1fExcUhPj4earUasbGxqF+//jOPugFAly5dsGzZMqSlpSEtLQ2ff/654dDl/v37cePGDQghUKZMGSgUCshksmKfx+MehbOFCxciKysLt2/fxpo1a9CtWzfDOiEhIdi5cye2bt2Krl27Gr7fu3dvfPfddzh79iyEEMjJycFvv/2GrKysZ3p+q1atwv3795GUlIT169cjODgYQPF9ql+/PipUqIBPPvkEOTk5yM/Px+nTpwtst2vXroiOjkZERESB4J+Wlob169dDo9Fg586duHLlSpGjupUrV0ajRo0QGxuL/Px8XLp0CT/88EOB1+fixYvYvXs3tFot1q1bB6VSiQYNGjz1eXt6ehY4jF6/fn24urpixYoVyMvLg06nwz///INz584VuY3s7Gy4urrC1dUVV65cwbfffltgefny5Yu8r1z9+vWhUqmwcuVKaDQaHD9+HPv27TO8/kT2guGNqAQeXV3ZunVrLF++HBEREQVuEzJhwgRUq1YNffr0QePGjTF06FBcu3YNwMMPmo8++sgw8jVw4EAkJiYWua/58+dDo9EgODgYTZs2xbhx43D37l0AwJgxY/DXX3/B19cXkZGR6NChg+FxarUan3zyCfz8/ODv74+0tDRER0cDAAYPHozAwEAMGzYMjRo1Qp8+fYr8YG3RogXefvttjB07Fv7+/khISMDChQuf63UbNWoU6tati27duqFbt27w8fHBqFGjAAA3btxAREQEGjVqhL59+yI8PBzNmzcv9nn827Rp0+Ds7IygoCD0798fXbt2RVhYmGF5gwYN4OzsjJSUlAJXKtarVw8ffPABZs2ahaZNm6JDhw6Ii4t75ufXrl079OzZE927d0ebNm3Qq1cvAMX3SaFQYPny5bhx4wbatm2LgIAA7Ny584lt9+jRA6NHj8aQIUMMQal+/fq4ceMGmjdvjkWLFuGzzz5DuXLliqwvNjYWt2/fRqtWrTBmzBiMHTsWLVq0KFD/jh070LRpU2zevBmLFy+Go6PjU593r169cPnyZfj6+mLUqFGG53Tp0iW0a9cOzZs3x/vvv19sGJ40aRK2bduGxo0bY9q0aU8ErzFjxiAmJga+vr4Fzp0DAKVSieXLl+PgwYNo3rw5Zs6cifnz5xc4l5TIHsiEEMLaRRAR0fOJi4vDpk2bnhihel6LFy/GjRs3sGDBApNsj4hMjyNvRERERBJi0QsWkpKSMHHiRKSmpkImk6FPnz5PTKWTmZmJCRMmIDExETqdDsOGDStwuIOIiIioNLPoYdOUlBTcvXsXPj4+yMrKQlhYGD7//HPDFWHAw+mGHgW4tLQ0dOrUCYcOHXri0ngiIiKi0siih029vLwM9+txc3ODt7d3gfvvAA8vG390/6Ps7Gy4u7tzrjsiIiKi/7FaKrp16xbi4+OfuPx8wIABGDlyJFq1aoXs7GwsXLjQZidRJiIiIrI0q4S37OxsjBs3DlOmTClwl28AOHToEGrXro3169fj5s2biIiIgK+v7xPrPU4IAV4zK10yGdg/iWLvpI39kzb2T7rk8qff7Ls4Fg9vGo0G48aNQ0hISIF7HD0SFxeHyMhIyGQyVKtWDVWrVsXVq1dRv379IrcpBJCa+mw30STb4eHhgowMTlEjReydtLF/0sb+SVeFCmVK9HiLHo8UQmDq1Knw9vZGREREoetUrlwZR48eBfBwHsBr16491x3ciYiIiOyRRa82PXXqFAYMGIBatWoZzmOLjo423FU+PDwcycnJmDx5Mu7evQshBIYPH47Q0NBit6vXC468SRj/epQu9k7a2D9pY/+kq6Qjb3YxwwLDm7TxDUi62DtpY/+kjf2TLkkdNiUiIiKikmF4IyIiIpIQhjciIiIiCWF4IyIiIpIQhjciIiIiCWF4IyIiIpIQhjciIiIiCWF4IyIiIpIQhjciIiIiCWF4IyIiIpIQuwhvFy8CWZwdi4iIiEoBuwhv+fkyrF6ttHYZRERERGZnF+GtbFmBpUsdOfpGREREds8uwluVKkBamhyrVnH0jYiIiOybXYQ3FxegXTstli5VIjPT2tUQERERmY9dhDcAmDAhH+npMo6+ERERkV2zm/DWuLEe7dtz9I2IiIjsm92EN+Dh6FtGhgxffsnRNyIiIrJPdhXeGjbUo2NHLZYtU+LBA2tXQ0RERGR6dhXegIejb/fvy7BiBUffiIiIyP7YXXirX1+PTp00WL5cifv3rV0NERERkWnZXXgDgAkT1HjwQIYvvuDoGxEREdkXuwxv9erpERyswRdfKJGebu1qiIiIiEzHLsMbAEycqEZWFrBsGUffiIiIyH7YbXirU0eP0FAtVqxQ4t49mbXLISIiIjIJuw1vwMNz3/LygCVLOPpGRERE9sGuw9urr+rRs6cWa9Y4IjmZo29EREQkfXYd3gDgvffyoVYDixdz9I2IiIikz+7Dm7e3QN++Gqxb54jERI6+ERERkbTZfXgDgOhoNXQ6YNEijr4RERGRtJWK8PbyywL9+2vw9deOSEjg6BsRERFJV6kIbwAwfrwaMhmwcCFH34iIiEi6Sk14e/FFgcGDNfj2W46+ERERkXRZNLwlJSVh0KBBCA4ORpcuXbBu3bpC1zt+/DhCQ0PRpUsXDBw40GT7HzlSDSGAdescTbZNIiIiIktysOTOFAoFYmJi4OPjg6ysLISFhaFly5aoWbOmYZ0HDx5g5syZWLlyJapUqYLU1FST7f+llwQ6ddJiwwZHvPuuGs7OJts0ERERkUVYdOTNy8sLPj4+AAA3Nzd4e3sjOTm5wDpbt25F+/btUaVKFQCAp6enSWt46y0N0tLk+Plni+ZWIiIiIpOw2jlvt27dQnx8PBo0aFDg+9evX8eDBw8waNAg9OzZEz///LNJ99uypQ61a+uwcqUSQph000RERERmZ5Xhp+zsbIwbNw5TpkyBm5tbgWU6nQ4XL17E2rVrkZeXh379+qFBgwaoXr16kduTyQAPDxej9z9mjAyjR8sRH++CFi2e+2mQiSgU8mfqH9kO9k7a2D9pY/9KL4uHN41Gg3HjxiEkJAQdOnR4YnmlSpXg4eEBFxcXuLi4wNfXF5cuXSo2vAkBZGTkGF1DcDDg7u6GRYv0qFMn77meB5mOh4fLM/WPbAd7J23sn7Sxf9JVoUKZEj3eoodNhRCYOnUqvL29ERERUeg67dq1w+nTp6HVapGbm4tz586hRo0aJq3D1RUID9dg2zYH3LnD24YQERGRdFh05O306dPYvHkzatWqhdDQUABAdHQ0EhMTAQDh4eGoUaMGWrVqhW7dukEul6NXr16oVauWyWuJiFDjiy8csXatI2Ji1CbfPhEREZE5yISQ/mn7er1AamrWMz9uwABn/PGHHH/8kQ0nJzMURkbh0L90sXfSxv5JG/snXZI6bGpr3nxTjXv35NiyhbcNISIiImko1eGtTRsdatTQY9UqzndKRERE0lCqw5tc/nD07cwZBc6cKdUvBREREUlEqU8sfftq4OoqsHo1R9+IiIjI9pX68FamDNCzpwZbtzogM9Pa1RAREREVr9SHN+DhPd9yc2XYvNnR2qUQERERFYvhDUCTJnq8+qoO333Hq06JiIjItjG84eHcqH37anHihAOuXOGMC0RERGS7GN7+p08fDeRygY0beeiUiIiIbBfD2/9UqiTQtq0OGzc6QqezdjVEREREhWN4e0x4uAZJSXIcPKiwdilEREREhWJ4e0zHjlp4eAh89x0PnRIREZFtYnh7jJPTw3u+7djhgIwMa1dDRERE9CSGt38JD9cgP1+Gn3/m6BsRERHZHoa3f6lfX4/atXU8dEpEREQ2ieHtX2QyoF8/Dc6cUeDvv/nyEBERkW1hOilEr15aODjwwgUiIiKyPQxvhahQQSAoSItNmxyg1Vq7GiIiIqL/x/BWhH79tEhJ4T3fiIiIyLYwvBUhMFALV1eB7ds5WT0RERHZDoa3IqhUQFCQFr/84sDpsoiIiMhmMLwVo3NnLe7elePUKR46JSIiItvA8FaMoCAtHB0FduzgoVMiIiKyDQxvxShbFmjVSocdOxwghLWrISIiImJ4e6rgYC1u3JAjPp4vFREREVkfE8lTdOqkhUzGQ6dERERkGxjensLLS6BpUx3DGxEREdkEhjcjdO6sxYULCty4IbN2KURERFTKMbwZITj44RxZO3dy9I2IiIisi+HNCNWrC9SurWN4IyIiIqtjeDNScLAWx48rcO8eD50SERGR9TC8GSk4WAu9XoZduzj6RkRERNZj0fCWlJSEQYMGITg4GF26dMG6deuKXPfcuXOoU6cOfvnlFwtWWLS6dfV4+WU9rzolIiIiq7JoElEoFIiJiYGPjw+ysrIQFhaGli1bombNmgXW0+l0WLBgAVq2bGnJ8oolkz286nTNGkdkZQFubtauiIiIiEoji468eXl5wcfHBwDg5uYGb29vJCcnP7HeV199hY4dO8LT09OS5T1VcLAWarUMe/dy9I2IiIisw2rnvN26dQvx8fFo0KBBge8nJydjz549CA8Pt1JlRWvWTIfy5fX48UcH6PXWroaIiIhKI6sMIWVnZ2PcuHGYMmUK3P51/HH27Nl47733IJcbnytlMsDDw8XUZRZq8GAgNtYR3bo5YPFiPRo1sshu7ZpCIbdY/8i02DtpY/+kjf0rvWRCCGHJHWo0GkRFRcHf3x8RERFPLA8MDDT8Oz09HSqVCh988AGCgoKK3KZeL5CammWWev9NCGDTJgfMnOmEe/dkGDpUg8mT8+HhYZHd2yUPDxdkZORYuwx6DuydtLF/0sb+SVeFCmVK9HiLhjchBCZNmgR3d3dMnTr1qevHxMSgTZs26NSpU7HrWTK8PXL/PjB/vhNWrXJEuXIC06blo18/LZ5hwJD+h29A0sXeSRv7J23sn3SVNLxZ9LDp6dOnsXnzZtSqVQuhoaEAgOjoaCQmJgKATZ7nVhR3d2D27HyEh2sQE+OEd95xRk5OHt56S2Pt0oiIiMiOWfywqTlYY+St4P6Brl1dkJ4uw5Ej2ZBxEoZnwr8epYu9kzb2T9rYP+kq6cgbD/KZgFwODB2qxpUrchw+rLB2OURERGTHGN5MJCREi3LlBNatc7R2KURERGTHGN5MxNkZ6NNHg+3bHZCSwuOmREREZB4MbyY0ZIgaWq0M337L0TciIiIyD4Y3E6pZU8DfX4uvvnLkDAxERERkFgxvJjZkiAY3b8rx22+8cIGIiIhMj+HNxDp31qJ8eT3WruWhUyIiIjI9hjcTUyqB/v012L3bAYmJvHCBiIiITIvhzQwGDtRACGDDBo6+ERERkWkxvJnBK68ItG2rw9dfO0KrtXY1REREZE8Y3sxkyBANkpLk+PVXi04fS0RERHaO4c1M2rfXonJlPWdcICIiIpNieDMTBwdgwAAN9u9X4Pp1XrhAREREpsHwZkaDBmkglwPr1imtXQoRERHZCYY3M6pcWaBzZy2++cYRubnWroaIiIjsAcObmUVEaJCeLsOWLbxwgYiIiEqO4c3M/P11ePVVHdau5aFTIiIiKjmGNzOTyR6Ovp0+rcDZs3y5iYiIqGSYJiygTx8NXFwE5zslIiKiEmN4s4CyZYGwMA3i4hyRkWHtaoiIiEjKGN4sZOhQDXJzZdi4kaNvRERE9PwY3iykXj09mjbVYc0aJfR6a1dDREREUsXwZkEREWpcvSrHwYMKa5dCREREEsXwZkEhIVp4euqxZg0PnRIREdHzYXizICenh/Od7trlgNu3Od8pERERPTuGNwsbMkQDIcDRNyIiInouDG8W9tJLAt26abFqlRL37nH0jYiIiJ4Nw5sVTJigRm4usGQJp8wiIiKiZ8PwZgW1aunRq5cWq1c7IjmZo29ERERkPIY3K3n33XxotcCiRRx9IyIiIuMxvFlJ9eoC4eEarF/viIQEjr4RERGRcRjerCg6Wg2ZDFi4kKNvREREZByjwtvYsWNx4MAB6Dmvk0m9+KLA0KEafPutI65e5egbERERPZ1R4S0jIwNRUVEICAjAggULcPXq1efaWVJSEgYNGoTg4GB06dIF69ate2KdLVu2ICQkBCEhIejXrx8uXbr0XPuSirFj1XByAj7+2MnapRAREZEEyIQQwpgVExISEBcXh82bNyMpKQkNGjRAWFgYOnfuDDc3N6N2lpKSgrt378LHxwdZWVkICwvD559/jpo1axrWOXPmDGrUqAF3d3ccOHAAS5YswaZNm4rdrl4vkJqaZVQNtuiDD5RYskSJAwdy8PrrpW9008PDBRkZOdYug54Deydt7J+0sX/SVaFCmRI93uhz3l566SW8/fbb2LdvH1avXo2XX34Zc+bMQatWrTBp0iQcP378qdvw8vKCj48PAMDNzQ3e3t5ITk4usE7jxo3h7u4OAGjYsCHu3LnzLM9HkkaPVsPVFZg/n+e+ERERUfGe64KFhg0bws/PD9WrV0dubi6OHTuGIUOGIDQ0FH/99ZdR27h16xbi4+PRoEGDItf54YcfEBAQ8DwlSsoLLwBRUWps2+aIY8cU1i6HiIiIbJjRh00B4MSJE/jpp5+wa9cuODg4IDg4GL169ULdunVx+fJlfPDBB0hLS8PWrVuL3U52djYGDRqEqKgodOjQodB1jh07hpkzZ+Kbb75BuXLlit2eEAJarbQPN2ZlAU2ayCGXA6dO6eHqau2KLEehkEOnk3b/Siv2TtrYP2lj/6TL0bFkAzVGhbdNDrgJAAAgAElEQVQlS5Zg8+bNSEhIQNOmTQ3nujk5FTzJ/syZMxgwYADi4+OL3JZGo0FUVBT8/f0RERFR6DqXLl3CmDFj8OWXX6J69epPfRJSP+ftkSNHFOje3QVvvqnGRx/lW7sci+F5G9LF3kkb+ydt7J90lfScNwdjVtq4cSN69OiBsLAwVKtWrcj1vL29MWfOnCKXCyEwdepUeHt7FxncEhMTMXbsWMyfP9+o4GZPWrTQITJSjRUrlAgO1qJVK521SyIiIiIbY9TIm16vh1xe8vv5njp1CgMGDECtWrUM24uOjkZiYiIAIDw8HFOnTsXu3btRpUoVAIBCoUBcXNxT6rOPkTcAyMkBAgNdodEABw5kw8gLeSWNfz1KF3snbeyftLF/0lXSkbdnOuft6tWrOH/+PO7evYsKFSqgbt26qFGjRokKMAV7Cm8AcPKkHCEhLhgwQINPPrH/w6d8A5Iu9k7a2D9pY/+kyyKHTbOysvD+++9j9+7d0Ov1cHFxQU5ODuRyOdq3b4/Zs2cbfa83erqmTfUYOVKDzz9XoksXLQIDefiUiIiIHjLqWOiMGTNw+PBhzJs3D3/++SfOnDmDP//8E3PnzsWRI0cwY8YMM5dZ+kyalI/XXtNh/HgV7t+3djVERERkK4wKb3v37sXEiRMREhIClUoFAFCpVOjWrRsmTJiAvXv3mrXI0kilAhYvzkNKigwzZnDqLCIiInrIqPDm6uqKChUqFLrMy8sLLi4uJi2KHmrYUI+oKA2+/lqJs2dLfsEIERERSZ9RiaB///5YtWoV8vLyCnw/NzcXq1atQnh4uFmKIyA6Oh/ly+vx/vtOMP7SEiIiIrJXRl2wkJmZiRs3bqB169Zo2bIlXnjhBaSlpeHw4cNQqVSoW7cu5s+fDwCQyWSYMGGCWYsuTcqUAaZMUSM6WoWtWx3QrZvW2iURERGRFRl1q5DAwEDjNyiTWfwcOHu7Vci/6XRAUJALHjyQ4fDhbPzvtEO7wcvdpYu9kzb2T9rYP+myyK1C9u3bV6KdUMkoFMAHH+SjZ08XfPGFEm+/rbZ2SURERGQlPAteIvz9dejcWYNFi5RITpZZuxwiIiKyEqNG3gAgISEBK1euxJkzZ5CRkQEPDw80adIEb775Jl566SVz1kj/85//5KNVKwfMnavEwoX2P/MCERERPcmokbcLFy4gNDQUu3fvRt26ddG9e3fUrVsXu3fvRvfu3XHx4kVz10kAvL0Fhg/X4JtvHHH+PAdNiYiISiOjLlgYNGgQhBD48ssv4ezsbPh+bm4uIiMjIZPJsH79erMWWhx7v2DhcQ8eAM2bu6JWLT1++ikXMjs4gsqTbqWLvZM29k/a2D/pKukFC0YN35w/fx5vvfVWgeAGAM7Ozhg2bBjOnTtXoiLIeGXLApMmqXHkiAM+/FAJHac9JSIiKlWMCm9OTk7IyMgodNn9+/fh5MTpmyxp4EANBg1SY/FiJ4SHOyMtzdoVERERkaUYFd7atGmDBQsW4NSpUwW+f+rUKXzyySdo27atWYqjwikUwCef5CM2Ng9HjijQoYMrzp3jOXBERESlgVHnvKWnp2PUqFH4888/4enpaZhhITU1FQ0bNsTSpUtRrlw5S9RbqNJ0ztu/nTkjx7BhzkhLk2H+/Dz06ye9GRh43oZ0sXfSxv5JG/snXSU9582o8PbIwYMHcf78edy9excVKlRAgwYN4O/vX6ICTKE0hzcAuHtXhhEjVDh0yAEjRqgxa1a+pC5k4BuQdLF30sb+SRv7J11mn2FBrVZj1apVaNu2LQICAhAQEFCiHZLpVagg8P33uZg2zQlffKHEa6/pMXCgxtplERERkRk89UQppVKJ5cuX48GDB5aoh56TgwPw4Yf5CAjQYsoUJ1y8yHPgiIiI7JFRn/D169fHX3/9Ze5aqIQUCmDp0jyULSswfLgKWaX3SDIREZHdMiq8TZgwAd988w02bNiAhIQE5OTkIDc3t8B/ZBu8vASWL8/D1atyTJyogvFnNBIREZEUGHXBwuuvv/7/DyjiTPj4+HjTVfWMSvsFC4VZsECJ+fOdsGhRLvr3t+0rUHnSrXSxd9LG/kkb+yddZr9gAQDmzJlTZGgj2zR+vBpHjyowebIKjRrloHZtvbVLIiIiIhN4pluF2CqOvBUuJUWGtm1d4OEhsGtXDtzcrF1R4fjXo3Sxd9LG/kkb+yddFpnbtF27drh06VKhy/755x+0a9euREWQeTw6/+3yZTkWLOAUZkRERPbAqPB2+/ZtqNXqQpfl5eUhOTnZpEWR6bRqpUOvXlqsWeOIu3d56JuIiEjqijznLSsrq8C93e7evYvExMQC6+Tn52P79u3w8vIyX4VUYtHR+fjxRwd8/rkSM2bkW7scIiIiKoEiw9vatWuxZMkSyGQyyGQyjBkzptD1hBCIiYkxW4FUcjVqCPTs+XD0bfRoNSpUkPxpjkRERKVWkeGta9euqFu3LoQQGDlyJCZNmoTq1asXWMfR0RHVq1dHlSpVzF4olUx0dD7i4hywdKkS//kPR9+IiIikyqirTU+cOIE6derAzUYvV+TVpsYZOVKFnTsdcPJktk2NvvGKKeli76SN/ZM29k+6LHK1abNmzQzBTafTPTG7AmdYkIboaDXy8oClS5XWLoWIiIiek1E36c3KykJsbCx2796NtLQ0FDZYZ80ZFsg4r76qR48e/3/uW/nytjP6RkRERMYxKrxNnz4d+/fvR+/evVGzZk04Ojo+186SkpIwceJEpKamQiaToU+fPhgyZEiBdYQQmD17Ng4cOACVSoW5c+fCx8fnufZHT4qOVv/v3DdHTJ9e+O1fiIiIyHYZFd4OHTqEKVOmoHfv3iXamUKhQExMDHx8fJCVlYWwsDC0bNkSNWvWNKxz8OBBXL9+Hbt378bZs2cxY8YMbNq0qUT7pf/3aPRt9WolRo3ScPSNiIhIYow6583Z2RkVK1Ys8c68vLwMo2hubm7w9vZ+4ga/e/fuRffu3SGTydCwYUM8ePAAKSkpJd43/b9331UjNxdYtuz5RlCJiIjIeowKbxEREfjmm2+g15tucvNbt24hPj4eDRo0KPD95ORkVKpUyfB1pUqVOIODiT0afVu1SomrVznrAhERkZQYddg0OTkZf//9Nzp16gQ/Pz+UKVPwEleZTIYJEyYYvdPs7GyMGzcOU6ZMMcntR2Syh5dMk/HmzQN++w146y1XHDqkh4sVXz6FQs7+SRR7J23sn7Sxf6WXUeFt165dkMlk0Gq1OHz48BPLnyW8aTQajBs3DiEhIejQocMTyytWrIg7d+4Yvr5z585TD9kKAd7r5hm5uwNLlyoQHu6Mt97S4/PP8yCz0iAc71UkXeydtLF/0sb+SVdJ7/NmVHjbt29fiXbyiBACU6dOhbe3NyIiIgpdJzAwEBs2bECXLl1w9uxZlClThnOnmklgoA6TJqkxd64TmjTR4c03NdYuiYiIiJ7CqPBmKqdPn8bmzZtRq1YthIaGAgCio6MNE96Hh4ejdevWOHDgANq3bw9nZ2fMmTPHkiWWOu+8o8aZMwpMm+aEevV0aNbMdOc1EhERkekZNT0WAFy6dAnLly/HhQsXcOfOHWzcuBE+Pj5YuHAhGjdujNatW5u71iJxeqySuX8faN/eFXl5wK+/5qBiRcvePoRD/9LF3kkb+ydt7J90WWR6rAMHDiAsLAz37t1D9+7dodVqDcscHR2xYcOGEhVB1uXuDqxenYv792WIjFRBw6OnRERENsuo8BYbG4sePXpgw4YNiIqKKrCsdu3anBrLDtStq8eCBXk4etQB06c7wbjxWCIiIrI0o8Lb1atXERwcDODhlaWPc3Nzw/37901fGVlc795aREWpsWqVEgsXcvJ6IiIiW2TUBQuenp5ISEgodNnly5dRpUoVkxZF1jNjRj5SU2WYO9cJ5coJRETwGCoREZEtMWrkLTg4GJ999hlOnTpl+J5MJsO1a9fw5ZdfIiQkxGwFkmXJ5cCiRXno2FGLmBgnxMVZ9IJkIiIiegqjrjZVq9UYO3YsDh48iPLly+Pu3buoVKkS7t27h5YtW2LJkiVwdLTePJm82tT0cnOBfv2ccfKkAhs25CIwUGe2ffGKKeli76SN/ZM29k+6Snq1qdG3CgGAo0eP4ujRo0hPT4e7uzveeOMNtGzZskQFmALDm3k8eAD06OGCy5fl2LQpx2z3gOMbkHSxd9LG/kkb+yddFg1vtorhzXzu3pUhJMQFqaky/PJLNmrUMP2PC9+ApIu9kzb2T9rYP+myyH3eqPSqUEHg++9zIJcDkZHOyM+3dkVERESlG8MbPdXLLwt89lkuzp9X4IMPnKxdDhERUanG8EZG6dhRh8hINVasUOKXXxTWLoeIiKjUYngjo02blo969XR4+21nJCbKnv4AIiIiMjmGNzKakxPw5Ze5UKuBqCgVHpviloiIiCzEqPC2a9cubNq0yfB1QkIC+vXrB19fX4wdOxYPHjwwW4FkW7y9BebPz8OxYw6IjeUUWkRERJZmVHhbtmwZsrOzDV9/+OGHSE9PR2RkJC5evIiFCxearUCyPb17a9G3rwaxsUocPszz34iIiCzJqPCWkJCAWrVqAQAyMzNx+PBhTJ48GZGRkRg/fjz2799v1iLJ9nz0UR68vfUYPlyF69d5/hsREZGlGH3Om0z28AP6xIkTkMvlaNGiBQCgUqVKSEtLM091ZLPc3IB16/Kg08nQr9/Dm/gSERGR+RkV3l5//XVs2bIFOTk52LRpE/z8/KBUPjzfKTExEZ6enmYtkmzTq6/qsX59LhITZRg40Bk5vNE3ERGR2RkV3saPH489e/agSZMmOHnyJMaOHWtYtnfvXtSvX99sBZJt8/PTYdmyPJw5I0dUlAo6881fT0RERHiGuU2zsrJw/fp1vPzyyyhbtqzh+wcOHMDLL7+M6tWrm63Ip+Hcpta3apUjJk9WYehQNebNy4fsGY6icn4+6WLvpI39kzb2T7pKOrepg7Erurm5oW7dugW+9+DBA7Ru3bpEBZB9ePNNDW7flmHJEie8+KLA22+rrV0SERGRXTLqsOk333yDL7/80vB1fHw8AgIC4Ofnh549e+LOnTtmK5Ck4/331QgL02D2bCds2WL03wVERET0DIwKbxs2bICbm5vh6w8//BBeXl5YsGAB9Ho9FixYYLYCSTrkcuDTT/Pg66vDO++ocO0ar0AlIiIyNaPCW1JSkuGctrS0NJw5cwYTJkxAly5dMGrUKBw7dsysRZJ0KJXAF1/kwsEBiIx0Rn6+tSsiIiKyL0aFN6VSCY1GAwA4duwYVCoVfH19AQDu7u7IzMw0X4UkOS+9JPDpp3k4e1aBDz5wsnY5REREdsWo8FavXj18/fXX+O9//4uvvvoKrVq1gkLxcFqkhIQEeHl5mbVIkp7OnbWIjFRjxQolduzg+W9ERESmYlR4i4mJweXLlxESEoI7d+5g/PjxhmU7d+5E48aNzVYgSde0aflo0ECHt99WISGB578RERGZgtH3eQOA9PR0eHh4GKbKAoC///4bFSpUwAsvvGCWAo3B+7zZrmvXZAgKckWtWnps2ZIDR8cn1+G9iqSLvZM29k/a2D/pKul93oye2xQAypUrh4yMDFy/fh3p6ekAgNdee82qwY1sW/XqArGxeTh9WoE5c3j+GxERUUkZfTLSjh07sHjxYly/ft3wvVdeeQXjxo1D586dzVEb2YnQUC0OHVLj88+V8PHRoVcvrbVLIiIikiyjwtu2bdvw3nvvISAgACNGjICnpydSU1OxY8cOREdHQ6/Xo0uXLuaulSRs9ux8/Pe/crzzjgovvZQLPz9OgkpERPQ8jDrnrWvXrmjcuDFmzZr1xLLp06fjzJkz2LZtm1kKNAbPeZOG9HSgc2dX3L8P7NiRg+rVH/7o8bwN6WLvpI39kzb2T7oscs7bjRs30LFjx0KXdezYETdu3ChREVQ6lCsHfPNNDvR6GQYOdEZGhrUrIiIikh6jwlv58uVx4cKFQpdduHAB5cuXN2pnkydPxhtvvIGuXbsWujwzMxNRUVHo1q0bunTpgh9//NGo7ZJ0eHsLrF2bi+vX5XjzTWf8797PREREZCSjwlvPnj2xePFiLF26FFeuXMH9+/dx9epVLF26FEuWLEFYWJhRO+vZsydWrlxZ5PKvv/4aNWrUwJYtW/DVV19h3rx5UKvVxj0Tkow33tAhNjYPv//ugIkTnWD8zWqIiIjIqAsWRo8eDa1Wiy+//BKLFy82fF+lUmHYsGEYPXq0UTtr2rQpbt26VeRymUyG7OxsCCGQnZ0Nd3d3ODjw7vz2qG9fLa5dy0dsrBOaNNFj4EBrV0RERCQNRiUjuVyO8ePHY9iwYfjvf/+LlJQUeHl54dVXX4W7u7vJihkwYABGjhyJVq1aITs7GwsXLoRc/ky3oiMJmTRJjfPnFXj/fQVatZKhWjUOwRERET3NU8Nbfn4+Ro4ciREjRsDPz88wIb05HDp0CLVr18b69etx8+ZNREREwNfXF25ubsU+TiZ7eNUNSc/y5UCDBsCUKa7Ytk0PGWfRkhSFQs7fPQlj/6SN/Su9nhrenJyccP78eej1erMXExcXh8jISMhkMlSrVg1Vq1bF1atXUb9+/WIfJwR4ubRElSkDfPihK955R46VK9Xo3Zs38JUS3qpA2tg/aWP/pMsitwoJDAzEnj17SrQjY1SuXBlHjx4FANy7dw/Xrl1D1apVzb5fsq4RIwSaNNFh+nQnpKZy6I2IiKg4Rt2kd+vWrZg/fz4aNWqEgIAAlC9fvsDk9ADQunXrp+4sOjoaJ06cQHp6Ojw9PTF27FhotQ9HWsLDw5GcnIzJkyfj7t27EEJg+PDhCA0Nfep2eZNeafPwcMHRo3kICnJB9+5afP55nrVLIiPxL39pY/+kjf2TrpKOvBkV3l5//fXiNyKTIT4+vkSFlATDm7Q9egOaO1eJ2FgnbNyYg7ZtOX2WFPDDQ9rYP2lj/6TLIuHt9u3bT93Qiy++WKJCSoLhTdoevQHl5QFt27pCowEOHMiGq6u1K6On4YeHtLF/0sb+SVdJw5tRtwqxZjCj0kOlAj75JA/du7tgwQIn/Oc/+dYuiYiIyOYUecFCSkoKxo4di99//73IB//+++8YO3YsUlNTzVIclT4tWugwcKAay5c74vhxhbXLISIisjlFhrfVq1cjISEB/v7+RT7Y398ft27dwurVq81SHJVOM2bk46WXBCIjVbh3j1efEhERPa7I8LZ//37069fviatKHyeTydC3b1/s3bvXLMVR6VS2LLBqVS7S0mQYNUoFHa9dICIiMigyvCUmJqJmzZpP3UCNGjWMuqCB6FnUq6fHnDn5+O03ByxapLR2OURERDajyPCmUqmQlfX0KzhzcnKgUqlMWhQRAAwcqEGvXhrMn6/EwYM8/42IiAgoJrzVqVMH+/bte+oG9u7dizp16pi0KCLg4Zy1H3+ch1q19IiKUuHOHZ7/RkREVGR469+/P3744Qf89NNPRT74559/RlxcHAYOHGiW4ohcXYGVK/OQkyPDiBEqaDn1KRERlXJF3uetY8eOGDx4MCZPnowNGzagVatWqFKlCmQyGRITE3Ho0CFcuHABQ4cORfv27S1ZM5Uyr72mx8cf52H0aGfMmaPE9Olqa5dERERkNU+dYWHfvn1Yt24d/vjjD6jVDz80lUolGjdujCFDhqBt27YWKbQ4nGFB2oy9S/iECU5Yt06JFSty0b07h+BsAe/wLm3sn7Sxf9JlkemxAECr1SIjIwMA4OHhAQcHoyZnsAiGN2kz9g1IrQZ69nTG+fMKbNuWg3r19BaojorDDw9pY/+kjf2TrpKGtyLPefs3BwcHlC9fHuXLl7ep4Ealh1IJrF6dh3LlBIYMccbdu7yAgYiISh+jwxuRLfDyEli3Lhf37snw5psqqHn6GxERlTIMbyQ5DRrosXBhHo4dc8D77ztZuxwiIiKL4vFPkqSwMC0uXszHkiVOqFtXj8GDNdYuiYiIyCI48kaSNXWqGoGBWkye7ITz5/mjTEREpQM/8UiyFApg+fJcuLoCH3/M+U+JiKh0YHgjSfPwAEaMUOOXXxw5+kZERKUCP+1I8t56S42yZQUWLuToGxER2T+GN5I8d3dg+HA1tm1zRHw8f6SJiMi+8ZOO7EJkpBpubhx9IyIi+8fwRnahXLmHh083b3bAP//wx5qIiOwXP+XIbowYoYGzMxAby9E3IiKyXwxvZDc8PQWGDVPj558dcPky5z0lIiL7xPBGdmXkSA2cnIBFizhtFhER2SeGN7IrFSoIDBmiwY8/OuDqVY6+ERGR/WF4I7szerQajo7Ap59y9I2IiOwPwxvZnYoVH46+bdzogLNn+SNORET2hZ9sZJcmTMiHp6fAxIkq6HTWroaIiMh0GN7ILpUtC8ycmY8//lBgwwZHa5dDRERkMgxvZLfCwrRo2VKL2bOdcO8eL14gIiL7YNHwNnnyZLzxxhvo2rVrkescP34coaGh6NKlCwYOHGjB6sjeyGTA3Ln5yMoCPviAFy8QEZF9sGh469mzJ1auXFnk8gcPHmDmzJlYtmwZtm/fjk8//dSC1ZE9eu01PUaOVOPbbx1x/LjC2uUQERGVmEXDW9OmTeHu7l7k8q1bt6J9+/aoUqUKAMDT09NSpZEdi45W48UX9Zg40QlarbWrISIiKhmbOuft+vXrePDgAQYNGoSePXvi559/tnZJZAdcXYEPP8xHfLwCK1fy4gUiIpI2B2sX8DidToeLFy9i7dq1yMvLQ79+/dCgQQNUr1692MfJZICHh4uFqiRTUyjkZu9f//7A998LzJ/vhD59HOHtbdbdlRqW6B2ZD/snbexf6WVT4a1SpUrw8PCAi4sLXFxc4Ovri0uXLj01vAkBZGTkWKhKMjUPDxeL9G/mTBnatHFFw4ZyRERoMHasGuXLC7Pv155ZqndkHuyftLF/0lWhQpkSPd6mDpu2a9cOp0+fhlarRW5uLs6dO4caNWpYuyyyE6+8IvDbb9no1k2LL75wRNOmrpg7V4n7961dGRERkfFkQgiLDT1ER0fjxIkTSE9Ph6enJ8aOHQvt/84gDw8PBwCsXLkScXFxkMvl6NWrF4YOHfrU7er1AqmpWeYsnczIGn89/vOPHPPnK7FliyPc3QVGjVLjrbfUKFOyP4ZKHf7lL23sn7Sxf9JV0pE3i4Y3c2F4kzZrvgGdPy/HvHlO2L3bAWXLCrz1lhrDh2vg6Sn5XwuL4IeHtLF/0sb+SZddHTYlsrR69fTYsCEXv/6ajVattIiNdUKTJq6YPt0Jd+5wVgYiIrI9HHkjq7Olvx7//luOTz9V4qefHKBQAH5+Onh761G9uv5//xeoVk0PlcraldoGW+odPTv2T9rYP+niYVMwvEmdLb4BXb8uwxdfKPHHHwpcuyZHevr/j8IplQJdu2oxdKgGfn46yErxAJ0t9o6Mx/5JG/snXQxvYHiTOim8AaWnA9euyXH1qhynTyuwaZMjHjyQoXZtHQYP1qB3bw3KlrV2lZYnhd5R0dg/aWP/pIvhDQxvUifFN6DsbGDzZgesXavEn38q4OIiMHy4GjExaihK0RSqUuwd/T/2T9rYP+niBQtEVuDqCvTvr8Xu3TnYvTsbnTpp8emnThg40BmZmdaujoiI7BnDG1EJNWyox/LleViwIA8HDijQpYsLbtwoxSfCERGRWTG8EZnI4MEafPddLpKS5Ojc2QUnTvDXi4iITI+fLkQmFBCgw86d2ShbFujZ0wU//GBT0wcTEZEdYHgjMrGaNQV27sxGs2Y6jBrlzABHREQmxfBGZAblygHffZcLPz8tYmJUuH2b58AREZFpMLwRmYlSCXz2WR60WuCdd1SQ/k15iIjIFjC8EZlR9eoCM2bk48ABB6xZ42jtcoiIyA4wvBGZ2ZAhGrRpo8WsWU64epWHT4mIqGQY3ojMTCYDFi3Kg4MDMHasM3Q6a1dERERSxvBGZAFVqgh89FEeTp5UYOlSpbXLISIiCWN4I7KQXr20CA7WYN48Jf76i796RET0fPgJQmQhMhmwYEE+ypYVGDlShYwMa1dERERSxPBGZEHlywt8/nkerlyRo3dvFwY4IiJ6ZgxvRBbWtq0Oq1fnIj6eAY6IiJ4dwxuRFXTowABHRETPh+GNyEoY4IiI6HkwvBFZEQMcERE9K4Y3Iit7PMCFhrogKYmzMBARUdEY3ohsQIcOOnzzTS4SEuQIDnbB33/zV5OIiArHTwgiGxEQoMPmzTnQaICQEBccP66wdklERGSDGN6IbEi9enps354DT0+B3r2dsWOHg7VLIiIiG8PwRmRjqlUT2LYtBz4+egwbpsLatY7WLomIiGwIwxuRDfL0FPjxxxwEBekwcaIK+/fzECoRET3E8EZko1xcgJUrc/HqqzpER6uQmWntioiIyBYwvBHZMJUK+PTTPCQlyTBjhpO1yyEiIhvA8EZk43x99Rg5UoOvvlLit994+JSIqLRjeCOSgEmT8nn4lIiIAFg4vE2ePBlvvPEGunbtWux6586dQ506dfDLL79YqDIi2/bo8GliogwzZ/LwKRFRaWbR8NazZ0+sXLmy2HV0Oh0WLFiAli1bWqgqImnw9dUjKkqD9euVOHCAh0+JiEori4a3pk2bwt3dvdh1vvrqK3Ts2BGenp4WqopIOiZNykfNmjqMH69CVpa1qyEiImuwqXPekpOTsWfPHoSHh1u7FCKb5Oz8/4dPp0/n4VMiotLIpubemT17Nt577z3I5c+WKWUywMPDxUxVkbkpFHL27xm0bw+8+67Axx8rERysQK9e1quFvZM29k/a2L/Sy6bC24ULFxAdHQ0ASE9Px4EDB+Dg4ICgoKBiHycEkJGRY4kSyQw8PFzYv2f0zjvAvn0uiIqSo1atbLz8srBKHeydtLF/0sb+SXNccDAAABLDSURBVFeFCmVK9HibCm/79u0z/DsmJgZt2rR5anAjKo0cHYHly3MRGOiKESOcsWVLDhw5BSoRUalg0XPeoqOj0a9fP1y7dg0BAQHYtGkTvv32W3z77beWLIPILlSrJvDJJ3k4fVqB+fOV1i6HiIgsRCaEsM7xFhPS6wVSU3npnVRx6L9koqOd8PXXjti0KRcBATqL7pu9kzb2T9rYP+myq8OmRPTsPvwwHydOKDBqlAr79+egQoWCf49lZj68qMfNzfhtarXArVsyXL8ux/Xrcty+LUP79lo0a6Y3cfVERPSsOPJGVse/Hkvur7/k6NjRBc2a6dC+vRaXL8tx5Yoc//2vHCkpciiVAkFBWoSFaREUpIWzc8HH378P7N/vgF9/dcDJkwrcuiWDVisrsI6jo0BsbB769tUavsfeSRv7J23sn3SVdOSN4Y2sjm9AprFmjSMmTVIBADw8BGrW1Bv+S06W4eefHXD3rhxubgJdumgRHKzF1asy7NnjgGPHFNDpZChXTsDfX4saNfR45RU9XnlF4JVX9HB2FnjrLWf8/rsD3nsvHxMmqA236GHvpIv9kzb2T7oY3sDwJnV8AzKd+Hg5KlQQ8PQUkBUcOINWCxw+rEBcnCO2bXNAZubDFerUeTha1769Fk2a6KEoYuYttRp47z0VvvvOEb17axAbm4eKFdk7KePvnrSxf9LF8AaGN6njG5Dl5eUBJ04o4O2tR9Wqxr8FCAEsWqTERx85oUULLX76SQaZjL2TKv7uSRv7J10lDW82NT0WEVmGSgUEBOieKbgBDy98GD/+/9q796Ao6/0P4O9nWS5LhIJxGY9MozLhDSSPJpTapLJiyyKgOViHkuaMSmcgg2HCtJlEoWzE0/FkpZHjbQzmoFkKkYgVjLgoXsJKKjMVCNbkIsplYR++vz8Y95cXbirsPvZ+zTjj8+xz+Yzf+Tzz9rm248MPW1FWZoennlJh+3Z7NDYOUKFERHQbhjci6rf5883IyWmFvT2QnOyECRNcEBvrhNxcNUwma1dHRPRgY3gjorsSFCTj9OlOHDrUjNjYDpSW2iE2VoOAABe8954DlH9DBhGRbWJ4I6K7JklAQEAn1qwxoby8GZ9+2oInnpCRnu6I115zhNnc+zaIiKh/GN6I6L5Qq4FZs2Ts2NGKpCQTdu92wEsvadDC+6mJiO4rhjciuq8kCXj99Xa8+24bCgvtMH++M+rqpN5XJCKiPmF4I6IBsXhxBz75pA3ff6+CXq/BpUsMcERE9wPDGxENGJ3OjP/9rxV//KGCTueMM2d4yCEiulc8khLRgAoKkrF/fwvUakCvd0ZentraJRERKRrDGxENuDFjOpGf34IxYzoRG+uE//6XrxIhIrpbDG9ENCi8vAQ++6wF4eFmrFnjiOXLndDebu2qiIiUh9cviGjQaDTA5s1t8PXtREaGIy5elLB1ayvc3a1dGRGRcvDMGxENKpWq61UiH3zQ9X3U8HBnNDVZuyoiIuVgeCMiq1iwwIysrFacP69CXJwGsmztioiIlIHhjYisZto0GWlpJhQUqPHOOw7WLoeISBF4zxsRWdXixR34/nsV/vMfR4wf34mICH4QlYioJzzzRkRWJUnA22+b8MQTZrz6qhNf5EtE1AseJYnI6hwcgK1b2+DmJvDSSxpcucJPaRERdYfhjYhsgqenwLZtrbhyRcI//+mEjg5rV0REZJsY3ojIZgQGdmLDhjaUlKgRH+8EM29/IyK6DR9YICKbsmCBGTU1JqxZ4wiTqeulvg58EJWIyILhjYhsTnx8OxwdBVatckJsrIRPPmmFk9P93UdtrQSDwQ4Ggx1++kmFF1/sQGQkT/URke1jeCMim7RkSQccHYHkZCfExGiwfXsrnJ3///eaGgm7dtkjO9serq4CL7zQgQULOjBkyJ23V18PFBaqUVyshsFghwsXuu4acXYWeOQRgaVLNTh2rB2rV5t4po+IbJokhBDWLuJedXYK1NVdt3YZdJeGDnVGY2OLtcuguzAYY5eVpcarrzohOFjGzp1dn9Tavt0eX32lhixLePppM+rrJZw5YweNRkCvN+Mf/+jA1Kkyzp+XkJ+vxsGDapSW2qGzU4K7eyemTpURFNT1Z8KETgBAaqojNm92wN//LiMzsxV/+5viD429Yu8pG8dPuTw8Hr6n9RneyOp4AFKuwRq7vXvV+Ne/nODgALS2Shg2rBOLFnUgJqYDI0d2HcK++06FnTvtsXevPa5flzB0qEBjY9crR8aNkxEaaoZWa0ZgYCdU3TyqtX9/V1B0cBD48MM2PPPMg/3NLvaesnH8lIvhDQxvSscDkHIN5th9+aUau3bZIyqqA2FhZjg63nm55mbgiy/UKCpSY/JkGVqtGT4+fT/M/fqrhJdf1qCiQoXUVBOWLn1w31nC3lM2jp9yMbyB4U3peABSrgd17JqbgVWrHFFXJ2HHjjZrlzNgHtTx+6vg+CnXvYY3PrBARHSLhx4C/v1vk7XLICK6o0F9Se+KFSsQHByMsLCwO/7+xRdfQK/XQ6/XIzo6GhUVFYNZHhEREZHNG9TwFhUVhczMzG5/HzFiBHbt2oX9+/cjLi4Ob7755iBWR0RERGT7BvWy6ZQpU1BVVdXt75MmTbL8PTAwELW1tYNRFhEREZFi2Ow9bzk5OZgxY0aflpWkrhs3SZns7FQcP4Xi2Ckbx0/ZOH5/XTYZ3gwGA3JycrB79+4+LS8E+MSNgvGJKeXi2Ckbx0/ZOH7K9cA9bVpRUYFVq1bh448/hpubm7XLISIiIrIpg/rAQm9+//13xMfH491338XIkSOtXQ4RERGRzRnUM2+JiYk4duwYGhoaMGPGDMTHx8NsNgMAFi1ahE2bNqGxsRGrV68GANjZ2WHv3r2DWSIRERGRTeMXFsjqeN+GcnHslI3jp2wcP+W613vebOqyKRERERH1jOGNiIiISEEY3oiIiIgUhOGNiIiISEEY3oiIiIgU5IF42pSIiIjor4Jn3oiIiIgUhOGNiIiISEEY3oiIiIgUhOGNiIiISEEY3oiIiIgUhOGNiIiISEEUH96KioowZ84chISEYMuWLdYuh3pQU1ODmJgYPPvss9DpdNi+fTsAoLGxEbGxsdBqtYiNjcXVq1etXCn1RJZlREREYOnSpQCAyspKPPfccwgJCcHy5cvR3t5u5QrpTpqampCQkIDQ0FDMnTsXp06dYu8pyLZt26DT6RAWFobExESYTCb2ng1bsWIFgoODERYWZpnXXb8JIbB27VqEhIRAr9fjhx9+6HX7ig5vsiwjNTUVmZmZyM3NxYEDB3Du3Dlrl0XdsLOzQ0pKCvLy8pCdnY3du3fj3Llz2LJlC4KDg3Hw4EEEBwczhNu4HTt2YPTo0Zbp9evXY/HixSgoKICrqytycnKsWB11Jy0tDdOnT0d+fj4+//xzjB49mr2nEEajETt27MCePXtw4MAByLKM3Nxc9p4Ni4qKQmZm5k3zuuu3oqIiXLhwAQcPHsSaNWvw1ltv9bp9RYe38vJyPProo/Dx8YGDgwN0Oh0KCwutXRZ1w9PTE+PHjwcAuLi4YNSoUTAajSgsLERERAQAICIiAocOHbJmmdSD2tpafPPNN1iwYAGArv8xGgwGzJkzBwAQGRnJHrRB165dw/Hjxy3j5uDgAFdXV/aegsiyjLa2NpjNZrS1tcHDw4O9Z8OmTJmCIUOG3DSvu367MV+SJAQGBqKpqQmXL1/ucfuKDm9GoxHe3t6WaS8vLxiNRitWRH1VVVWFs2fPYuLEiairq4OnpycAwMPDA3V1dVaujrqTnp6O5ORkqFRdh46Ghga4urpCrVYDALy9vdmDNqiqqgru7u5YsWIFIiIisHLlSrS0tLD3FMLLywsvv/wynnnmGUybNg0uLi4YP348e09huuu3W7NMX8ZS0eGNlKm5uRkJCQl444034OLictNvkiRBkiQrVUY9+frrr+Hu7o4JEyZYuxTqJ7PZjB9//BGLFi3Cvn37oNFobrtEyt6zXVevXkVhYSEKCwtRXFyM1tZWFBcXW7ssugf32m/q+1jLoPPy8kJtba1l2mg0wsvLy4oVUW86OjqQkJAAvV4PrVYLABg2bBguX74MT09PXL58Ge7u7lauku7k5MmTOHz4MIqKimAymXD9+nWkpaWhqakJZrMZarUatbW17EEb5O3tDW9vb0ycOBEAEBoaii1btrD3FKKkpAQjRoywjI9Wq8XJkyfZewrTXb/dmmX6MpaKPvPm7++PCxcuoLKyEu3t7cjNzcXMmTOtXRZ1QwiBlStXYtSoUYiNjbXMnzlzJvbt2wcA2LdvH2bNmmWtEqkHSUlJKCoqwuHDh7FhwwYEBQUhIyMDU6dOxVdffQUA+Oyzz9iDNsjDwwPe3t44f/48AODo0aMYPXo0e08hhg8fju+++w6tra0QQuDo0aPw9fVl7ylMd/12Y74QAqdPn8bDDz9subzaHUkIIQa84gH07bffIj09HbIsY/78+YiLi7N2SdSNsrIyvPDCC3jssccs90wlJiYiICAAy5cvR01NDYYPH4733nsPQ4cOtXK11JPS0lJs3boVmzdvRmVlJV577TVcvXoVY8eOxfr16+Hg4GDtEukWZ8+excqVK9HR0QEfHx+8/fbb6OzsZO8pxMaNG5GXlwe1Wo2xY8ciLS0NRqORvWejEhMTcezYMTQ0NGDYsGGIj4/H7Nmz79hvQgikpqaiuLgYGo0G6enp8Pf373H7ig9vRERERH8lir5sSkRERPRXw/BGREREpCAMb0REREQKwvBGREREpCAMb0REREQKwvBGRDahsLCw1w+jG41GJCQkAAD27t2L1NTUfu3jo48+6nWZlJQU5Ofn97pcdHQ0gK5PT+3fv79fdfTm1jpv7IuICGB4IyIbMWvWLCxZsqTHZby8vLBx48a73sfmzZvvet1bZWVlAQCqq6tx4MCBfq1rNpt7/P3WOm/si4gIYHgjogFWVVWF0NBQpKSkYM6cOUhKSkJJSQmio6Oh1WpRXl4O4OYzaSkpKVi7di2io6Mxa9Ysy5mwqqoqhIWFWbZdU1ODmJgYaLVavP/++5b5r7zyCqKioqDT6ZCdnQ0AWL9+Pdra2jBv3jwkJSUB6HrLuV6vR3h4OJKTky3rl5WV3bbvWz3++OMAgIyMDJSVlWHevHnYtm0bZFnGunXrMH/+fOj1ekvwKi0txfPPP49ly5ZBp9P1q84b+xJCYN26dQgLC4Ner0deXp5l2zExMUhISEBoaCiSkpLAV3gSPcAEEdEAqqysFGPHjhUVFRVClmURGRkpUlJSRGdnpygoKBBxcXFCCCH27NkjVq9eLYQQ4vXXXxfx8fFClmXxyy+/iNmzZ1u2pdPpLMs/9dRTor6+XrS2tgqdTifKy8uFEEI0NDQIIYRlfn19vRBCiMDAQEtdP//8s9BqtaKuru6mdbrb961ubMtgMIglS5ZY5mdlZYlNmzYJIYQwmUwiMjJSXLp0SRgMBjFx4kRx6dIly7J9qfPP0/n5+WLx4sXCbDaLP/74Qzz99NPCaDQKg8EgJk2aJGpqaoQsy2LhwoXi+PHjfRofIlIeRX+YnoiUYcSIEfDz8wMA+Pr6Ijg4GJIkwc/PD9XV1XdcZ/bs2VCpVPD19cWVK1fuuMyTTz4JNzc3AEBISAhOnDgBf39/7Ny5EwUFBQC6zs5dvHjRstwNBoMBoaGhlo9D//mzUH3Zd3eOHDmCn376yfLNyWvXruHixYuwt7eHv78/fHx8LMv2pc4/O3HiBHQ6Hezs7PDII49gypQpOHPmDFxcXBAQEABvb28AwJgxY1BdXY3Jkyf3q3YiUgaGNyIacH/+3qJKpbJMS5IEWZZ7Xac7kiTdNl1aWoqSkhJkZ2dDo9EgJiYGJpPpruvtLyEEVq1ahenTp980v7S0FM7OzjdN32ud3dVsZ2fX7b8rESkf73kjIsU6cuQIGhsb0dbWhkOHDmHSpEm4du0ahgwZAo1Gg19//RWnT5+2LK9Wq9HR0QEACAoKQn5+PhoaGgAAjY2Nd1XDQw89hObmZsv0tGnT8Omnn1r289tvv6GlpeW29fpa559NnjwZX375JWRZRn19PcrKyhAQEHBXdRORcvHMGxEpVkBAAOLj42E0GhEeHg5/f3/4+fkhKysLc+fOxciRIxEYGGhZfuHChQgPD8e4ceOQkZGBZcuWISYmBiqVCuPGjcM777zT7xr8/PygUqkQHh6OqKgovPjii6iurkZUVBSEEHBzc8MHH3xw23ozZszoc503hISE4NSpU5g3bx4kSUJycjI8PDxw/vz5ftdNRMolCcFHkoiIiIiUgpdNiYiIiBSE4Y2IiIhIQRjeiIiIiBSE4Y2IiIhIQRjeiIiIiBSE4Y2IiIhIQRjeiIiIiBSE4Y2IiIhIQf4PkCgewLl01HwAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 720x504 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the loss over the iterations\n",
    "fig = plt.figure(figsize=(10, 7))\n",
    "plt.plot(ls_of_loss, 'b-')\n",
    "plt.xlabel('minibatch iteration')\n",
    "plt.ylabel('Cross entropy', fontsize=15)\n",
    "plt.title('Decrease of loss over backprop iteration')\n",
    "plt.xlim(0, 100)\n",
    "fig.subplots_adjust(bottom=0.2)\n",
    "plt.show()\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter id: 0 Numerical gradient of -0.003120 is not close to the backpropagation gradient of -0.003120!\n",
      "Gradient is correct\n",
      "Parameter id: 2 Numerical gradient of -0.008396 is not close to the backpropagation gradient of -0.008396!\n",
      "Parameter id: 3 Numerical gradient of -0.007772 is not close to the backpropagation gradient of -0.007772!\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Parameter id: 6 Numerical gradient of 0.002510 is not close to the backpropagation gradient of 0.002510!\n",
      "Parameter id: 7 Numerical gradient of 0.002813 is not close to the backpropagation gradient of 0.002813!\n",
      "Parameter id: 8 Numerical gradient of -0.002950 is not close to the backpropagation gradient of -0.002950!\n",
      "Parameter id: 9 Numerical gradient of -0.008088 is not close to the backpropagation gradient of -0.008088!\n",
      "Gradient is correct\n",
      "Parameter id: 11 Numerical gradient of 0.002315 is not close to the backpropagation gradient of 0.002315!\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Parameter id: 16 Numerical gradient of -0.003718 is not close to the backpropagation gradient of -0.003718!\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Parameter id: 20 Numerical gradient of -0.001298 is not close to the backpropagation gradient of -0.001298!\n",
      "Parameter id: 21 Numerical gradient of 0.004547 is not close to the backpropagation gradient of 0.004547!\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Parameter id: 24 Numerical gradient of -0.003391 is not close to the backpropagation gradient of -0.003391!\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Parameter id: 32 Numerical gradient of 0.001114 is not close to the backpropagation gradient of 0.001114!\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Parameter id: 37 Numerical gradient of -0.004368 is not close to the backpropagation gradient of -0.004368!\n",
      "Parameter id: 38 Numerical gradient of -0.002141 is not close to the backpropagation gradient of -0.002141!\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Parameter id: 42 Numerical gradient of -0.001801 is not close to the backpropagation gradient of -0.001801!\n",
      "Gradient is correct\n",
      "Parameter id: 44 Numerical gradient of -0.001691 is not close to the backpropagation gradient of -0.001691!\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Parameter id: 55 Numerical gradient of 0.006403 is not close to the backpropagation gradient of 0.006403!\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Parameter id: 59 Numerical gradient of -0.005267 is not close to the backpropagation gradient of -0.005267!\n",
      "Gradient is correct\n",
      "Parameter id: 61 Numerical gradient of -0.006773 is not close to the backpropagation gradient of -0.006773!\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Parameter id: 64 Numerical gradient of -0.007385 is not close to the backpropagation gradient of -0.007385!\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Parameter id: 67 Numerical gradient of 0.002590 is not close to the backpropagation gradient of 0.002590!\n",
      "Gradient is correct\n",
      "Parameter id: 69 Numerical gradient of 0.004733 is not close to the backpropagation gradient of 0.004733!\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Parameter id: 73 Numerical gradient of 0.001329 is not close to the backpropagation gradient of 0.001329!\n",
      "Parameter id: 74 Numerical gradient of 0.000625 is not close to the backpropagation gradient of 0.000625!\n",
      "Gradient is correct\n",
      "Parameter id: 76 Numerical gradient of -0.009660 is not close to the backpropagation gradient of -0.009660!\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Parameter id: 85 Numerical gradient of 0.001780 is not close to the backpropagation gradient of 0.001780!\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Parameter id: 90 Numerical gradient of 0.006866 is not close to the backpropagation gradient of 0.006866!\n",
      "Parameter id: 91 Numerical gradient of -0.005850 is not close to the backpropagation gradient of -0.005850!\n",
      "Gradient is correct\n",
      "Parameter id: 93 Numerical gradient of -0.007446 is not close to the backpropagation gradient of -0.007446!\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Parameter id: 96 Numerical gradient of -0.006886 is not close to the backpropagation gradient of -0.006886!\n",
      "Gradient is correct\n",
      "Parameter id: 98 Numerical gradient of 0.000038 is not close to the backpropagation gradient of 0.000038!\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Parameter id: 102 Numerical gradient of -0.000746 is not close to the backpropagation gradient of -0.000746!\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Parameter id: 105 Numerical gradient of 0.008033 is not close to the backpropagation gradient of 0.008033!\n",
      "Parameter id: 106 Numerical gradient of 0.004623 is not close to the backpropagation gradient of 0.004623!\n",
      "Gradient is correct\n",
      "Parameter id: 108 Numerical gradient of -0.000671 is not close to the backpropagation gradient of -0.000671!\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Parameter id: 111 Numerical gradient of 0.005568 is not close to the backpropagation gradient of 0.005568!\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Parameter id: 115 Numerical gradient of -0.002789 is not close to the backpropagation gradient of -0.002789!\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Parameter id: 119 Numerical gradient of 0.002418 is not close to the backpropagation gradient of 0.002418!\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Parameter id: 127 Numerical gradient of 0.003803 is not close to the backpropagation gradient of 0.003803!\n",
      "Parameter id: 128 Numerical gradient of 0.001999 is not close to the backpropagation gradient of 0.001999!\n",
      "Parameter id: 129 Numerical gradient of -0.006197 is not close to the backpropagation gradient of -0.006197!\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Parameter id: 132 Numerical gradient of 0.001525 is not close to the backpropagation gradient of 0.001525!\n",
      "Gradient is correct\n",
      "Parameter id: 134 Numerical gradient of 0.006110 is not close to the backpropagation gradient of 0.006110!\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Parameter id: 138 Numerical gradient of 0.001664 is not close to the backpropagation gradient of 0.001664!\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Parameter id: 144 Numerical gradient of 0.008127 is not close to the backpropagation gradient of 0.008127!\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Parameter id: 149 Numerical gradient of -0.002069 is not close to the backpropagation gradient of -0.002069!\n",
      "Gradient is correct\n",
      "Parameter id: 151 Numerical gradient of -0.004037 is not close to the backpropagation gradient of -0.004037!\n",
      "Parameter id: 152 Numerical gradient of -0.011939 is not close to the backpropagation gradient of -0.011939!\n",
      "Parameter id: 153 Numerical gradient of -0.001087 is not close to the backpropagation gradient of -0.001087!\n",
      "Parameter id: 154 Numerical gradient of -0.004668 is not close to the backpropagation gradient of -0.004668!\n",
      "Parameter id: 155 Numerical gradient of 0.002671 is not close to the backpropagation gradient of 0.002671!\n",
      "Parameter id: 156 Numerical gradient of -0.002059 is not close to the backpropagation gradient of -0.002059!\n",
      "Gradient is correct\n",
      "Parameter id: 158 Numerical gradient of -0.001473 is not close to the backpropagation gradient of -0.001473!\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Parameter id: 161 Numerical gradient of 0.003196 is not close to the backpropagation gradient of 0.003196!\n",
      "Parameter id: 162 Numerical gradient of -0.000020 is not close to the backpropagation gradient of -0.000020!\n",
      "Parameter id: 163 Numerical gradient of 0.003842 is not close to the backpropagation gradient of 0.003842!\n",
      "Parameter id: 164 Numerical gradient of -0.000487 is not close to the backpropagation gradient of -0.000487!\n",
      "Parameter id: 165 Numerical gradient of 0.007430 is not close to the backpropagation gradient of 0.007430!\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Parameter id: 168 Numerical gradient of -0.002118 is not close to the backpropagation gradient of -0.002118!\n",
      "Parameter id: 169 Numerical gradient of 0.008448 is not close to the backpropagation gradient of 0.008448!\n",
      "Parameter id: 170 Numerical gradient of -0.007070 is not close to the backpropagation gradient of -0.007070!\n",
      "Parameter id: 171 Numerical gradient of 0.004273 is not close to the backpropagation gradient of 0.004273!\n",
      "Gradient is correct\n",
      "Parameter id: 173 Numerical gradient of 0.001083 is not close to the backpropagation gradient of 0.001083!\n",
      "Parameter id: 174 Numerical gradient of -0.003254 is not close to the backpropagation gradient of -0.003254!\n",
      "Parameter id: 175 Numerical gradient of -0.002913 is not close to the backpropagation gradient of -0.002913!\n",
      "Parameter id: 176 Numerical gradient of -0.009538 is not close to the backpropagation gradient of -0.009538!\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Parameter id: 179 Numerical gradient of 0.000931 is not close to the backpropagation gradient of 0.000931!\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Parameter id: 182 Numerical gradient of 0.005332 is not close to the backpropagation gradient of 0.005332!\n",
      "Gradient is correct\n",
      "Parameter id: 184 Numerical gradient of 0.013032 is not close to the backpropagation gradient of 0.013032!\n",
      "Gradient is correct\n",
      "Parameter id: 186 Numerical gradient of 0.006185 is not close to the backpropagation gradient of 0.006185!\n",
      "Parameter id: 187 Numerical gradient of 0.000913 is not close to the backpropagation gradient of 0.000913!\n",
      "Gradient is correct\n",
      "Parameter id: 189 Numerical gradient of -0.004403 is not close to the backpropagation gradient of -0.004403!\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Parameter id: 192 Numerical gradient of 0.002266 is not close to the backpropagation gradient of 0.002266!\n",
      "Parameter id: 193 Numerical gradient of -0.008069 is not close to the backpropagation gradient of -0.008069!\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Parameter id: 196 Numerical gradient of -0.001277 is not close to the backpropagation gradient of -0.001277!\n",
      "Gradient is correct\n",
      "Parameter id: 198 Numerical gradient of 0.000239 is not close to the backpropagation gradient of 0.000239!\n",
      "Parameter id: 199 Numerical gradient of -0.008610 is not close to the backpropagation gradient of -0.008610!\n",
      "Gradient is correct\n",
      "Parameter id: 201 Numerical gradient of -0.003890 is not close to the backpropagation gradient of -0.003890!\n",
      "Parameter id: 202 Numerical gradient of -0.011074 is not close to the backpropagation gradient of -0.011074!\n",
      "Gradient is correct\n",
      "Parameter id: 204 Numerical gradient of 0.004183 is not close to the backpropagation gradient of 0.004183!\n",
      "Gradient is correct\n",
      "Parameter id: 206 Numerical gradient of 0.010028 is not close to the backpropagation gradient of 0.010028!\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Parameter id: 209 Numerical gradient of -0.003449 is not close to the backpropagation gradient of -0.003449!\n",
      "Gradient is correct\n",
      "Parameter id: 211 Numerical gradient of -0.003667 is not close to the backpropagation gradient of -0.003667!\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Parameter id: 216 Numerical gradient of -0.005822 is not close to the backpropagation gradient of -0.005822!\n",
      "Parameter id: 217 Numerical gradient of -0.001981 is not close to the backpropagation gradient of -0.001981!\n",
      "Parameter id: 218 Numerical gradient of 0.001212 is not close to the backpropagation gradient of 0.001212!\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Parameter id: 223 Numerical gradient of 0.008611 is not close to the backpropagation gradient of 0.008611!\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Parameter id: 226 Numerical gradient of 0.001372 is not close to the backpropagation gradient of 0.001372!\n",
      "Gradient is correct\n",
      "Parameter id: 228 Numerical gradient of -0.001143 is not close to the backpropagation gradient of -0.001143!\n",
      "Parameter id: 229 Numerical gradient of 0.011670 is not close to the backpropagation gradient of 0.011670!\n",
      "Gradient is correct\n",
      "Parameter id: 231 Numerical gradient of 0.002403 is not close to the backpropagation gradient of 0.002403!\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Parameter id: 234 Numerical gradient of -0.004828 is not close to the backpropagation gradient of -0.004828!\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Parameter id: 240 Numerical gradient of 0.009085 is not close to the backpropagation gradient of 0.009085!\n",
      "Parameter id: 241 Numerical gradient of -0.005070 is not close to the backpropagation gradient of -0.005070!\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Parameter id: 247 Numerical gradient of -0.001786 is not close to the backpropagation gradient of -0.001786!\n",
      "Parameter id: 248 Numerical gradient of 0.000926 is not close to the backpropagation gradient of 0.000926!\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Parameter id: 252 Numerical gradient of -0.002464 is not close to the backpropagation gradient of -0.002464!\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Parameter id: 258 Numerical gradient of -0.000046 is not close to the backpropagation gradient of -0.000046!\n",
      "Gradient is correct\n",
      "Parameter id: 260 Numerical gradient of -0.008044 is not close to the backpropagation gradient of -0.008044!\n",
      "Parameter id: 261 Numerical gradient of 0.003828 is not close to the backpropagation gradient of 0.003828!\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Parameter id: 269 Numerical gradient of 0.001491 is not close to the backpropagation gradient of 0.001491!\n",
      "Parameter id: 270 Numerical gradient of -0.013636 is not close to the backpropagation gradient of -0.013636!\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Parameter id: 278 Numerical gradient of -0.000033 is not close to the backpropagation gradient of -0.000033!\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Parameter id: 282 Numerical gradient of 0.003431 is not close to the backpropagation gradient of 0.003431!\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Parameter id: 288 Numerical gradient of 0.000884 is not close to the backpropagation gradient of 0.000884!\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Parameter id: 291 Numerical gradient of -0.008426 is not close to the backpropagation gradient of -0.008426!\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Parameter id: 295 Numerical gradient of 0.001505 is not close to the backpropagation gradient of 0.001505!\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Parameter id: 299 Numerical gradient of -0.001512 is not close to the backpropagation gradient of -0.001512!\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Parameter id: 302 Numerical gradient of 0.015554 is not close to the backpropagation gradient of 0.015554!\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Parameter id: 307 Numerical gradient of 0.003038 is not close to the backpropagation gradient of 0.003038!\n",
      "Parameter id: 308 Numerical gradient of -0.000290 is not close to the backpropagation gradient of -0.000290!\n",
      "Parameter id: 309 Numerical gradient of -0.007714 is not close to the backpropagation gradient of -0.007714!\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Parameter id: 312 Numerical gradient of 0.003736 is not close to the backpropagation gradient of 0.003736!\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Parameter id: 318 Numerical gradient of 0.000393 is not close to the backpropagation gradient of 0.000393!\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Parameter id: 321 Numerical gradient of -0.007432 is not close to the backpropagation gradient of -0.007432!\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Parameter id: 324 Numerical gradient of 0.010524 is not close to the backpropagation gradient of 0.010524!\n",
      "Parameter id: 325 Numerical gradient of 0.001558 is not close to the backpropagation gradient of 0.001558!\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Parameter id: 337 Numerical gradient of -0.003508 is not close to the backpropagation gradient of -0.003508!\n",
      "Parameter id: 338 Numerical gradient of 0.000236 is not close to the backpropagation gradient of 0.000236!\n",
      "Parameter id: 339 Numerical gradient of 0.007882 is not close to the backpropagation gradient of 0.007882!\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Parameter id: 342 Numerical gradient of -0.003716 is not close to the backpropagation gradient of -0.003716!\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Parameter id: 348 Numerical gradient of -0.000516 is not close to the backpropagation gradient of -0.000516!\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Parameter id: 355 Numerical gradient of -0.000810 is not close to the backpropagation gradient of -0.000810!\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Parameter id: 359 Numerical gradient of 0.000274 is not close to the backpropagation gradient of 0.000274!\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Parameter id: 363 Numerical gradient of -0.012658 is not close to the backpropagation gradient of -0.012658!\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Parameter id: 367 Numerical gradient of -0.003457 is not close to the backpropagation gradient of -0.003457!\n",
      "Parameter id: 368 Numerical gradient of 0.000273 is not close to the backpropagation gradient of 0.000273!\n",
      "Parameter id: 369 Numerical gradient of 0.007552 is not close to the backpropagation gradient of 0.007552!\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Parameter id: 372 Numerical gradient of -0.003635 is not close to the backpropagation gradient of -0.003635!\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Parameter id: 375 Numerical gradient of 0.010988 is not close to the backpropagation gradient of 0.010988!\n",
      "Parameter id: 376 Numerical gradient of 0.008056 is not close to the backpropagation gradient of 0.008056!\n",
      "Gradient is correct\n",
      "Parameter id: 378 Numerical gradient of -0.001098 is not close to the backpropagation gradient of -0.001098!\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Parameter id: 381 Numerical gradient of 0.006166 is not close to the backpropagation gradient of 0.006166!\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Parameter id: 396 Numerical gradient of 0.004898 is not close to the backpropagation gradient of 0.004898!\n",
      "Parameter id: 397 Numerical gradient of 0.001654 is not close to the backpropagation gradient of 0.001654!\n",
      "Parameter id: 398 Numerical gradient of -0.001711 is not close to the backpropagation gradient of -0.001711!\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Parameter id: 402 Numerical gradient of 0.001994 is not close to the backpropagation gradient of 0.001994!\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Parameter id: 405 Numerical gradient of -0.006841 is not close to the backpropagation gradient of -0.006841!\n",
      "Parameter id: 406 Numerical gradient of -0.000197 is not close to the backpropagation gradient of -0.000197!\n",
      "Gradient is correct\n",
      "Parameter id: 408 Numerical gradient of 0.001058 is not close to the backpropagation gradient of 0.001058!\n",
      "Gradient is correct\n",
      "Parameter id: 410 Numerical gradient of 0.016189 is not close to the backpropagation gradient of 0.016189!\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Parameter id: 414 Numerical gradient of 0.003502 is not close to the backpropagation gradient of 0.003502!\n",
      "Parameter id: 415 Numerical gradient of 0.004647 is not close to the backpropagation gradient of 0.004647!\n",
      "Parameter id: 416 Numerical gradient of 0.012302 is not close to the backpropagation gradient of 0.012302!\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Parameter id: 419 Numerical gradient of -0.005034 is not close to the backpropagation gradient of -0.005034!\n",
      "Gradient is correct\n",
      "Parameter id: 421 Numerical gradient of -0.001013 is not close to the backpropagation gradient of -0.001013!\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Parameter id: 427 Numerical gradient of -0.000701 is not close to the backpropagation gradient of -0.000701!\n",
      "Parameter id: 428 Numerical gradient of 0.001585 is not close to the backpropagation gradient of 0.001585!\n",
      "Parameter id: 429 Numerical gradient of 0.003621 is not close to the backpropagation gradient of 0.003621!\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Parameter id: 432 Numerical gradient of -0.001771 is not close to the backpropagation gradient of -0.001771!\n",
      "Parameter id: 433 Numerical gradient of 0.006851 is not close to the backpropagation gradient of 0.006851!\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Parameter id: 436 Numerical gradient of -0.001042 is not close to the backpropagation gradient of -0.001042!\n",
      "Gradient is correct\n",
      "Parameter id: 438 Numerical gradient of -0.000753 is not close to the backpropagation gradient of -0.000753!\n",
      "Gradient is correct\n",
      "Parameter id: 440 Numerical gradient of -0.015966 is not close to the backpropagation gradient of -0.015966!\n",
      "Parameter id: 441 Numerical gradient of 0.003066 is not close to the backpropagation gradient of 0.003066!\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Parameter id: 444 Numerical gradient of -0.002075 is not close to the backpropagation gradient of -0.002075!\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Parameter id: 448 Numerical gradient of -0.008820 is not close to the backpropagation gradient of -0.008820!\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Parameter id: 458 Numerical gradient of 0.000067 is not close to the backpropagation gradient of 0.000067!\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Parameter id: 465 Numerical gradient of 0.011709 is not close to the backpropagation gradient of 0.011709!\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Parameter id: 468 Numerical gradient of -0.001199 is not close to the backpropagation gradient of -0.001199!\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Parameter id: 471 Numerical gradient of 0.007182 is not close to the backpropagation gradient of 0.007182!\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Parameter id: 479 Numerical gradient of 0.001354 is not close to the backpropagation gradient of 0.001354!\n",
      "Gradient is correct\n",
      "Parameter id: 481 Numerical gradient of -0.009013 is not close to the backpropagation gradient of -0.009013!\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Parameter id: 487 Numerical gradient of -0.003033 is not close to the backpropagation gradient of -0.003033!\n",
      "Parameter id: 488 Numerical gradient of 0.000537 is not close to the backpropagation gradient of 0.000537!\n",
      "Parameter id: 489 Numerical gradient of 0.007602 is not close to the backpropagation gradient of 0.007602!\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Parameter id: 494 Numerical gradient of -0.013559 is not close to the backpropagation gradient of -0.013559!\n",
      "Parameter id: 495 Numerical gradient of 0.010500 is not close to the backpropagation gradient of 0.010500!\n",
      "Parameter id: 496 Numerical gradient of 0.007698 is not close to the backpropagation gradient of 0.007698!\n",
      "Gradient is correct\n",
      "Parameter id: 498 Numerical gradient of -0.000943 is not close to the backpropagation gradient of -0.000943!\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Parameter id: 501 Numerical gradient of 0.007187 is not close to the backpropagation gradient of 0.007187!\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Parameter id: 505 Numerical gradient of -0.001932 is not close to the backpropagation gradient of -0.001932!\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Parameter id: 509 Numerical gradient of 0.001823 is not close to the backpropagation gradient of 0.001823!\n",
      "Gradient is correct\n",
      "Parameter id: 511 Numerical gradient of 0.003221 is not close to the backpropagation gradient of 0.003221!\n",
      "Parameter id: 512 Numerical gradient of 0.001788 is not close to the backpropagation gradient of 0.001788!\n",
      "Parameter id: 513 Numerical gradient of 0.005303 is not close to the backpropagation gradient of 0.005303!\n",
      "Parameter id: 514 Numerical gradient of 0.005849 is not close to the backpropagation gradient of 0.005849!\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Parameter id: 517 Numerical gradient of 0.000318 is not close to the backpropagation gradient of 0.000318!\n",
      "Parameter id: 518 Numerical gradient of -0.001716 is not close to the backpropagation gradient of -0.001716!\n",
      "Parameter id: 519 Numerical gradient of -0.002888 is not close to the backpropagation gradient of -0.002888!\n",
      "Gradient is correct\n",
      "Parameter id: 521 Numerical gradient of -0.011342 is not close to the backpropagation gradient of -0.011342!\n",
      "Parameter id: 522 Numerical gradient of 0.001667 is not close to the backpropagation gradient of 0.001667!\n",
      "Parameter id: 523 Numerical gradient of -0.004416 is not close to the backpropagation gradient of -0.004416!\n",
      "Gradient is correct\n",
      "Parameter id: 525 Numerical gradient of -0.001028 is not close to the backpropagation gradient of -0.001028!\n",
      "Parameter id: 526 Numerical gradient of 0.000524 is not close to the backpropagation gradient of 0.000524!\n",
      "Gradient is correct\n",
      "Parameter id: 528 Numerical gradient of -0.000245 is not close to the backpropagation gradient of -0.000245!\n",
      "Parameter id: 529 Numerical gradient of -0.002789 is not close to the backpropagation gradient of -0.002789!\n",
      "Parameter id: 530 Numerical gradient of 0.003633 is not close to the backpropagation gradient of 0.003633!\n",
      "Parameter id: 531 Numerical gradient of -0.000361 is not close to the backpropagation gradient of -0.000361!\n",
      "Parameter id: 532 Numerical gradient of -0.005890 is not close to the backpropagation gradient of -0.005890!\n",
      "Gradient is correct\n",
      "Parameter id: 534 Numerical gradient of 0.002938 is not close to the backpropagation gradient of 0.002938!\n",
      "Parameter id: 535 Numerical gradient of 0.000283 is not close to the backpropagation gradient of 0.000283!\n",
      "Gradient is correct\n",
      "Parameter id: 537 Numerical gradient of -0.002261 is not close to the backpropagation gradient of -0.002261!\n",
      "Gradient is correct\n",
      "Parameter id: 539 Numerical gradient of -0.000835 is not close to the backpropagation gradient of -0.000835!\n",
      "Parameter id: 540 Numerical gradient of -0.013209 is not close to the backpropagation gradient of -0.013209!\n",
      "Parameter id: 541 Numerical gradient of 0.008863 is not close to the backpropagation gradient of 0.008863!\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Parameter id: 547 Numerical gradient of 0.003311 is not close to the backpropagation gradient of 0.003311!\n",
      "Parameter id: 548 Numerical gradient of 0.000169 is not close to the backpropagation gradient of 0.000169!\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Parameter id: 552 Numerical gradient of 0.002302 is not close to the backpropagation gradient of 0.002302!\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Parameter id: 558 Numerical gradient of 0.001431 is not close to the backpropagation gradient of 0.001431!\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Parameter id: 561 Numerical gradient of -0.006023 is not close to the backpropagation gradient of -0.006023!\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Parameter id: 564 Numerical gradient of 0.009251 is not close to the backpropagation gradient of 0.009251!\n",
      "Parameter id: 565 Numerical gradient of 0.003867 is not close to the backpropagation gradient of 0.003867!\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Parameter id: 577 Numerical gradient of -0.002363 is not close to the backpropagation gradient of -0.002363!\n",
      "Parameter id: 578 Numerical gradient of -0.000546 is not close to the backpropagation gradient of -0.000546!\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Parameter id: 583 Numerical gradient of 0.012558 is not close to the backpropagation gradient of 0.012558!\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Parameter id: 588 Numerical gradient of -0.000088 is not close to the backpropagation gradient of -0.000088!\n",
      "Gradient is correct\n",
      "Parameter id: 590 Numerical gradient of -0.011377 is not close to the backpropagation gradient of -0.011377!\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Parameter id: 595 Numerical gradient of -0.001367 is not close to the backpropagation gradient of -0.001367!\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Parameter id: 600 Numerical gradient of -0.004864 is not close to the backpropagation gradient of -0.004864!\n",
      "Parameter id: 601 Numerical gradient of 0.001372 is not close to the backpropagation gradient of 0.001372!\n",
      "Parameter id: 602 Numerical gradient of 0.001656 is not close to the backpropagation gradient of 0.001656!\n",
      "Gradient is correct\n",
      "Parameter id: 604 Numerical gradient of 0.011401 is not close to the backpropagation gradient of 0.011401!\n",
      "Gradient is correct\n",
      "Parameter id: 606 Numerical gradient of 0.004667 is not close to the backpropagation gradient of 0.004667!\n",
      "Gradient is correct\n",
      "Parameter id: 608 Numerical gradient of -0.001870 is not close to the backpropagation gradient of -0.001870!\n",
      "Parameter id: 609 Numerical gradient of -0.002463 is not close to the backpropagation gradient of -0.002463!\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Parameter id: 612 Numerical gradient of 0.001503 is not close to the backpropagation gradient of 0.001503!\n",
      "Parameter id: 613 Numerical gradient of -0.006100 is not close to the backpropagation gradient of -0.006100!\n",
      "Parameter id: 614 Numerical gradient of 0.008901 is not close to the backpropagation gradient of 0.008901!\n",
      "Parameter id: 615 Numerical gradient of -0.004241 is not close to the backpropagation gradient of -0.004241!\n",
      "Parameter id: 616 Numerical gradient of 0.001317 is not close to the backpropagation gradient of 0.001317!\n",
      "Gradient is correct\n",
      "Parameter id: 618 Numerical gradient of 0.000112 is not close to the backpropagation gradient of 0.000112!\n",
      "Parameter id: 619 Numerical gradient of -0.006065 is not close to the backpropagation gradient of -0.006065!\n",
      "Gradient is correct\n",
      "Parameter id: 621 Numerical gradient of -0.002751 is not close to the backpropagation gradient of -0.002751!\n",
      "Gradient is correct\n",
      "Parameter id: 623 Numerical gradient of -0.012993 is not close to the backpropagation gradient of -0.012993!\n",
      "Parameter id: 624 Numerical gradient of 0.001610 is not close to the backpropagation gradient of 0.001610!\n",
      "Parameter id: 625 Numerical gradient of 0.002771 is not close to the backpropagation gradient of 0.002771!\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Parameter id: 629 Numerical gradient of -0.004384 is not close to the backpropagation gradient of -0.004384!\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Parameter id: 633 Numerical gradient of -0.012710 is not close to the backpropagation gradient of -0.012710!\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Parameter id: 637 Numerical gradient of -0.003846 is not close to the backpropagation gradient of -0.003846!\n",
      "Parameter id: 638 Numerical gradient of -0.000405 is not close to the backpropagation gradient of -0.000405!\n",
      "Parameter id: 639 Numerical gradient of 0.007314 is not close to the backpropagation gradient of 0.007314!\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Parameter id: 642 Numerical gradient of -0.003366 is not close to the backpropagation gradient of -0.003366!\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Parameter id: 648 Numerical gradient of -0.001204 is not close to the backpropagation gradient of -0.001204!\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Parameter id: 651 Numerical gradient of 0.006373 is not close to the backpropagation gradient of 0.006373!\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Parameter id: 655 Numerical gradient of -0.002756 is not close to the backpropagation gradient of -0.002756!\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Parameter id: 659 Numerical gradient of 0.000586 is not close to the backpropagation gradient of 0.000586!\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Parameter id: 664 Numerical gradient of -0.011900 is not close to the backpropagation gradient of -0.011900!\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Parameter id: 668 Numerical gradient of -0.000071 is not close to the backpropagation gradient of -0.000071!\n",
      "Parameter id: 669 Numerical gradient of 0.006197 is not close to the backpropagation gradient of 0.006197!\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Parameter id: 672 Numerical gradient of -0.002302 is not close to the backpropagation gradient of -0.002302!\n",
      "Parameter id: 673 Numerical gradient of 0.009297 is not close to the backpropagation gradient of 0.009297!\n",
      "Gradient is correct\n",
      "Parameter id: 675 Numerical gradient of 0.008341 is not close to the backpropagation gradient of 0.008341!\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Parameter id: 678 Numerical gradient of -0.001239 is not close to the backpropagation gradient of -0.001239!\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Parameter id: 681 Numerical gradient of 0.005559 is not close to the backpropagation gradient of 0.005559!\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Parameter id: 685 Numerical gradient of -0.003475 is not close to the backpropagation gradient of -0.003475!\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Parameter id: 689 Numerical gradient of 0.002222 is not close to the backpropagation gradient of 0.002222!\n",
      "Gradient is correct\n",
      "Parameter id: 691 Numerical gradient of 0.003455 is not close to the backpropagation gradient of 0.003455!\n",
      "Parameter id: 692 Numerical gradient of 0.006444 is not close to the backpropagation gradient of 0.006444!\n",
      "Parameter id: 693 Numerical gradient of 0.006903 is not close to the backpropagation gradient of 0.006903!\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Parameter id: 696 Numerical gradient of 0.005722 is not close to the backpropagation gradient of 0.005722!\n",
      "Gradient is correct\n",
      "Parameter id: 698 Numerical gradient of 0.000400 is not close to the backpropagation gradient of 0.000400!\n",
      "Parameter id: 699 Numerical gradient of -0.002945 is not close to the backpropagation gradient of -0.002945!\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Parameter id: 702 Numerical gradient of 0.001333 is not close to the backpropagation gradient of 0.001333!\n",
      "Parameter id: 703 Numerical gradient of -0.005423 is not close to the backpropagation gradient of -0.005423!\n",
      "Gradient is correct\n",
      "Parameter id: 705 Numerical gradient of -0.004251 is not close to the backpropagation gradient of -0.004251!\n",
      "Parameter id: 706 Numerical gradient of -0.004694 is not close to the backpropagation gradient of -0.004694!\n",
      "Gradient is correct\n",
      "Parameter id: 708 Numerical gradient of 0.000065 is not close to the backpropagation gradient of 0.000065!\n",
      "Parameter id: 709 Numerical gradient of -0.005634 is not close to the backpropagation gradient of -0.005634!\n",
      "Gradient is correct\n",
      "Parameter id: 711 Numerical gradient of -0.006257 is not close to the backpropagation gradient of -0.006257!\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Parameter id: 715 Numerical gradient of 0.000936 is not close to the backpropagation gradient of 0.000936!\n",
      "Gradient is correct\n",
      "Parameter id: 717 Numerical gradient of -0.010289 is not close to the backpropagation gradient of -0.010289!\n",
      "Gradient is correct\n",
      "Parameter id: 719 Numerical gradient of -0.000893 is not close to the backpropagation gradient of -0.000893!\n",
      "Parameter id: 720 Numerical gradient of -0.005793 is not close to the backpropagation gradient of -0.005793!\n",
      "Parameter id: 721 Numerical gradient of 0.001569 is not close to the backpropagation gradient of 0.001569!\n",
      "Parameter id: 722 Numerical gradient of -0.001702 is not close to the backpropagation gradient of -0.001702!\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Parameter id: 727 Numerical gradient of -0.000269 is not close to the backpropagation gradient of -0.000269!\n",
      "Parameter id: 728 Numerical gradient of -0.002560 is not close to the backpropagation gradient of -0.002560!\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Parameter id: 732 Numerical gradient of 0.002358 is not close to the backpropagation gradient of 0.002358!\n",
      "Parameter id: 733 Numerical gradient of -0.006404 is not close to the backpropagation gradient of -0.006404!\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Parameter id: 736 Numerical gradient of 0.001725 is not close to the backpropagation gradient of 0.001725!\n",
      "Gradient is correct\n",
      "Parameter id: 738 Numerical gradient of -0.000830 is not close to the backpropagation gradient of -0.000830!\n",
      "Parameter id: 739 Numerical gradient of -0.004018 is not close to the backpropagation gradient of -0.004018!\n",
      "Parameter id: 740 Numerical gradient of 0.006765 is not close to the backpropagation gradient of 0.006765!\n",
      "Parameter id: 741 Numerical gradient of -0.002076 is not close to the backpropagation gradient of -0.002076!\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Parameter id: 745 Numerical gradient of 0.000901 is not close to the backpropagation gradient of 0.000901!\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Parameter id: 748 Numerical gradient of 0.006269 is not close to the backpropagation gradient of 0.006269!\n",
      "Parameter id: 749 Numerical gradient of -0.002751 is not close to the backpropagation gradient of -0.002751!\n",
      "Gradient is correct\n",
      "Parameter id: 751 Numerical gradient of -0.006408 is not close to the backpropagation gradient of -0.006408!\n",
      "Gradient is correct\n",
      "Parameter id: 753 Numerical gradient of -0.011216 is not close to the backpropagation gradient of -0.011216!\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Parameter id: 756 Numerical gradient of -0.009669 is not close to the backpropagation gradient of -0.009669!\n",
      "Parameter id: 757 Numerical gradient of -0.001022 is not close to the backpropagation gradient of -0.001022!\n",
      "Parameter id: 758 Numerical gradient of 0.000945 is not close to the backpropagation gradient of 0.000945!\n",
      "Parameter id: 759 Numerical gradient of 0.005640 is not close to the backpropagation gradient of 0.005640!\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Parameter id: 765 Numerical gradient of 0.007162 is not close to the backpropagation gradient of 0.007162!\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Parameter id: 768 Numerical gradient of 0.000514 is not close to the backpropagation gradient of 0.000514!\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Parameter id: 775 Numerical gradient of -0.000482 is not close to the backpropagation gradient of -0.000482!\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Parameter id: 779 Numerical gradient of 0.001671 is not close to the backpropagation gradient of 0.001671!\n",
      "Parameter id: 780 Numerical gradient of -0.003748 is not close to the backpropagation gradient of -0.003748!\n",
      "Parameter id: 781 Numerical gradient of 0.001485 is not close to the backpropagation gradient of 0.001485!\n",
      "Gradient is correct\n",
      "Parameter id: 783 Numerical gradient of 0.009074 is not close to the backpropagation gradient of 0.009074!\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Parameter id: 786 Numerical gradient of 0.006020 is not close to the backpropagation gradient of 0.006020!\n",
      "Parameter id: 787 Numerical gradient of -0.001724 is not close to the backpropagation gradient of -0.001724!\n",
      "Parameter id: 788 Numerical gradient of -0.003856 is not close to the backpropagation gradient of -0.003856!\n",
      "Parameter id: 789 Numerical gradient of -0.003210 is not close to the backpropagation gradient of -0.003210!\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Parameter id: 792 Numerical gradient of 0.002694 is not close to the backpropagation gradient of 0.002694!\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Parameter id: 795 Numerical gradient of 0.000019 is not close to the backpropagation gradient of 0.000019!\n",
      "Parameter id: 796 Numerical gradient of 0.004375 is not close to the backpropagation gradient of 0.004375!\n",
      "Gradient is correct\n",
      "Parameter id: 798 Numerical gradient of -0.001704 is not close to the backpropagation gradient of -0.001704!\n",
      "Parameter id: 799 Numerical gradient of -0.001681 is not close to the backpropagation gradient of -0.001681!\n",
      "Parameter id: 800 Numerical gradient of 0.003295 is not close to the backpropagation gradient of 0.003295!\n",
      "Parameter id: 801 Numerical gradient of -0.002561 is not close to the backpropagation gradient of -0.002561!\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Parameter id: 804 Numerical gradient of 0.001212 is not close to the backpropagation gradient of 0.001212!\n",
      "Parameter id: 805 Numerical gradient of -0.000804 is not close to the backpropagation gradient of -0.000804!\n",
      "Parameter id: 806 Numerical gradient of -0.002720 is not close to the backpropagation gradient of -0.002720!\n",
      "Parameter id: 807 Numerical gradient of -0.001555 is not close to the backpropagation gradient of -0.001555!\n",
      "Parameter id: 808 Numerical gradient of 0.002776 is not close to the backpropagation gradient of 0.002776!\n",
      "Parameter id: 809 Numerical gradient of -0.002782 is not close to the backpropagation gradient of -0.002782!\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Parameter id: 816 Numerical gradient of 0.010672 is not close to the backpropagation gradient of 0.010672!\n",
      "Parameter id: 817 Numerical gradient of 0.001326 is not close to the backpropagation gradient of 0.001326!\n",
      "Parameter id: 818 Numerical gradient of 0.000066 is not close to the backpropagation gradient of 0.000066!\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Parameter id: 822 Numerical gradient of 0.003097 is not close to the backpropagation gradient of 0.003097!\n",
      "Gradient is correct\n",
      "Parameter id: 824 Numerical gradient of 0.011368 is not close to the backpropagation gradient of 0.011368!\n",
      "Parameter id: 825 Numerical gradient of -0.007286 is not close to the backpropagation gradient of -0.007286!\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Parameter id: 828 Numerical gradient of -0.000826 is not close to the backpropagation gradient of -0.000826!\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Parameter id: 835 Numerical gradient of -0.000365 is not close to the backpropagation gradient of -0.000365!\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Parameter id: 839 Numerical gradient of -0.000501 is not close to the backpropagation gradient of -0.000501!\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Parameter id: 847 Numerical gradient of 0.002242 is not close to the backpropagation gradient of 0.002242!\n",
      "Parameter id: 848 Numerical gradient of -0.000837 is not close to the backpropagation gradient of -0.000837!\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Parameter id: 852 Numerical gradient of 0.003077 is not close to the backpropagation gradient of 0.003077!\n",
      "Parameter id: 853 Numerical gradient of -0.009777 is not close to the backpropagation gradient of -0.009777!\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Parameter id: 858 Numerical gradient of 0.000619 is not close to the backpropagation gradient of 0.000619!\n",
      "Gradient is correct\n",
      "Parameter id: 860 Numerical gradient of 0.014615 is not close to the backpropagation gradient of 0.014615!\n",
      "Parameter id: 861 Numerical gradient of -0.006205 is not close to the backpropagation gradient of -0.006205!\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Parameter id: 864 Numerical gradient of 0.004966 is not close to the backpropagation gradient of 0.004966!\n",
      "Parameter id: 865 Numerical gradient of 0.003044 is not close to the backpropagation gradient of 0.003044!\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Parameter id: 869 Numerical gradient of -0.004168 is not close to the backpropagation gradient of -0.004168!\n",
      "Gradient is correct\n",
      "Parameter id: 871 Numerical gradient of -0.003517 is not close to the backpropagation gradient of -0.003517!\n",
      "Parameter id: 872 Numerical gradient of -0.001715 is not close to the backpropagation gradient of -0.001715!\n",
      "Gradient is correct\n",
      "Parameter id: 874 Numerical gradient of -0.009027 is not close to the backpropagation gradient of -0.009027!\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Parameter id: 877 Numerical gradient of 0.000223 is not close to the backpropagation gradient of 0.000223!\n",
      "Parameter id: 878 Numerical gradient of 0.000653 is not close to the backpropagation gradient of 0.000653!\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Parameter id: 882 Numerical gradient of -0.002441 is not close to the backpropagation gradient of -0.002441!\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Parameter id: 885 Numerical gradient of 0.002670 is not close to the backpropagation gradient of 0.002670!\n",
      "Parameter id: 886 Numerical gradient of 0.003964 is not close to the backpropagation gradient of 0.003964!\n",
      "Gradient is correct\n",
      "Parameter id: 888 Numerical gradient of 0.001431 is not close to the backpropagation gradient of 0.001431!\n",
      "Parameter id: 889 Numerical gradient of 0.004606 is not close to the backpropagation gradient of 0.004606!\n",
      "Gradient is correct\n",
      "Parameter id: 891 Numerical gradient of 0.007206 is not close to the backpropagation gradient of 0.007206!\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Parameter id: 894 Numerical gradient of -0.004865 is not close to the backpropagation gradient of -0.004865!\n",
      "Parameter id: 895 Numerical gradient of 0.001801 is not close to the backpropagation gradient of 0.001801!\n",
      "Gradient is correct\n",
      "Parameter id: 897 Numerical gradient of 0.008791 is not close to the backpropagation gradient of 0.008791!\n",
      "Gradient is correct\n",
      "Parameter id: 899 Numerical gradient of 0.000130 is not close to the backpropagation gradient of 0.000130!\n",
      "Parameter id: 900 Numerical gradient of 0.000864 is not close to the backpropagation gradient of 0.000864!\n",
      "Parameter id: 901 Numerical gradient of 0.000860 is not close to the backpropagation gradient of 0.000860!\n",
      "Gradient is correct\n",
      "Parameter id: 903 Numerical gradient of -0.004059 is not close to the backpropagation gradient of -0.004059!\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Parameter id: 906 Numerical gradient of -0.002442 is not close to the backpropagation gradient of -0.002442!\n",
      "Gradient is correct\n",
      "Parameter id: 908 Numerical gradient of 0.003433 is not close to the backpropagation gradient of 0.003433!\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Parameter id: 912 Numerical gradient of -0.001257 is not close to the backpropagation gradient of -0.001257!\n",
      "Parameter id: 913 Numerical gradient of 0.002746 is not close to the backpropagation gradient of 0.002746!\n",
      "Parameter id: 914 Numerical gradient of -0.007619 is not close to the backpropagation gradient of -0.007619!\n",
      "Parameter id: 915 Numerical gradient of -0.002221 is not close to the backpropagation gradient of -0.002221!\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Parameter id: 918 Numerical gradient of 0.001218 is not close to the backpropagation gradient of 0.001218!\n",
      "Parameter id: 919 Numerical gradient of -0.002231 is not close to the backpropagation gradient of -0.002231!\n",
      "Parameter id: 920 Numerical gradient of -0.001929 is not close to the backpropagation gradient of -0.001929!\n",
      "Parameter id: 921 Numerical gradient of -0.001875 is not close to the backpropagation gradient of -0.001875!\n",
      "Parameter id: 922 Numerical gradient of -0.001406 is not close to the backpropagation gradient of -0.001406!\n",
      "Gradient is correct\n",
      "Parameter id: 924 Numerical gradient of 0.001076 is not close to the backpropagation gradient of 0.001076!\n",
      "Parameter id: 925 Numerical gradient of 0.000836 is not close to the backpropagation gradient of 0.000836!\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Parameter id: 929 Numerical gradient of 0.001861 is not close to the backpropagation gradient of 0.001861!\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Parameter id: 933 Numerical gradient of 0.010392 is not close to the backpropagation gradient of 0.010392!\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Parameter id: 937 Numerical gradient of 0.000074 is not close to the backpropagation gradient of 0.000074!\n",
      "Parameter id: 938 Numerical gradient of -0.000453 is not close to the backpropagation gradient of -0.000453!\n",
      "Parameter id: 939 Numerical gradient of -0.004203 is not close to the backpropagation gradient of -0.004203!\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Parameter id: 943 Numerical gradient of -0.010572 is not close to the backpropagation gradient of -0.010572!\n",
      "Gradient is correct\n",
      "Parameter id: 945 Numerical gradient of -0.006098 is not close to the backpropagation gradient of -0.006098!\n",
      "Parameter id: 946 Numerical gradient of -0.005624 is not close to the backpropagation gradient of -0.005624!\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Parameter id: 951 Numerical gradient of -0.008060 is not close to the backpropagation gradient of -0.008060!\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Parameter id: 954 Numerical gradient of 0.005939 is not close to the backpropagation gradient of 0.005939!\n",
      "Parameter id: 955 Numerical gradient of 0.000097 is not close to the backpropagation gradient of 0.000097!\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Parameter id: 958 Numerical gradient of 0.019086 is not close to the backpropagation gradient of 0.019086!\n",
      "Parameter id: 959 Numerical gradient of -0.001174 is not close to the backpropagation gradient of -0.001174!\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Parameter id: 967 Numerical gradient of -0.001051 is not close to the backpropagation gradient of -0.001051!\n",
      "Parameter id: 968 Numerical gradient of 0.002445 is not close to the backpropagation gradient of 0.002445!\n",
      "Parameter id: 969 Numerical gradient of 0.007185 is not close to the backpropagation gradient of 0.007185!\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Parameter id: 976 Numerical gradient of 0.003054 is not close to the backpropagation gradient of 0.003054!\n",
      "Gradient is correct\n",
      "Parameter id: 978 Numerical gradient of 0.000883 is not close to the backpropagation gradient of 0.000883!\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Parameter id: 985 Numerical gradient of 0.000981 is not close to the backpropagation gradient of 0.000981!\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Parameter id: 989 Numerical gradient of 0.002207 is not close to the backpropagation gradient of 0.002207!\n",
      "Gradient is correct\n",
      "Parameter id: 991 Numerical gradient of -0.006773 is not close to the backpropagation gradient of -0.006773!\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Parameter id: 994 Numerical gradient of -0.007385 is not close to the backpropagation gradient of -0.007385!\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Parameter id: 997 Numerical gradient of 0.002590 is not close to the backpropagation gradient of 0.002590!\n",
      "Gradient is correct\n",
      "Parameter id: 999 Numerical gradient of 0.004733 is not close to the backpropagation gradient of 0.004733!\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Parameter id: 1003 Numerical gradient of 0.001329 is not close to the backpropagation gradient of 0.001329!\n",
      "Parameter id: 1004 Numerical gradient of 0.000625 is not close to the backpropagation gradient of 0.000625!\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Parameter id: 1015 Numerical gradient of 0.001780 is not close to the backpropagation gradient of 0.001780!\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Parameter id: 1021 Numerical gradient of 0.004930 is not close to the backpropagation gradient of 0.004930!\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Parameter id: 1025 Numerical gradient of 0.000184 is not close to the backpropagation gradient of 0.000184!\n",
      "Parameter id: 1026 Numerical gradient of -0.009937 is not close to the backpropagation gradient of -0.009937!\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Parameter id: 1034 Numerical gradient of -0.005612 is not close to the backpropagation gradient of -0.005612!\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Parameter id: 1043 Numerical gradient of -0.003133 is not close to the backpropagation gradient of -0.003133!\n",
      "Parameter id: 1044 Numerical gradient of -0.007626 is not close to the backpropagation gradient of -0.007626!\n",
      "Gradient is correct\n",
      "Parameter id: 1046 Numerical gradient of -0.011109 is not close to the backpropagation gradient of -0.011109!\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Parameter id: 1052 Numerical gradient of 0.008605 is not close to the backpropagation gradient of 0.008605!\n",
      "Parameter id: 1053 Numerical gradient of 0.013057 is not close to the backpropagation gradient of 0.013057!\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Parameter id: 1058 Numerical gradient of 0.005678 is not close to the backpropagation gradient of 0.005678!\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Parameter id: 1070 Numerical gradient of -0.001756 is not close to the backpropagation gradient of -0.001756!\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Parameter id: 1073 Numerical gradient of -0.015851 is not close to the backpropagation gradient of -0.015851!\n",
      "Gradient is correct\n",
      "Parameter id: 1075 Numerical gradient of -0.005387 is not close to the backpropagation gradient of -0.005387!\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Parameter id: 1079 Numerical gradient of -0.003156 is not close to the backpropagation gradient of -0.003156!\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Parameter id: 1084 Numerical gradient of -0.006130 is not close to the backpropagation gradient of -0.006130!\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Parameter id: 1088 Numerical gradient of -0.002777 is not close to the backpropagation gradient of -0.002777!\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Parameter id: 1093 Numerical gradient of 0.003716 is not close to the backpropagation gradient of 0.003716!\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Parameter id: 1097 Numerical gradient of 0.005369 is not close to the backpropagation gradient of 0.005369!\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Parameter id: 1110 Numerical gradient of -0.009804 is not close to the backpropagation gradient of -0.009804!\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Parameter id: 1129 Numerical gradient of 0.008535 is not close to the backpropagation gradient of 0.008535!\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Parameter id: 1133 Numerical gradient of 0.002310 is not close to the backpropagation gradient of 0.002310!\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Parameter id: 1139 Numerical gradient of -0.012328 is not close to the backpropagation gradient of -0.012328!\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Parameter id: 1142 Numerical gradient of -0.000484 is not close to the backpropagation gradient of -0.000484!\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Parameter id: 1146 Numerical gradient of 0.000737 is not close to the backpropagation gradient of 0.000737!\n",
      "Parameter id: 1147 Numerical gradient of -0.000831 is not close to the backpropagation gradient of -0.000831!\n",
      "Parameter id: 1148 Numerical gradient of 0.000970 is not close to the backpropagation gradient of 0.000970!\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Parameter id: 1151 Numerical gradient of 0.003355 is not close to the backpropagation gradient of 0.003355!\n",
      "Parameter id: 1152 Numerical gradient of 0.001075 is not close to the backpropagation gradient of 0.001075!\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Parameter id: 1173 Numerical gradient of -0.000353 is not close to the backpropagation gradient of -0.000353!\n",
      "Gradient is correct\n",
      "Parameter id: 1175 Numerical gradient of -0.012196 is not close to the backpropagation gradient of -0.012196!\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Parameter id: 1183 Numerical gradient of 0.007054 is not close to the backpropagation gradient of 0.007054!\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Parameter id: 1187 Numerical gradient of 0.003873 is not close to the backpropagation gradient of 0.003873!\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Parameter id: 1193 Numerical gradient of -0.011631 is not close to the backpropagation gradient of -0.011631!\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Parameter id: 1196 Numerical gradient of -0.001444 is not close to the backpropagation gradient of -0.001444!\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Parameter id: 1201 Numerical gradient of -0.000077 is not close to the backpropagation gradient of -0.000077!\n",
      "Parameter id: 1202 Numerical gradient of 0.008217 is not close to the backpropagation gradient of 0.008217!\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Parameter id: 1209 Numerical gradient of -0.000994 is not close to the backpropagation gradient of -0.000994!\n",
      "Parameter id: 1210 Numerical gradient of -0.005977 is not close to the backpropagation gradient of -0.005977!\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Parameter id: 1214 Numerical gradient of 0.008949 is not close to the backpropagation gradient of 0.008949!\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Parameter id: 1217 Numerical gradient of 0.008010 is not close to the backpropagation gradient of 0.008010!\n",
      "Gradient is correct\n",
      "Parameter id: 1219 Numerical gradient of 0.001091 is not close to the backpropagation gradient of 0.001091!\n",
      "Parameter id: 1220 Numerical gradient of -0.004103 is not close to the backpropagation gradient of -0.004103!\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Parameter id: 1223 Numerical gradient of 0.001992 is not close to the backpropagation gradient of 0.001992!\n",
      "Parameter id: 1224 Numerical gradient of -0.017466 is not close to the backpropagation gradient of -0.017466!\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Parameter id: 1227 Numerical gradient of 0.012872 is not close to the backpropagation gradient of 0.012872!\n",
      "Parameter id: 1228 Numerical gradient of -0.001254 is not close to the backpropagation gradient of -0.001254!\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Parameter id: 1233 Numerical gradient of 0.011409 is not close to the backpropagation gradient of 0.011409!\n",
      "Gradient is correct\n",
      "Parameter id: 1235 Numerical gradient of 0.005267 is not close to the backpropagation gradient of 0.005267!\n",
      "Gradient is correct\n",
      "Parameter id: 1237 Numerical gradient of 0.002385 is not close to the backpropagation gradient of 0.002385!\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Parameter id: 1247 Numerical gradient of -0.002612 is not close to the backpropagation gradient of -0.002612!\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Parameter id: 1250 Numerical gradient of 0.007771 is not close to the backpropagation gradient of 0.007771!\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Parameter id: 1256 Numerical gradient of -0.003705 is not close to the backpropagation gradient of -0.003705!\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Parameter id: 1264 Numerical gradient of -0.000978 is not close to the backpropagation gradient of -0.000978!\n",
      "Gradient is correct\n",
      "Parameter id: 1266 Numerical gradient of 0.001140 is not close to the backpropagation gradient of 0.001140!\n",
      "Parameter id: 1267 Numerical gradient of -0.004759 is not close to the backpropagation gradient of -0.004759!\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Parameter id: 1271 Numerical gradient of 0.003843 is not close to the backpropagation gradient of 0.003843!\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Parameter id: 1282 Numerical gradient of 0.004113 is not close to the backpropagation gradient of 0.004113!\n",
      "Parameter id: 1283 Numerical gradient of 0.000523 is not close to the backpropagation gradient of 0.000523!\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Parameter id: 1291 Numerical gradient of 0.004800 is not close to the backpropagation gradient of 0.004800!\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n",
      "Gradient is correct\n"
     ]
    }
   ],
   "source": [
    "# Do gradient checking\n",
    "# Define an RNN to test\n",
    "RNN = RnnBinaryAdder(1, output_size, hidden_size, seq_length)\n",
    "# Get the gradients of the parameters from a subset of the data\n",
    "backprop_grads = RNN.getParamGrads(\n",
    "    x_train[:100], y_train[:100])\n",
    "\n",
    "eps = 1e-7  # Set the small change to compute the numerical gradient\n",
    "# Compute the numerical gradients of the parameters in all layers.\n",
    "for p_idx, param in enumerate(RNN.get_params_iter()):\n",
    "    grad_backprop = backprop_grads[p_idx]\n",
    "    # + eps\n",
    "    param += eps\n",
    "    plus_loss = RNN.loss(\n",
    "        RNN.getOutput(x_train[0:100]), y_train[0:100])\n",
    "    # - eps\n",
    "    param -= 2 * eps\n",
    "    min_loss = RNN.loss(\n",
    "        RNN.getOutput(x_train[0:100]), y_train[0:100])\n",
    "    # reset param value\n",
    "    param += eps\n",
    "    # calculate numerical gradient\n",
    "    grad_num = (plus_loss - min_loss) / (2*eps)\n",
    "    # Raise error if the numerical grade is not close to the \n",
    "    numerator = np.linalg.norm(grad_backprop - grad_num)\n",
    "    denominator = np.linalg.norm(grad_backprop) + np.linalg.norm(grad_num)\n",
    "    difference = numerator / denominator\n",
    "    #  backprop gradient\n",
    "    if difference < 1e-7:\n",
    "    #if not np.isclose(grad_num, grad_backprop):\n",
    "        print('Gradient is correct')\n",
    "    else:\n",
    "        print((\n",
    "            f'Parameter id: {p_idx} '\n",
    "            f'Numerical gradient of {grad_num:.6f} is not close '\n",
    "            f'to the backpropagation gradient of {grad_backprop:.6f}!'\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
