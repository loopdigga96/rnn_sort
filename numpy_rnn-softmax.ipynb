{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "%matplotlib inline\n",
    "\n",
    "import sys\n",
    "import itertools\n",
    "import numpy as np  # Matrix and vector computation package\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt  # Plotting library\n",
    "import seaborn as sns  # Fancier plots\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Set seaborn plotting style\n",
    "sns.set_style('darkgrid')\n",
    "# Set the seed for reproducability\n",
    "np.random.seed(seed=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sort_dataset(dataset_length, seq_length, max_number=999):\n",
    "    x_train = np.random.randint(low=0, high=max_number+1, size=(dataset_length, seq_length, 1))\n",
    "    y_train = np.sort(x_train, axis=1)\n",
    "    \n",
    "    x_test = np.random.randint(low=0, high=max_number+1, size=(dataset_length, seq_length, 1))\n",
    "    y_test = np.sort(x_test, axis=1)\n",
    "    \n",
    "    return x_train, y_train, x_test, y_test\n",
    "\n",
    "def create_dummy_dataset(dataset_length, seq_length, max_number):\n",
    "    lower_bound = -1 * max_number\n",
    "    x_train = np.random.randint(low=lower_bound, high=max_number+1, size=(dataset_length, seq_length, 1))\n",
    "    y_train = np.where(x_train.sum(axis=2) > 0, 1, 0).reshape(x_train.shape)\n",
    "    \n",
    "    x_test = np.random.randint(low=lower_bound, high=max_number+1, size=(dataset_length, seq_length, 1))\n",
    "    y_test = np.where(x_test.sum(axis=2) > 0, 1, 0).reshape(x_train.shape)\n",
    "    \n",
    "    return x_train, y_train, x_test, y_test\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the linear tensor transformation layer\n",
    "class TensorLinear(object):\n",
    "    \"\"\"The linear tensor layer applies a linear tensor dot product \n",
    "    and a bias to its input.\"\"\"\n",
    "    def __init__(self, n_in, n_out, tensor_order, W=None, b=None):\n",
    "        \"\"\"Initialse the weight W and bias b parameters.\"\"\"\n",
    "        a = np.sqrt(6.0 / (n_in + n_out))\n",
    "        self.W = (np.random.uniform(-a, a, (n_in, n_out)) \n",
    "                  if W is None else W)\n",
    "        self.b = (np.zeros((n_out)) if b is None else b)\n",
    "        # Axes summed over in backprop\n",
    "        self.bpAxes = tuple(range(tensor_order-1))\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"Perform forward step transformation with the help \n",
    "        of a tensor product.\"\"\"\n",
    "        # Same as: Y[i,j,:] = np.dot(X[i,j,:], self.W) + self.b \n",
    "        #          (for i,j in X.shape[0:1])\n",
    "        # Same as: Y = np.einsum('ijk,kl->ijl', X, self.W) + self.b\n",
    "        return np.tensordot(X, self.W, axes=((-1),(0))) + self.b\n",
    "\n",
    "    def backward(self, X, gY):\n",
    "        \"\"\"Return the gradient of the parmeters and the inputs of \n",
    "        this layer.\"\"\"\n",
    "        # Same as: gW = np.einsum('ijk,ijl->kl', X, gY)\n",
    "        # Same as: gW += np.dot(X[:,j,:].T, gY[:,j,:]) \n",
    "        #          (for i,j in X.shape[0:1])\n",
    "        gW = np.tensordot(X, gY, axes=(self.bpAxes, self.bpAxes))\n",
    "        gB = np.sum(gY, axis=self.bpAxes)\n",
    "        # Same as: gX = np.einsum('ijk,kl->ijl', gY, self.W.T)\n",
    "        # Same as: gX[i,j,:] = np.dot(gY[i,j,:], self.W.T) \n",
    "        #          (for i,j in gY.shape[0:1])\n",
    "        gX = np.tensordot(gY, self.W.T, axes=((-1),(0)))  \n",
    "        return gX, gW, gB\n",
    "\n",
    "# Define the logistic classifier layer\n",
    "class LogisticClassifier(object):\n",
    "    \"\"\"The logistic layer applies the logistic function to its \n",
    "    inputs.\"\"\"\n",
    "   \n",
    "    def forward(self, X):\n",
    "        \"\"\"Perform the forward step transformation.\"\"\"\n",
    "        return 1. / (1. + np.exp(-X))\n",
    "    \n",
    "    def backward(self, Y, T):\n",
    "        \"\"\"Return the gradient with respect to the loss function \n",
    "        at the inputs of this layer.\"\"\"\n",
    "        # Average by the number of samples and sequence length.\n",
    "        return (Y - T) / (Y.shape[0] * Y.shape[1])\n",
    "    \n",
    "    def loss(self, Y, T):\n",
    "        \"\"\"Compute the loss at the output.\"\"\"\n",
    "        return -np.mean((T * np.log(Y)) + ((1-T) * np.log(1-Y)))\n",
    "\n",
    "class LogisticClassifierSoftmax:\n",
    "    def forward(self, X, theta = 1.0, axis = 2):\n",
    "        \"Takes X as 3d tensor\"\n",
    "\n",
    "        # multiply y against the theta parameter,\n",
    "        y = X * float(theta)\n",
    "        # subtract the max for numerical stability\n",
    "        y = y - np.expand_dims(np.max(y, axis = axis), axis)\n",
    "        # exponentiate y\n",
    "        y = np.exp(y)\n",
    "        # take the sum along the specified axis\n",
    "        ax_sum = np.expand_dims(np.sum(y, axis = axis), axis)\n",
    "        # finally: divide elementwise\n",
    "        return y / ax_sum\n",
    "        \n",
    "    def loss(self, Y, T):\n",
    "        \"\"\"\n",
    "        Y is the output from fully connected layer passed through softmax (batch_size x num_examples x num_classes)\n",
    "        T is labels (batch_size x num_examples x 1)\n",
    "            Note that y is not one-hot encoded vector. \n",
    "            It can be computed as y.argmax(axis=1) from one-hot encoded vectors of labels if required.\n",
    "        \"\"\"\n",
    "#         print(f\"Y.shape {Y.shape}\")\n",
    "#         print(f\"T.shape {T.shape}\")\n",
    "        m = T.shape[1]\n",
    "        #ps = self.forward(Y, axis=2)\n",
    "\n",
    "        losses = []\n",
    "        for idx, p in enumerate(Y):  \n",
    "            log_likelihood = -np.log(p[range(m), T[idx].flatten()])\n",
    "            loss = np.sum(log_likelihood) / m\n",
    "            losses.append(loss)\n",
    "        return np.mean(losses)\n",
    "\n",
    "    def backward(self, X, T):\n",
    "        \"\"\"\n",
    "        X is the output from fully connected layer passed through softmax (batch_size x num_examples x num_classes)\n",
    "        T is labels (batch_size x num_examples x 1)\n",
    "            Note that y is not one-hot encoded vector. \n",
    "            It can be computed as y.argmax(axis=1) from one-hot encoded vectors of labels if required.\n",
    "        \"\"\"\n",
    "        delta = np.zeros(X.shape)\n",
    "        m = T.shape[1]\n",
    "        \n",
    "        for idx in range(len(delta)):\n",
    "            #x = Y[idx]\n",
    "            #grad = self.forward(x, axis=1)\n",
    "            #grad = x\n",
    "            grad = X[idx]\n",
    "            grad[range(m),T[idx].flatten()] -= 1\n",
    "            grad = grad/m\n",
    "            delta[idx] = grad\n",
    "        return delta\n",
    "\n",
    "# Define tanh layer\n",
    "class TanH(object):\n",
    "    \"\"\"TanH applies the tanh function to its inputs.\"\"\"\n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"Perform the forward step transformation.\"\"\"\n",
    "        return np.tanh(X) \n",
    "    \n",
    "    def backward(self, Y, output_grad):\n",
    "        \"\"\"Return the gradient at the inputs of this layer.\"\"\"\n",
    "        gTanh = 1.0 - (Y**2)\n",
    "        return (gTanh * output_grad)\n",
    "\n",
    "# Define internal state update layer\n",
    "class RecurrentStateUpdate(object):\n",
    "    \"\"\"Update a given state.\"\"\"\n",
    "    def __init__(self, nbStates, W, b):\n",
    "        \"\"\"Initialse the linear transformation and tanh transfer \n",
    "        function.\"\"\"\n",
    "        self.linear = TensorLinear(nbStates, nbStates, 2, W, b)\n",
    "        self.tanh = TanH()\n",
    "\n",
    "    def forward(self, Xk, Sk):\n",
    "        \"\"\"Return state k+1 from input and state k.\"\"\"\n",
    "        return self.tanh.forward(Xk + self.linear.forward(Sk))\n",
    "    \n",
    "    def backward(self, Sk0, Sk1, output_grad):\n",
    "        \"\"\"Return the gradient of the parmeters and the inputs of \n",
    "        this layer.\"\"\"\n",
    "        gZ = self.tanh.backward(Sk1, output_grad)\n",
    "        gSk0, gW, gB = self.linear.backward(Sk0, gZ)\n",
    "        return gZ, gSk0, gW, gB\n",
    "\n",
    "# Define layer that unfolds the states over time\n",
    "class RecurrentStateUnfold(object):\n",
    "    \"\"\"Unfold the recurrent states.\"\"\"\n",
    "    def __init__(self, nbStates, nbTimesteps):\n",
    "        \"\"\"Initialse the shared parameters, the inital state and \n",
    "        state update function.\"\"\"\n",
    "        a = np.sqrt(6. / (nbStates * 2))\n",
    "        self.W = np.random.uniform(-a, a, (nbStates, nbStates))\n",
    "        self.b = np.zeros((self.W.shape[0]))  # Shared bias\n",
    "        self.S0 = np.zeros(nbStates)  # Initial state\n",
    "        self.nbTimesteps = nbTimesteps  # Timesteps to unfold\n",
    "        self.stateUpdate = RecurrentStateUpdate(\n",
    "            nbStates, self.W, self.b)  # State update function\n",
    "        \n",
    "    def forward(self, X):\n",
    "        \"\"\"Iteratively apply forward step to all states.\"\"\"\n",
    "        # State tensor\n",
    "        S = np.zeros((X.shape[0], X.shape[1]+1, self.W.shape[0]))\n",
    "        S[:,0,:] = self.S0  # Set initial state\n",
    "        for k in range(self.nbTimesteps):\n",
    "            # Update the states iteratively\n",
    "            S[:,k+1,:] = self.stateUpdate.forward(X[:,k,:], S[:,k,:])\n",
    "        return S\n",
    "    \n",
    "    def backward(self, X, S, gY):\n",
    "        \"\"\"Return the gradient of the parmeters and the inputs of \n",
    "        this layer.\"\"\"\n",
    "        # Initialise gradient of state outputs\n",
    "        gSk = np.zeros_like(gY[:,self.nbTimesteps-1,:])\n",
    "        # Initialse gradient tensor for state inputs\n",
    "        gZ = np.zeros_like(X)\n",
    "        gWSum = np.zeros_like(self.W)  # Initialise weight gradients\n",
    "        gBSum = np.zeros_like(self.b)  # Initialse bias gradients\n",
    "        # Propagate the gradients iteratively\n",
    "        for k in range(self.nbTimesteps-1, -1, -1):\n",
    "            # Gradient at state output is gradient from previous state \n",
    "            #  plus gradient from output\n",
    "            gSk += gY[:,k,:]\n",
    "            # Propgate the gradient back through one state\n",
    "            gZ[:,k,:], gSk, gW, gB = self.stateUpdate.backward(\n",
    "                S[:,k,:], S[:,k+1,:], gSk)\n",
    "            gWSum += gW  # Update total weight gradient\n",
    "            gBSum += gB  # Update total bias gradient\n",
    "        # Get gradient of initial state over all samples\n",
    "        gS0 = np.sum(gSk, axis=0)\n",
    "        return gZ, gWSum, gBSum, gS0\n",
    "\n",
    "# Define the full network\n",
    "class RnnBinaryAdder(object):\n",
    "    \"\"\"RNN to perform binary addition of 2 numbers.\"\"\"\n",
    "    def __init__(self, nb_of_inputs, nb_of_outputs, nb_of_states, \n",
    "                 sequence_len):\n",
    "        \"\"\"Initialse the network layers.\"\"\"\n",
    "        # Input layer\n",
    "        self.tensorInput = TensorLinear(nb_of_inputs, nb_of_states, 3)\n",
    "        # Recurrent layer\n",
    "        self.rnnUnfold = RecurrentStateUnfold(nb_of_states, sequence_len)\n",
    "        # Linear output transform\n",
    "        self.tensorOutput = TensorLinear(nb_of_states, nb_of_outputs, 3)\n",
    "        self.classifier = LogisticClassifierSoftmax()  # Classification output\n",
    "        #self.classifier = LogisticClassifier()  # Classification output\n",
    "        self.sequence_len = sequence_len\n",
    "        \n",
    "    def forward(self, X):\n",
    "        \"\"\"Perform the forward propagation of input X through all \n",
    "        layers.\"\"\"\n",
    "        # Linear input transformation\n",
    "        recIn = self.tensorInput.forward(X)\n",
    "        # Forward propagate through time and return states\n",
    "        S = self.rnnUnfold.forward(recIn)\n",
    "        # Linear output transformation\n",
    "        Z = self.tensorOutput.forward(S[:,1:self.sequence_len+1,:])\n",
    "        Y = self.classifier.forward(Z)  # Classification probabilities\n",
    "        # Return: input to recurrent layer, states, input to classifier, \n",
    "        #  output\n",
    "        return recIn, S, Z, Y\n",
    "    \n",
    "    def backward(self, X, Y, recIn, S, T):\n",
    "        \"\"\"Perform the backward propagation through all layers.\n",
    "        Input: input samples, network output, input to recurrent \n",
    "        layer, states, targets.\"\"\"\n",
    "        gZ = self.classifier.backward(Y, T)  # Get output gradient\n",
    "        gRecOut, gWout, gBout = self.tensorOutput.backward(\n",
    "            S[:,1:self.sequence_len+1,:], gZ)\n",
    "        # Propagate gradient backwards through time\n",
    "        gRnnIn, gWrec, gBrec, gS0 = self.rnnUnfold.backward(\n",
    "            recIn, S, gRecOut)\n",
    "        gX, gWin, gBin = self.tensorInput.backward(X, gRnnIn)\n",
    "        # Return the parameter gradients of: linear output weights, \n",
    "        #  linear output bias, recursive weights, recursive bias, #\n",
    "        #  linear input weights, linear input bias, initial state.\n",
    "        return gWout, gBout, gWrec, gBrec, gWin, gBin, gS0\n",
    "    \n",
    "    def getOutput(self, X):\n",
    "        \"\"\"Get the output probabilities of input X.\"\"\"\n",
    "        recIn, S, Z, Y = self.forward(X)\n",
    "        return Y\n",
    "    \n",
    "    def getBinaryOutput(self, X):\n",
    "        \"\"\"Get the binary output of input X.\"\"\"\n",
    "        return np.around(self.getOutput(X))\n",
    "    \n",
    "    def getParamGrads(self, X, T):\n",
    "        \"\"\"Return the gradients with respect to input X and \n",
    "        target T as a list. The list has the same order as the \n",
    "        get_params_iter iterator.\"\"\"\n",
    "        recIn, S, Z, Y = self.forward(X)\n",
    "        gWout, gBout, gWrec, gBrec, gWin, gBin, gS0 = self.backward(\n",
    "            X, Y, recIn, S, T)\n",
    "        return [g for g in itertools.chain(\n",
    "                np.nditer(gS0),\n",
    "                np.nditer(gWin),\n",
    "                np.nditer(gBin),\n",
    "                np.nditer(gWrec),\n",
    "                np.nditer(gBrec),\n",
    "                np.nditer(gWout),\n",
    "                np.nditer(gBout))]\n",
    "    \n",
    "    def loss(self, Y, T):\n",
    "        \"\"\"Return the loss of input X w.r.t. targets T.\"\"\"\n",
    "        return self.classifier.loss(Y, T)\n",
    "    \n",
    "    def get_params_iter(self):\n",
    "        \"\"\"Return an iterator over the parameters.\n",
    "        The iterator has the same order as get_params_grad.\n",
    "        The elements returned by the iterator are editable in-place.\"\"\"\n",
    "        return itertools.chain(\n",
    "            np.nditer(self.rnnUnfold.S0, op_flags=['readwrite']),\n",
    "            np.nditer(self.tensorInput.W, op_flags=['readwrite']),\n",
    "            np.nditer(self.tensorInput.b, op_flags=['readwrite']),\n",
    "            np.nditer(self.rnnUnfold.W, op_flags=['readwrite']),\n",
    "            np.nditer(self.rnnUnfold.b, op_flags=['readwrite']),\n",
    "            np.nditer(self.tensorOutput.W, op_flags=['readwrite']), \n",
    "            np.nditer(self.tensorOutput.b, op_flags=['readwrite']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "Epoch 2\n",
      "Epoch 3\n",
      "Epoch 4\n",
      "Epoch 5\n",
      "Epoch 6\n",
      "Epoch 7\n",
      "Epoch 8\n",
      "Epoch 9\n",
      "Epoch 10\n",
      "Epoch 11\n",
      "Epoch 12\n",
      "Epoch 13\n",
      "Epoch 14\n",
      "Epoch 15\n",
      "Epoch 16\n",
      "Epoch 17\n",
      "Epoch 18\n",
      "Epoch 19\n",
      "Epoch 20\n",
      "Epoch 21\n",
      "Epoch 22\n",
      "Epoch 23\n",
      "Epoch 24\n",
      "Epoch 25\n",
      "Epoch 26\n",
      "Epoch 27\n",
      "Epoch 28\n",
      "Epoch 29\n",
      "Epoch 30\n",
      "Epoch 31\n",
      "Epoch 32\n",
      "Epoch 33\n",
      "Epoch 34\n",
      "Epoch 35\n",
      "Epoch 36\n",
      "Epoch 37\n",
      "Epoch 38\n",
      "Epoch 39\n",
      "Epoch 40\n",
      "Epoch 41\n",
      "Epoch 42\n",
      "Epoch 43\n",
      "Epoch 44\n",
      "Epoch 45\n",
      "Epoch 46\n",
      "Epoch 47\n",
      "Epoch 48\n",
      "Epoch 49\n",
      "Epoch 50\n"
     ]
    }
   ],
   "source": [
    "# Set hyper-parameters\n",
    "lmbd = 0.5  # Rmsprop lambda\n",
    "learning_rate = 0.001  # Learning rate\n",
    "momentum_term = 0.80  # Momentum term\n",
    "eps = 1e-6  # Numerical stability term to prevent division by zero\n",
    "mb_size = 20  # Size of the minibatches (number of samples)\n",
    "max_num = 8\n",
    "seq_length = 9\n",
    "hidden_size = 30\n",
    "nb_train = 200\n",
    "\n",
    "x_train, y_train, x_test, y_test = create_sort_dataset(nb_train, seq_length, max_num)\n",
    "\n",
    "input_size = 1\n",
    "output_size = max_num+1\n",
    "\n",
    "# Create the network\n",
    "RNN = RnnBinaryAdder(1, max_num+1, hidden_size, seq_length)\n",
    "#RNN = RnnBinaryAdder(1, output_size, hidden_size, seq_length)\n",
    "# Set the initial parameters\n",
    "# Number of parameters in the network\n",
    "nbParameters =  sum(1 for _ in RNN.get_params_iter())\n",
    "# Rmsprop moving average\n",
    "maSquare = [0.0 for _ in range(nbParameters)]\n",
    "Vs = [0.0 for _ in range(nbParameters)]  # Momentum\n",
    "\n",
    "# Create a list of minibatch losses to be plotted\n",
    "ls_of_loss = [\n",
    "    RNN.loss(RNN.getOutput(x_train[0:mb_size]), y_train[0:mb_size])]\n",
    "\n",
    "# Iterate over some iterations\n",
    "for i in range(50):\n",
    "    print(f'Epoch {i+1}')\n",
    "    # Iterate over all the minibatches\n",
    "    for mb in range(nb_train // mb_size):\n",
    "        X_mb = x_train[mb:mb+mb_size,:,:]  # Input minibatch\n",
    "        T_mb = y_train[mb:mb+mb_size,:,:]  # Target minibatch\n",
    "        V_tmp = [v * momentum_term for v in Vs]\n",
    "        # Update each parameters according to previous gradient\n",
    "        for pIdx, P in enumerate(RNN.get_params_iter()):\n",
    "            P += V_tmp[pIdx]\n",
    "        # Get gradients after following old velocity\n",
    "        # Get the parameter gradients\n",
    "        backprop_grads = RNN.getParamGrads(X_mb, T_mb)    \n",
    "        # Update each parameter seperately\n",
    "        for pIdx, P in enumerate(RNN.get_params_iter()):\n",
    "            #P -= learning_rate * backprop_grads[pIdx]\n",
    "            # Update the Rmsprop moving averages\n",
    "            maSquare[pIdx] = lmbd * maSquare[pIdx] + (\n",
    "                1-lmbd) * backprop_grads[pIdx]**2\n",
    "            # Calculate the Rmsprop normalised gradient\n",
    "            pGradNorm = ((\n",
    "                learning_rate * backprop_grads[pIdx]) / np.sqrt(\n",
    "                maSquare[pIdx]) + eps)\n",
    "            # Update the momentum\n",
    "            Vs[pIdx] = V_tmp[pIdx] - pGradNorm     \n",
    "            P -= pGradNorm   # Update the parameter\n",
    "        # Add loss to list to plot\n",
    "        ls_of_loss.append(RNN.loss(RNN.getOutput(X_mb), T_mb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x504 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the loss over the iterations\n",
    "fig = plt.figure(figsize=(10, 7))\n",
    "plt.plot(ls_of_loss, 'b-')\n",
    "plt.xlabel('minibatch iteration')\n",
    "plt.ylabel('Cross entropy', fontsize=15)\n",
    "plt.title('Decrease of loss over backprop iteration')\n",
    "plt.xlim(0, 100)\n",
    "fig.subplots_adjust(bottom=0.2)\n",
    "plt.show()\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, x, y):\n",
    "    predictions = np.argmax(model.getOutput(x), axis=-1)\n",
    "    #print(predictions)\n",
    "    #print(y.squeeze())\n",
    "    print(classification_report(y.squeeze().flatten(), predictions.flatten()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.80      0.80        25\n",
      "           1       0.65      0.61      0.63        18\n",
      "           2       0.58      0.85      0.69        13\n",
      "           3       0.95      0.83      0.88        23\n",
      "           4       0.95      0.95      0.95        20\n",
      "           5       0.93      0.87      0.90        15\n",
      "           6       1.00      0.93      0.97        15\n",
      "           7       1.00      1.00      1.00        26\n",
      "           8       1.00      1.00      1.00        25\n",
      "\n",
      "   micro avg       0.88      0.88      0.88       180\n",
      "   macro avg       0.87      0.87      0.87       180\n",
      "weighted avg       0.89      0.88      0.88       180\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_model(RNN, X_mb, T_mb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter id: 0 Numerical gradient of -0.005755 is not close to the backpropagation gradient of -0.575475!\n",
      "Parameter id: 1 Numerical gradient of -0.028922 is not close to the backpropagation gradient of -2.892207!\n",
      "Parameter id: 2 Numerical gradient of 0.025352 is not close to the backpropagation gradient of 2.535227!\n",
      "Parameter id: 3 Numerical gradient of -0.007176 is not close to the backpropagation gradient of -0.717639!\n",
      "Parameter id: 4 Numerical gradient of -0.018423 is not close to the backpropagation gradient of -1.842317!\n",
      "Parameter id: 5 Numerical gradient of 0.018113 is not close to the backpropagation gradient of 1.811309!\n",
      "Parameter id: 6 Numerical gradient of 0.008659 is not close to the backpropagation gradient of 0.865924!\n",
      "Parameter id: 7 Numerical gradient of -0.000331 is not close to the backpropagation gradient of -0.033145!\n",
      "Parameter id: 8 Numerical gradient of 0.014325 is not close to the backpropagation gradient of 1.432529!\n",
      "Parameter id: 9 Numerical gradient of 0.023490 is not close to the backpropagation gradient of 2.348997!\n",
      "Parameter id: 10 Numerical gradient of 0.016192 is not close to the backpropagation gradient of 1.619222!\n",
      "Parameter id: 11 Numerical gradient of 0.015522 is not close to the backpropagation gradient of 1.552151!\n",
      "Parameter id: 12 Numerical gradient of -0.345824 is not close to the backpropagation gradient of -34.582450!\n",
      "Parameter id: 13 Numerical gradient of -0.252419 is not close to the backpropagation gradient of -25.241873!\n",
      "Parameter id: 14 Numerical gradient of -0.042503 is not close to the backpropagation gradient of -4.250269!\n",
      "Parameter id: 15 Numerical gradient of -0.036129 is not close to the backpropagation gradient of -3.612852!\n",
      "Parameter id: 16 Numerical gradient of 0.103125 is not close to the backpropagation gradient of 10.312540!\n",
      "Parameter id: 17 Numerical gradient of -0.079043 is not close to the backpropagation gradient of -7.904278!\n",
      "Parameter id: 18 Numerical gradient of -0.050214 is not close to the backpropagation gradient of -5.021386!\n",
      "Parameter id: 19 Numerical gradient of -0.165955 is not close to the backpropagation gradient of -16.595458!\n",
      "Parameter id: 20 Numerical gradient of 0.008459 is not close to the backpropagation gradient of 0.845942!\n",
      "Parameter id: 21 Numerical gradient of 0.014926 is not close to the backpropagation gradient of 1.492558!\n",
      "Parameter id: 22 Numerical gradient of -0.069791 is not close to the backpropagation gradient of -6.979075!\n",
      "Parameter id: 23 Numerical gradient of -0.048870 is not close to the backpropagation gradient of -4.886983!\n",
      "Parameter id: 24 Numerical gradient of -0.018341 is not close to the backpropagation gradient of -1.834055!\n",
      "Parameter id: 25 Numerical gradient of -0.028869 is not close to the backpropagation gradient of -2.886863!\n",
      "Parameter id: 26 Numerical gradient of 0.002535 is not close to the backpropagation gradient of 0.253476!\n",
      "Parameter id: 27 Numerical gradient of -0.031540 is not close to the backpropagation gradient of -3.153953!\n",
      "Parameter id: 28 Numerical gradient of -0.021479 is not close to the backpropagation gradient of -2.147890!\n",
      "Parameter id: 29 Numerical gradient of -0.035170 is not close to the backpropagation gradient of -3.516963!\n",
      "Parameter id: 30 Numerical gradient of -0.000771 is not close to the backpropagation gradient of -0.077064!\n",
      "Parameter id: 31 Numerical gradient of 0.031146 is not close to the backpropagation gradient of 3.114593!\n",
      "Parameter id: 32 Numerical gradient of -0.055029 is not close to the backpropagation gradient of -5.502874!\n",
      "Parameter id: 33 Numerical gradient of -0.063185 is not close to the backpropagation gradient of -6.318483!\n",
      "Parameter id: 34 Numerical gradient of -0.008721 is not close to the backpropagation gradient of -0.872092!\n",
      "Parameter id: 35 Numerical gradient of -0.028151 is not close to the backpropagation gradient of -2.815105!\n",
      "Parameter id: 36 Numerical gradient of 0.028514 is not close to the backpropagation gradient of 2.851437!\n",
      "Parameter id: 37 Numerical gradient of -0.044227 is not close to the backpropagation gradient of -4.422696!\n",
      "Parameter id: 38 Numerical gradient of -0.019216 is not close to the backpropagation gradient of -1.921557!\n",
      "Parameter id: 39 Numerical gradient of -0.029647 is not close to the backpropagation gradient of -2.964666!\n",
      "Parameter id: 40 Numerical gradient of -0.001130 is not close to the backpropagation gradient of -0.112969!\n",
      "Parameter id: 41 Numerical gradient of -0.029978 is not close to the backpropagation gradient of -2.997822!\n",
      "Parameter id: 42 Numerical gradient of 0.050858 is not close to the backpropagation gradient of 5.085840!\n",
      "Parameter id: 43 Numerical gradient of 0.058589 is not close to the backpropagation gradient of 5.858902!\n",
      "Parameter id: 44 Numerical gradient of 0.011375 is not close to the backpropagation gradient of 1.137548!\n",
      "Parameter id: 45 Numerical gradient of 0.025637 is not close to the backpropagation gradient of 2.563679!\n",
      "Parameter id: 46 Numerical gradient of -0.025331 is not close to the backpropagation gradient of -2.533095!\n",
      "Parameter id: 47 Numerical gradient of 0.041180 is not close to the backpropagation gradient of 4.117954!\n",
      "Parameter id: 48 Numerical gradient of 0.018853 is not close to the backpropagation gradient of 1.885304!\n",
      "Parameter id: 49 Numerical gradient of 0.027894 is not close to the backpropagation gradient of 2.789394!\n",
      "Parameter id: 50 Numerical gradient of 0.001075 is not close to the backpropagation gradient of 0.107546!\n",
      "Parameter id: 51 Numerical gradient of -0.013560 is not close to the backpropagation gradient of -1.356027!\n",
      "Parameter id: 52 Numerical gradient of 0.024158 is not close to the backpropagation gradient of 2.415782!\n",
      "Parameter id: 53 Numerical gradient of 0.034703 is not close to the backpropagation gradient of 3.470344!\n",
      "Parameter id: 54 Numerical gradient of 0.002694 is not close to the backpropagation gradient of 0.269440!\n",
      "Parameter id: 55 Numerical gradient of 0.014829 is not close to the backpropagation gradient of 1.482926!\n",
      "Parameter id: 56 Numerical gradient of -0.012172 is not close to the backpropagation gradient of -1.217212!\n",
      "Parameter id: 57 Numerical gradient of 0.026929 is not close to the backpropagation gradient of 2.692902!\n",
      "Parameter id: 58 Numerical gradient of 0.013166 is not close to the backpropagation gradient of 1.316635!\n",
      "Parameter id: 59 Numerical gradient of 0.016952 is not close to the backpropagation gradient of 1.695204!\n",
      "Parameter id: 60 Numerical gradient of -0.002706 is not close to the backpropagation gradient of -0.270607!\n",
      "Parameter id: 61 Numerical gradient of -0.027221 is not close to the backpropagation gradient of -2.722146!\n",
      "Parameter id: 62 Numerical gradient of 0.048205 is not close to the backpropagation gradient of 4.820474!\n",
      "Parameter id: 63 Numerical gradient of 0.026833 is not close to the backpropagation gradient of 2.683293!\n",
      "Parameter id: 64 Numerical gradient of 0.008568 is not close to the backpropagation gradient of 0.856774!\n",
      "Parameter id: 65 Numerical gradient of 0.012011 is not close to the backpropagation gradient of 1.201069!\n",
      "Parameter id: 66 Numerical gradient of -0.030274 is not close to the backpropagation gradient of -3.027443!\n",
      "Parameter id: 67 Numerical gradient of 0.020763 is not close to the backpropagation gradient of 2.076314!\n",
      "Parameter id: 68 Numerical gradient of 0.005570 is not close to the backpropagation gradient of 0.557030!\n",
      "Parameter id: 69 Numerical gradient of 0.006930 is not close to the backpropagation gradient of 0.692955!\n",
      "Parameter id: 70 Numerical gradient of 0.000539 is not close to the backpropagation gradient of 0.053858!\n",
      "Parameter id: 71 Numerical gradient of -0.026650 is not close to the backpropagation gradient of -2.664950!\n",
      "Parameter id: 72 Numerical gradient of 0.048894 is not close to the backpropagation gradient of 4.889357!\n",
      "Parameter id: 73 Numerical gradient of 0.057963 is not close to the backpropagation gradient of 5.796303!\n",
      "Parameter id: 74 Numerical gradient of 0.007727 is not close to the backpropagation gradient of 0.772702!\n",
      "Parameter id: 75 Numerical gradient of 0.025041 is not close to the backpropagation gradient of 2.504142!\n",
      "Parameter id: 76 Numerical gradient of -0.026632 is not close to the backpropagation gradient of -2.663204!\n",
      "Parameter id: 77 Numerical gradient of 0.041685 is not close to the backpropagation gradient of 4.168468!\n",
      "Parameter id: 78 Numerical gradient of 0.018259 is not close to the backpropagation gradient of 1.825908!\n",
      "Parameter id: 79 Numerical gradient of 0.025934 is not close to the backpropagation gradient of 2.593449!\n",
      "Parameter id: 80 Numerical gradient of -0.000199 is not close to the backpropagation gradient of -0.019881!\n",
      "Parameter id: 81 Numerical gradient of -0.031608 is not close to the backpropagation gradient of -3.160768!\n",
      "Parameter id: 82 Numerical gradient of 0.057603 is not close to the backpropagation gradient of 5.760252!\n",
      "Parameter id: 83 Numerical gradient of 0.063813 is not close to the backpropagation gradient of 6.381342!\n",
      "Parameter id: 84 Numerical gradient of 0.010219 is not close to the backpropagation gradient of 1.021896!\n",
      "Parameter id: 85 Numerical gradient of 0.027688 is not close to the backpropagation gradient of 2.768763!\n",
      "Parameter id: 86 Numerical gradient of -0.030885 is not close to the backpropagation gradient of -3.088495!\n",
      "Parameter id: 87 Numerical gradient of 0.044056 is not close to the backpropagation gradient of 4.405594!\n",
      "Parameter id: 88 Numerical gradient of 0.018957 is not close to the backpropagation gradient of 1.895691!\n",
      "Parameter id: 89 Numerical gradient of 0.029071 is not close to the backpropagation gradient of 2.907053!\n",
      "Parameter id: 90 Numerical gradient of 0.000109 is not close to the backpropagation gradient of 0.010916!\n",
      "Parameter id: 91 Numerical gradient of -0.012058 is not close to the backpropagation gradient of -1.205779!\n",
      "Parameter id: 92 Numerical gradient of 0.018919 is not close to the backpropagation gradient of 1.891947!\n",
      "Parameter id: 93 Numerical gradient of 0.007841 is not close to the backpropagation gradient of 0.784100!\n",
      "Parameter id: 94 Numerical gradient of 0.002326 is not close to the backpropagation gradient of 0.232596!\n",
      "Parameter id: 95 Numerical gradient of 0.004961 is not close to the backpropagation gradient of 0.496078!\n",
      "Parameter id: 96 Numerical gradient of -0.014969 is not close to the backpropagation gradient of -1.496937!\n",
      "Parameter id: 97 Numerical gradient of 0.009306 is not close to the backpropagation gradient of 0.930587!\n",
      "Parameter id: 98 Numerical gradient of 0.000792 is not close to the backpropagation gradient of 0.079201!\n",
      "Parameter id: 99 Numerical gradient of 0.000477 is not close to the backpropagation gradient of 0.047730!\n",
      "Parameter id: 100 Numerical gradient of -0.003360 is not close to the backpropagation gradient of -0.336007!\n",
      "Parameter id: 101 Numerical gradient of -0.026558 is not close to the backpropagation gradient of -2.655779!\n",
      "Parameter id: 102 Numerical gradient of 0.045919 is not close to the backpropagation gradient of 4.591921!\n",
      "Parameter id: 103 Numerical gradient of 0.046732 is not close to the backpropagation gradient of 4.673191!\n",
      "Parameter id: 104 Numerical gradient of 0.010857 is not close to the backpropagation gradient of 1.085723!\n",
      "Parameter id: 105 Numerical gradient of 0.019055 is not close to the backpropagation gradient of 1.905489!\n",
      "Parameter id: 106 Numerical gradient of -0.023298 is not close to the backpropagation gradient of -2.329813!\n",
      "Parameter id: 107 Numerical gradient of 0.031562 is not close to the backpropagation gradient of 3.156170!\n",
      "Parameter id: 108 Numerical gradient of 0.013909 is not close to the backpropagation gradient of 1.390939!\n",
      "Parameter id: 109 Numerical gradient of 0.021334 is not close to the backpropagation gradient of 2.133364!\n",
      "Parameter id: 110 Numerical gradient of -0.000893 is not close to the backpropagation gradient of -0.089256!\n",
      "Parameter id: 111 Numerical gradient of -0.037020 is not close to the backpropagation gradient of -3.702028!\n",
      "Parameter id: 112 Numerical gradient of 0.073675 is not close to the backpropagation gradient of 7.367480!\n",
      "Parameter id: 113 Numerical gradient of 0.070601 is not close to the backpropagation gradient of 7.060103!\n",
      "Parameter id: 114 Numerical gradient of 0.012462 is not close to the backpropagation gradient of 1.246172!\n",
      "Parameter id: 115 Numerical gradient of 0.031489 is not close to the backpropagation gradient of 3.148901!\n",
      "Parameter id: 116 Numerical gradient of -0.038372 is not close to the backpropagation gradient of -3.837157!\n",
      "Parameter id: 117 Numerical gradient of 0.046374 is not close to the backpropagation gradient of 4.637356!\n",
      "Parameter id: 118 Numerical gradient of 0.020340 is not close to the backpropagation gradient of 2.033971!\n",
      "Parameter id: 119 Numerical gradient of 0.035720 is not close to the backpropagation gradient of 3.571957!\n",
      "Parameter id: 120 Numerical gradient of -0.002722 is not close to the backpropagation gradient of -0.272163!\n",
      "Parameter id: 121 Numerical gradient of -0.041280 is not close to the backpropagation gradient of -4.128027!\n",
      "Parameter id: 122 Numerical gradient of 0.077252 is not close to the backpropagation gradient of 7.725162!\n",
      "Parameter id: 123 Numerical gradient of 0.061379 is not close to the backpropagation gradient of 6.137947!\n",
      "Parameter id: 124 Numerical gradient of 0.013692 is not close to the backpropagation gradient of 1.369179!\n",
      "Parameter id: 125 Numerical gradient of 0.027252 is not close to the backpropagation gradient of 2.725182!\n",
      "Parameter id: 126 Numerical gradient of -0.042743 is not close to the backpropagation gradient of -4.274330!\n",
      "Parameter id: 127 Numerical gradient of 0.041870 is not close to the backpropagation gradient of 4.187013!\n",
      "Parameter id: 128 Numerical gradient of 0.016111 is not close to the backpropagation gradient of 1.611123!\n",
      "Parameter id: 129 Numerical gradient of 0.025543 is not close to the backpropagation gradient of 2.554266!\n",
      "Parameter id: 130 Numerical gradient of 0.008459 is not close to the backpropagation gradient of 0.845942!\n",
      "Parameter id: 131 Numerical gradient of 0.014926 is not close to the backpropagation gradient of 1.492558!\n",
      "Parameter id: 132 Numerical gradient of -0.069791 is not close to the backpropagation gradient of -6.979075!\n",
      "Parameter id: 133 Numerical gradient of -0.048870 is not close to the backpropagation gradient of -4.886983!\n",
      "Parameter id: 134 Numerical gradient of -0.018341 is not close to the backpropagation gradient of -1.834055!\n",
      "Parameter id: 135 Numerical gradient of -0.028869 is not close to the backpropagation gradient of -2.886863!\n",
      "Parameter id: 136 Numerical gradient of 0.002535 is not close to the backpropagation gradient of 0.253476!\n",
      "Parameter id: 137 Numerical gradient of -0.031540 is not close to the backpropagation gradient of -3.153953!\n",
      "Parameter id: 138 Numerical gradient of -0.021479 is not close to the backpropagation gradient of -2.147890!\n",
      "Parameter id: 139 Numerical gradient of -0.035170 is not close to the backpropagation gradient of -3.516963!\n",
      "Parameter id: 140 Numerical gradient of 0.024032 is not close to the backpropagation gradient of 2.403191!\n",
      "Parameter id: 141 Numerical gradient of -0.032966 is not close to the backpropagation gradient of -3.296600!\n",
      "Parameter id: 142 Numerical gradient of 0.015018 is not close to the backpropagation gradient of 1.501779!\n",
      "Parameter id: 143 Numerical gradient of 0.119561 is not close to the backpropagation gradient of 11.956118!\n",
      "Parameter id: 144 Numerical gradient of -0.085392 is not close to the backpropagation gradient of -8.539189!\n",
      "Parameter id: 145 Numerical gradient of 0.087768 is not close to the backpropagation gradient of 8.776786!\n",
      "Parameter id: 146 Numerical gradient of -0.006788 is not close to the backpropagation gradient of -0.678817!\n",
      "Parameter id: 147 Numerical gradient of -0.052447 is not close to the backpropagation gradient of -5.244700!\n",
      "Parameter id: 148 Numerical gradient of -0.068786 is not close to the backpropagation gradient of -6.878568!\n",
      "Parameter id: 149 Numerical gradient of -0.016096 is not close to the backpropagation gradient of -1.609550!\n",
      "Parameter id: 150 Numerical gradient of 0.025964 is not close to the backpropagation gradient of 2.596357!\n",
      "Parameter id: 151 Numerical gradient of -0.015844 is not close to the backpropagation gradient of -1.584382!\n",
      "Parameter id: 152 Numerical gradient of -0.110195 is not close to the backpropagation gradient of -11.019504!\n",
      "Parameter id: 153 Numerical gradient of 0.082321 is not close to the backpropagation gradient of 8.232062!\n",
      "Parameter id: 154 Numerical gradient of -0.083802 is not close to the backpropagation gradient of -8.380203!\n",
      "Parameter id: 155 Numerical gradient of 0.008784 is not close to the backpropagation gradient of 0.878416!\n",
      "Parameter id: 156 Numerical gradient of 0.044743 is not close to the backpropagation gradient of 4.474255!\n",
      "Parameter id: 157 Numerical gradient of 0.064125 is not close to the backpropagation gradient of 6.412549!\n",
      "Parameter id: 158 Numerical gradient of 0.004683 is not close to the backpropagation gradient of 0.468342!\n",
      "Parameter id: 159 Numerical gradient of 0.028262 is not close to the backpropagation gradient of 2.826178!\n",
      "Parameter id: 160 Numerical gradient of -0.008232 is not close to the backpropagation gradient of -0.823180!\n",
      "Parameter id: 161 Numerical gradient of -0.082805 is not close to the backpropagation gradient of -8.280480!\n",
      "Parameter id: 162 Numerical gradient of 0.051301 is not close to the backpropagation gradient of 5.130088!\n",
      "Parameter id: 163 Numerical gradient of -0.067371 is not close to the backpropagation gradient of -6.737123!\n",
      "Parameter id: 164 Numerical gradient of 0.006244 is not close to the backpropagation gradient of 0.624400!\n",
      "Parameter id: 165 Numerical gradient of 0.024128 is not close to the backpropagation gradient of 2.412800!\n",
      "Parameter id: 166 Numerical gradient of 0.043790 is not close to the backpropagation gradient of 4.378975!\n",
      "Parameter id: 167 Numerical gradient of -0.074091 is not close to the backpropagation gradient of -7.409080!\n",
      "Parameter id: 168 Numerical gradient of 0.010355 is not close to the backpropagation gradient of 1.035501!\n",
      "Parameter id: 169 Numerical gradient of -0.006226 is not close to the backpropagation gradient of -0.622617!\n",
      "Parameter id: 170 Numerical gradient of -0.039528 is not close to the backpropagation gradient of -3.952771!\n",
      "Parameter id: 171 Numerical gradient of 0.034651 is not close to the backpropagation gradient of 3.465135!\n",
      "Parameter id: 172 Numerical gradient of -0.001163 is not close to the backpropagation gradient of -0.116322!\n",
      "Parameter id: 173 Numerical gradient of 0.010362 is not close to the backpropagation gradient of 1.036217!\n",
      "Parameter id: 174 Numerical gradient of 0.034802 is not close to the backpropagation gradient of 3.480228!\n",
      "Parameter id: 175 Numerical gradient of 0.030837 is not close to the backpropagation gradient of 3.083709!\n",
      "Parameter id: 176 Numerical gradient of -0.018589 is not close to the backpropagation gradient of -1.858917!\n",
      "Parameter id: 177 Numerical gradient of 0.035115 is not close to the backpropagation gradient of 3.511468!\n",
      "Parameter id: 178 Numerical gradient of -0.014630 is not close to the backpropagation gradient of -1.462978!\n",
      "Parameter id: 179 Numerical gradient of -0.116284 is not close to the backpropagation gradient of -11.628367!\n",
      "Parameter id: 180 Numerical gradient of 0.078948 is not close to the backpropagation gradient of 7.894785!\n",
      "Parameter id: 181 Numerical gradient of -0.083921 is not close to the backpropagation gradient of -8.392149!\n",
      "Parameter id: 182 Numerical gradient of 0.007301 is not close to the backpropagation gradient of 0.730090!\n",
      "Parameter id: 183 Numerical gradient of 0.046903 is not close to the backpropagation gradient of 4.690336!\n",
      "Parameter id: 184 Numerical gradient of 0.065157 is not close to the backpropagation gradient of 6.515732!\n",
      "Parameter id: 185 Numerical gradient of -0.028528 is not close to the backpropagation gradient of -2.852837!\n",
      "Parameter id: 186 Numerical gradient of 0.031881 is not close to the backpropagation gradient of 3.188100!\n",
      "Parameter id: 187 Numerical gradient of -0.015857 is not close to the backpropagation gradient of -1.585731!\n",
      "Parameter id: 188 Numerical gradient of -0.117704 is not close to the backpropagation gradient of -11.770448!\n",
      "Parameter id: 189 Numerical gradient of 0.083940 is not close to the backpropagation gradient of 8.394024!\n",
      "Parameter id: 190 Numerical gradient of -0.084245 is not close to the backpropagation gradient of -8.424528!\n",
      "Parameter id: 191 Numerical gradient of 0.007885 is not close to the backpropagation gradient of 0.788476!\n",
      "Parameter id: 192 Numerical gradient of 0.053703 is not close to the backpropagation gradient of 5.370271!\n",
      "Parameter id: 193 Numerical gradient of 0.068927 is not close to the backpropagation gradient of 6.892674!\n",
      "Parameter id: 194 Numerical gradient of -0.034170 is not close to the backpropagation gradient of -3.417033!\n",
      "Parameter id: 195 Numerical gradient of 0.006384 is not close to the backpropagation gradient of 0.638358!\n",
      "Parameter id: 196 Numerical gradient of -0.002864 is not close to the backpropagation gradient of -0.286413!\n",
      "Parameter id: 197 Numerical gradient of -0.018322 is not close to the backpropagation gradient of -1.832183!\n",
      "Parameter id: 198 Numerical gradient of 0.010090 is not close to the backpropagation gradient of 1.008955!\n",
      "Parameter id: 199 Numerical gradient of 0.011261 is not close to the backpropagation gradient of 1.126087!\n",
      "Parameter id: 200 Numerical gradient of 0.002440 is not close to the backpropagation gradient of 0.243986!\n",
      "Parameter id: 201 Numerical gradient of 0.015714 is not close to the backpropagation gradient of 1.571350!\n",
      "Parameter id: 202 Numerical gradient of 0.009469 is not close to the backpropagation gradient of 0.946892!\n",
      "Parameter id: 203 Numerical gradient of -0.026176 is not close to the backpropagation gradient of -2.617552!\n",
      "Parameter id: 204 Numerical gradient of 0.017004 is not close to the backpropagation gradient of 1.700378!\n",
      "Parameter id: 205 Numerical gradient of -0.014239 is not close to the backpropagation gradient of -1.423909!\n",
      "Parameter id: 206 Numerical gradient of -0.085904 is not close to the backpropagation gradient of -8.590397!\n",
      "Parameter id: 207 Numerical gradient of 0.063674 is not close to the backpropagation gradient of 6.367447!\n",
      "Parameter id: 208 Numerical gradient of -0.062116 is not close to the backpropagation gradient of -6.211566!\n",
      "Parameter id: 209 Numerical gradient of 0.012868 is not close to the backpropagation gradient of 1.286754!\n",
      "Parameter id: 210 Numerical gradient of 0.039768 is not close to the backpropagation gradient of 3.976819!\n",
      "Parameter id: 211 Numerical gradient of 0.055120 is not close to the backpropagation gradient of 5.512027!\n",
      "Parameter id: 212 Numerical gradient of -0.050745 is not close to the backpropagation gradient of -5.074528!\n",
      "Parameter id: 213 Numerical gradient of 0.037137 is not close to the backpropagation gradient of 3.713726!\n",
      "Parameter id: 214 Numerical gradient of -0.014961 is not close to the backpropagation gradient of -1.496086!\n",
      "Parameter id: 215 Numerical gradient of -0.118792 is not close to the backpropagation gradient of -11.879240!\n",
      "Parameter id: 216 Numerical gradient of 0.087349 is not close to the backpropagation gradient of 8.734920!\n",
      "Parameter id: 217 Numerical gradient of -0.083606 is not close to the backpropagation gradient of -8.360632!\n",
      "Parameter id: 218 Numerical gradient of 0.010325 is not close to the backpropagation gradient of 1.032453!\n",
      "Parameter id: 219 Numerical gradient of 0.061335 is not close to the backpropagation gradient of 6.133501!\n",
      "Parameter id: 220 Numerical gradient of 0.071959 is not close to the backpropagation gradient of 7.195887!\n",
      "Parameter id: 221 Numerical gradient of -0.081445 is not close to the backpropagation gradient of -8.144549!\n",
      "Parameter id: 222 Numerical gradient of 0.027760 is not close to the backpropagation gradient of 2.775978!\n",
      "Parameter id: 223 Numerical gradient of -0.012642 is not close to the backpropagation gradient of -1.264172!\n",
      "Parameter id: 224 Numerical gradient of -0.096635 is not close to the backpropagation gradient of -9.663465!\n",
      "Parameter id: 225 Numerical gradient of 0.077580 is not close to the backpropagation gradient of 7.757992!\n",
      "Parameter id: 226 Numerical gradient of -0.055135 is not close to the backpropagation gradient of -5.513465!\n",
      "Parameter id: 227 Numerical gradient of 0.014826 is not close to the backpropagation gradient of 1.482594!\n",
      "Parameter id: 228 Numerical gradient of 0.059933 is not close to the backpropagation gradient of 5.993269!\n",
      "Parameter id: 229 Numerical gradient of 0.065758 is not close to the backpropagation gradient of 6.575819!\n",
      "Parameter id: 230 Numerical gradient of 0.040175 is not close to the backpropagation gradient of 4.017504!\n",
      "Parameter id: 231 Numerical gradient of -0.050378 is not close to the backpropagation gradient of -5.037803!\n",
      "Parameter id: 232 Numerical gradient of 0.016968 is not close to the backpropagation gradient of 1.696813!\n",
      "Parameter id: 233 Numerical gradient of 0.127432 is not close to the backpropagation gradient of 12.743185!\n",
      "Parameter id: 234 Numerical gradient of -0.085779 is not close to the backpropagation gradient of -8.577874!\n",
      "Parameter id: 235 Numerical gradient of 0.091067 is not close to the backpropagation gradient of 9.106733!\n",
      "Parameter id: 236 Numerical gradient of -0.005806 is not close to the backpropagation gradient of -0.580608!\n",
      "Parameter id: 237 Numerical gradient of -0.063689 is not close to the backpropagation gradient of -6.368908!\n",
      "Parameter id: 238 Numerical gradient of -0.069990 is not close to the backpropagation gradient of -6.999043!\n"
     ]
    }
   ],
   "source": [
    "# Do gradient checking\n",
    "# Define an RNN to test\n",
    "RNN = RnnBinaryAdder(1, output_size, hidden_size, seq_length)\n",
    "# Get the gradients of the parameters from a subset of the data\n",
    "backprop_grads = RNN.getParamGrads(\n",
    "    x_train[:100], y_train[:100])\n",
    "\n",
    "eps = 1e-7  # Set the small change to compute the numerical gradient\n",
    "# Compute the numerical gradients of the parameters in all layers.\n",
    "for p_idx, param in enumerate(RNN.get_params_iter()):\n",
    "    grad_backprop = backprop_grads[p_idx]\n",
    "    # + eps\n",
    "    param += eps\n",
    "    plus_loss = RNN.loss(\n",
    "        RNN.getOutput(x_train[0:100,:,:]), y_train[0:100,:,:])\n",
    "    # - eps\n",
    "    param -= 2 * eps\n",
    "    min_loss = RNN.loss(\n",
    "        RNN.getOutput(x_train[0:100,:,:]), y_train[0:100,:,:])\n",
    "    # reset param value\n",
    "    param += eps\n",
    "    # calculate numerical gradient\n",
    "    grad_num = (plus_loss - min_loss) / (2*eps)\n",
    "    # Raise error if the numerical grade is not close to the \n",
    "    #  backprop gradient\n",
    "    if not np.isclose(grad_num, grad_backprop):\n",
    "        print((\n",
    "            f'Parameter id: {p_idx} '\n",
    "            f'Numerical gradient of {grad_num:.6f} is not close '\n",
    "            f'to the backpropagation gradient of {grad_backprop:.6f}!'\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: read article till end\n",
    "# TODO: gradient of loss w.r.t to softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All network is good; error somewhere inside LOgisticSOftmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
